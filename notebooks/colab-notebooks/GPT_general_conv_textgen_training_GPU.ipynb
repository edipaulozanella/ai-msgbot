{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT2- general conv - textgen training GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NK7daXR_cAU1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf012b096e194ed8bbc285e0a0b89f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e8a06b836ad41e7a7af8c0f5ef5dd04",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0fcf995f88cf4d3a9ccdfa5d9b3a331e",
              "IPY_MODEL_cfaaee826aeb4416a7a7d3baced6e1d0",
              "IPY_MODEL_e19a0c2a458648238af731ebcda4c94b"
            ]
          }
        },
        "1e8a06b836ad41e7a7af8c0f5ef5dd04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0fcf995f88cf4d3a9ccdfa5d9b3a331e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c9c847127b8240fb8c11b5d69d5aa5fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "replacing speaker names: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86cb5d096e774edeaa6213bb5c543c9a"
          }
        },
        "cfaaee826aeb4416a7a7d3baced6e1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9bd2fcb3008c485281a031b0923cf1c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 216768,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 216768,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e88915ddaaed41af97addd04dcf1f05b"
          }
        },
        "e19a0c2a458648238af731ebcda4c94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_208f4bd24baf41f590f7d961fe26c064",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 216768/216768 [00:00&lt;00:00, 1020120.29it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ec93e16e8bc4b65b7d9fcdaa96ab6ef"
          }
        },
        "c9c847127b8240fb8c11b5d69d5aa5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86cb5d096e774edeaa6213bb5c543c9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9bd2fcb3008c485281a031b0923cf1c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e88915ddaaed41af97addd04dcf1f05b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "208f4bd24baf41f590f7d961fe26c064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ec93e16e8bc4b65b7d9fcdaa96ab6ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbc0a0c199564ed288d2b9584d3ca3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_21e62720b414427095091703843bf0d0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cdac6467788f4f97a107c0dc9d1224ef",
              "IPY_MODEL_844a77cbad2c4b0f99d8063d606897f5",
              "IPY_MODEL_4bfe5c8f68864586a669d073d9e9eb14"
            ]
          }
        },
        "21e62720b414427095091703843bf0d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "cdac6467788f4f97a107c0dc9d1224ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_41c5394f1e5241e189ae7a76e3c2f880",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_88aca4fa602947ea8b314b7924d5d8f0"
          }
        },
        "844a77cbad2c4b0f99d8063d606897f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba638242c831411b99980b3b23d439d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 216768,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 216768,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f346b497869e4e1da5ea5435f30fdd4a"
          }
        },
        "4bfe5c8f68864586a669d073d9e9eb14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_966ca024599e4a6bb20b05bf8b408fb5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 216768/216768 [00:04&lt;00:00, 45539.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_453fab1d5b314e0dafdd28dca183b391"
          }
        },
        "41c5394f1e5241e189ae7a76e3c2f880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "88aca4fa602947ea8b314b7924d5d8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba638242c831411b99980b3b23d439d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f346b497869e4e1da5ea5435f30fdd4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "966ca024599e4a6bb20b05bf8b408fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "453fab1d5b314e0dafdd28dca183b391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70b2571bf40b4c1c8af16e7cfea0fad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2976434b391f4a66b2b884a856603afc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18c43bb75612469ba58b2b66979ea503",
              "IPY_MODEL_e8a71495e5354db894c43384788b6174",
              "IPY_MODEL_badbd92e876740f3a73f3c069b9221d3"
            ]
          }
        },
        "2976434b391f4a66b2b884a856603afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "18c43bb75612469ba58b2b66979ea503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4bb26c814d8f43a0b890f4c7c4c44880",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Loss: 0.398 — Avg: 0.455 — GPU Mem: 16125 MB: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83d2c3f75936431f9d30943dbbfec4b1"
          }
        },
        "e8a71495e5354db894c43384788b6174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_553c0e89a84244998d27c8fb38bc8591",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9515d9525d444399abed7ba1bd74a125"
          }
        },
        "badbd92e876740f3a73f3c069b9221d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_168cb8ee67f5441487404d10b4895495",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [2:27:03&lt;00:00,  1.13it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c3ee715d4864539a5a1bf4b9df3f72f"
          }
        },
        "4bb26c814d8f43a0b890f4c7c4c44880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83d2c3f75936431f9d30943dbbfec4b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "553c0e89a84244998d27c8fb38bc8591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9515d9525d444399abed7ba1bd74a125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "168cb8ee67f5441487404d10b4895495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c3ee715d4864539a5a1bf4b9df3f72f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/workshop-updates-1/notebooks/colab-notebooks/GPT_general_conv_textgen_training_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  aitextgen — Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU\n",
        "\n",
        "This notebook is based on the original tutorial from `aitextgen`!\n",
        "\n",
        "- For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/).\n",
        "- for `ai-msgbot` (which is using `aitextgen` for chatbot-esque purposes) you can find the project repo [here](https://github.com/pszemraj/ai-msgbot)\n",
        "\n",
        "\n",
        "_updates made by [Peter](https://peterszemraj.ch/)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4MbTt1SwId0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhbtErR7wKxC"
      },
      "source": [
        "### GPU\n",
        "\n",
        "Colaboratory uses a Nvidia K80, an Nvidia P100, an Nvidia V100, or Nvidia A100. For finetuning GPT-2 124M, any of these GPUs will be fine, but for text generation, a k80 or a P100 is ideal since they have more VRAM. \n",
        "\n",
        "- In theory: **If you receive a T4 or a V100 GPU, you can enable `fp16=True` during training for faster/more memory efficient training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgMEhF7J3ubw",
        "outputId": "a8ea9da1-0ebf-4b23-c47f-681c67a599b1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 26 16:29:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "### formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemDfmdgjefZ"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "_UJ8ek31Uz-r",
        "outputId": "0bdfa9ee-f64e-44d1-fd09-06e714368473"
      },
      "source": [
        "# update torch in case using a A100 GPU\n",
        "# may take 5+ minutes\n",
        "!pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |██████████████▋                 | 834.1 MB 1.2 MB/s eta 0:13:22tcmalloc: large alloc 1147494400 bytes == 0x559d96aa8000 @  0x7f7a9f55f615 0x559d5e1db4cc 0x559d5e2bb47a 0x559d5e1de2ed 0x559d5e2cfe1d 0x559d5e251e99 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e251d00 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e1e0039 0x559d5e223409 0x559d5e1dec52 0x559d5e251c25 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24d915 0x559d5e1dfafa 0x559d5e24dc0d 0x559d5e24c9ee\n",
            "\u001b[K     |██████████████████▌             | 1055.7 MB 1.4 MB/s eta 0:09:27tcmalloc: large alloc 1434370048 bytes == 0x559ddb0fe000 @  0x7f7a9f55f615 0x559d5e1db4cc 0x559d5e2bb47a 0x559d5e1de2ed 0x559d5e2cfe1d 0x559d5e251e99 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e251d00 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e1e0039 0x559d5e223409 0x559d5e1dec52 0x559d5e251c25 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24d915 0x559d5e1dfafa 0x559d5e24dc0d 0x559d5e24c9ee\n",
            "\u001b[K     |███████████████████████▌        | 1336.2 MB 1.2 MB/s eta 0:06:31tcmalloc: large alloc 1792966656 bytes == 0x559d5ff30000 @  0x7f7a9f55f615 0x559d5e1db4cc 0x559d5e2bb47a 0x559d5e1de2ed 0x559d5e2cfe1d 0x559d5e251e99 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e251d00 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e1e0039 0x559d5e223409 0x559d5e1dec52 0x559d5e251c25 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24d915 0x559d5e1dfafa 0x559d5e24dc0d 0x559d5e24c9ee\n",
            "\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.2 MB/s eta 0:01:49tcmalloc: large alloc 2241208320 bytes == 0x559dcad18000 @  0x7f7a9f55f615 0x559d5e1db4cc 0x559d5e2bb47a 0x559d5e1de2ed 0x559d5e2cfe1d 0x559d5e251e99 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e251d00 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e2d0c66 0x559d5e24ddaf 0x559d5e1e0039 0x559d5e223409 0x559d5e1dec52 0x559d5e251c25 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24d915 0x559d5e1dfafa 0x559d5e24dc0d 0x559d5e24c9ee\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 1.4 MB/s eta 0:00:01tcmalloc: large alloc 1821458432 bytes == 0x559e5067a000 @  0x7f7a9f55e1e7 0x559d5e211067 0x559d5e1db4cc 0x559d5e2bb47a 0x559d5e1de2ed 0x559d5e2cfe1d 0x559d5e251e99 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e1dfafa 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee\n",
            "tcmalloc: large alloc 2276827136 bytes == 0x559ebcf8e000 @  0x7f7a9f55f615 0x559d5e1db4cc 0x559d5e2bb47a 0x559d5e1de2ed 0x559d5e2cfe1d 0x559d5e251e99 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24dc0d 0x559d5e1dfafa 0x559d5e24dc0d 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee 0x559d5e1dfbda 0x559d5e24e737 0x559d5e24c9ee 0x559d5e1e0271\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 5.7 kB/s \n",
            "\u001b[K     |████████████████████████████████| 24.6 MB 6.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "9198c556-0b61-4a05-9bcc-bbfa39c89274"
      },
      "source": [
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 572 kB 14.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 44.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 8.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 523 kB 41.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 48.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 47.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 329 kB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 69.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 61.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 66.3 MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DuC67no59vMe",
        "outputId": "531c5d7a-63fd-4c66-8b9e-0a11e955d62d"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "### Loading GPT-2 or GPT Neo\n",
        "\n",
        "\n",
        "- A common use case is *continuing* to fine-tune a model that was originally pretrained, and then fine-tuned a little bit, but needs to be fine-tuned more for accuracy/saliency reasons or because Google cut off the runtime earlier. \n",
        "    - in this case, `load_from_folder` should be set to `True` and `load_folder_dir` points to where the model checkpoint is on your google drive.\n",
        "- **the below section describes loading an new/pretrained model from the original tutorial.**\n",
        "\n",
        "> If you're retraining a model on new text, you need to download and load the GPT-2 model into the GPU. \n",
        "\n",
        "> There are several sizes of GPT-2:\n",
        "\n",
        "    * `124M` (default): the \"small\" model, 500MB on disk.\n",
        "    * `355M` (default): the \"medium\" model, 1.5GB on disk.\n",
        "    * `774M` (default): the \"large\" model, 3GB on disk.\n",
        "\n",
        "> You can also finetune a GPT Neo model instead ([_or any textgen GPT-architecture model on huggingface_](https://huggingface.co/models?pipeline_tag=text-generation)), which is more suitable for longer texts and the base model has more recent data:\n",
        "\n",
        "* `125M`: Analogous to the GPT-2 124M model. (355M parameter model was removed)\n",
        "*  `EleutherAI/gpt-neo-1.3B` : 1.3 billion parameter model. Have yet to see this train on Colab without crashing\n",
        "\n",
        "> The next cell downloads the model and saves it in the Colaboratory VM. If the model has already been downloaded, running this cell will reload it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPPsg8XDwHvl"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXIbld-ASHL9",
        "outputId": "2717bd70-1643-4c99-900c-b1ab401c7410"
      },
      "source": [
        "model_size = \"774M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = True #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Programming/ai-msgbot/GPT2_774M_chk_175KnatQAtrivDD_100kWoW\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "flqSlHjMIeIw",
        "outputId": "f88eece8-6c7d-4a98-e29f-a90f9621090f"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "else:\n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "# Comment out the above line and uncomment the below line to use GPT Neo instead.\n",
        "# ai = aitextgen(model='distilgpt2', \n",
        "#                to_gpu=True, \n",
        "#                gradient_checkpointing=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/26/2021 16:34:09 — INFO — aitextgen — Loading model from provided weights and config in //content/drive/MyDrive/Programming/ai-msgbot/GPT2_774M_chk_175KnatQAtrivDD_100kWoW.\n",
            "11/26/2021 16:34:47 — INFO — aitextgen — GPT2 loaded with 774M parameters.\n",
            "11/26/2021 16:34:47 — INFO — aitextgen — Gradient checkpointing enabled for model training.\n",
            "11/26/2021 16:34:47 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7oQAqQHiFiX"
      },
      "source": [
        "### load training data\n",
        "\n",
        "\n",
        "- <font color=\"orange\"> combine any other data from the a \"standard\" conversational dataset and do a final pass of training (with different data) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jUWsiVOCUlLo",
        "outputId": "f422af17-aa69-473a-b291-4b27f9d83792"
      },
      "source": [
        "dl_link = \"https://github.com/pszemraj/ai-msgbot/raw/main/conversation-data/Daily-Dialogues/daily_dialogue_augment.txt\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hirJkLvbUrMb",
        "outputId": "24a3d795-fb07-41b5-d4db-93d6ff49fcbb"
      },
      "source": [
        "# download test image\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import os\n",
        "vm_wd = os.getcwd()\n",
        "local_name = join(vm_wd, \"training_script.txt\")\n",
        "request.urlretrieve(dl_link, local_name)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/training_script.txt', <http.client.HTTPMessage at 0x7f1457c52450>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cEWZnWyYX3y8",
        "outputId": "ca04c54b-664a-441d-a1c7-e348b7a96f3d"
      },
      "source": [
        "# adjust names in script if needed  \n",
        "import pprint as pp\n",
        "from os.path import basename\n",
        "\n",
        "def update_script_names(local_name, spkr_from=\"speaker a\", \n",
        "                        spkr_to=\"person alpha\",\n",
        "                        resp_from=\"speaker b\", resp_to=\"person beta\",\n",
        "                        verbose=False):\n",
        "    \"\"\"\n",
        "    update_script_names - if the textfile script has different names for the \n",
        "    speaker/responder than desired (i.e. it is a group conversation, and the \n",
        "    chatbot is just supposed to simulate 1:1) this function can be used to \n",
        "    standardize\n",
        "    \"\"\"\n",
        "\n",
        "    with open(local_name, 'r', encoding='utf-8', errors='ignore') as fi:\n",
        "        orig_lines = fi.readlines()\n",
        "\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    upd_lines = []\n",
        "\n",
        "    for line in tqdm(orig_lines, total=len(orig_lines), \n",
        "                     desc=\"replacing speaker names\"):\n",
        "        \n",
        "        fixline = line.replace(spkr_from, spkr_to)\n",
        "        fixline = fixline.replace(resp_from, resp_to)\n",
        "        upd_lines.append(fixline)\n",
        "\n",
        "    local_namev2 = join(vm_wd, \"V2-rename-\" + basename(local_name))\n",
        "\n",
        "    with open(local_namev2, 'w', encoding='utf-8', errors='ignore') as fo:\n",
        "        fo.writelines(upd_lines)\n",
        "\n",
        "    if verbose: pp.pprint(upd_lines[:10])\n",
        "    # return filepath\n",
        "    return local_namev2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cf012b096e194ed8bbc285e0a0b89f2d",
            "1e8a06b836ad41e7a7af8c0f5ef5dd04",
            "0fcf995f88cf4d3a9ccdfa5d9b3a331e",
            "cfaaee826aeb4416a7a7d3baced6e1d0",
            "e19a0c2a458648238af731ebcda4c94b",
            "c9c847127b8240fb8c11b5d69d5aa5fb",
            "86cb5d096e774edeaa6213bb5c543c9a",
            "9bd2fcb3008c485281a031b0923cf1c7",
            "e88915ddaaed41af97addd04dcf1f05b",
            "208f4bd24baf41f590f7d961fe26c064",
            "5ec93e16e8bc4b65b7d9fcdaa96ab6ef"
          ]
        },
        "id": "6OFnPCLADfll",
        "outputId": "0c2743ee-4fcf-468a-a251-bf8d8064a6d7"
      },
      "source": [
        "local_name = update_script_names(local_name)\n",
        "\n",
        "file_name = local_name # update if using fn above"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf012b096e194ed8bbc285e0a0b89f2d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "replacing speaker names:   0%|          | 0/216768 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Train / Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "REDm2euJ1kHB",
        "outputId": "0fba79c1-18ec-4379-897c-719fa303a751"
      },
      "source": [
        "base_dir = \"/content/drive/MyDrive/Programming/ai-msgbot\" #@param {type:\"string\"}\n",
        "# update to yours"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sDmyL2A7uhaL",
        "outputId": "c5ac4267-9e9a-4ac1-8e2c-02f87c8d3efe"
      },
      "source": [
        "import gc, os\n",
        "from os.path import join\n",
        "from datetime import datetime\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime(\"%b-%d-%Y_t-%H\")\n",
        "\n",
        "temp_gpu_path = join(base_dir, \n",
        "                     \"GPT2-conversational-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                            dt=get_timestamp(),\n",
        "                                                            )\n",
        "                     )\n",
        "os.makedirs(temp_gpu_path, exist_ok=True)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzDXk_iGn-wF"
      },
      "source": [
        "### example outputs\n",
        "\n",
        "- this cell had its outputs cleared before notebook was posted.\n",
        "- the below is an example of what the outputs during training should look like.\n",
        "\n",
        "```\n",
        "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1819: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
        "  \"`trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7.\"\n",
        "1,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-25-2021_t-04\n",
        "1,500 steps reached: generating sample texts.\n",
        "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
        "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
        "==========\n",
        "\n",
        "\n",
        "person alpha:\n",
        "do you enjoy the movie?\n",
        "\n",
        "person beta:\n",
        "yes, i love the film is based in george r. r. martin scotland\n",
        "\n",
        "person alpha:\n",
        "oh cool, how does it get to be one?\n",
        "\n",
        "person beta:\n",
        "well it was a break between a single.\n",
        "\n",
        "person alpha:\n",
        "i really like the show but i don't know much about it, i do not know much about it\n",
        "\n",
        "person beta:\n",
        "i know there are many episodes for the show, and the show has to be the highest grosss in the show though.\n",
        "\n",
        "person alpha:\n",
        "i've been a new game of thrones, or twice a year\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1pdV5lovKg"
      },
      "source": [
        "### t r a i n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fbc0a0c199564ed288d2b9584d3ca3ec",
            "21e62720b414427095091703843bf0d0",
            "cdac6467788f4f97a107c0dc9d1224ef",
            "844a77cbad2c4b0f99d8063d606897f5",
            "4bfe5c8f68864586a669d073d9e9eb14",
            "41c5394f1e5241e189ae7a76e3c2f880",
            "88aca4fa602947ea8b314b7924d5d8f0",
            "ba638242c831411b99980b3b23d439d9",
            "f346b497869e4e1da5ea5435f30fdd4a",
            "966ca024599e4a6bb20b05bf8b408fb5",
            "453fab1d5b314e0dafdd28dca183b391",
            "70b2571bf40b4c1c8af16e7cfea0fad2",
            "2976434b391f4a66b2b884a856603afc",
            "18c43bb75612469ba58b2b66979ea503",
            "e8a71495e5354db894c43384788b6174",
            "badbd92e876740f3a73f3c069b9221d3",
            "4bb26c814d8f43a0b890f4c7c4c44880",
            "83d2c3f75936431f9d30943dbbfec4b1",
            "553c0e89a84244998d27c8fb38bc8591",
            "9515d9525d444399abed7ba1bd74a125",
            "168cb8ee67f5441487404d10b4895495",
            "4c3ee715d4864539a5a1bf4b9df3f72f"
          ]
        },
        "id": "aeXshJM-Cuaf",
        "outputId": "9a6cf5c4-604d-4a72-b036-d86e99aeed5e"
      },
      "source": [
        "# DO NOT USE WARMUP STEPS\n",
        "\n",
        "ai.train(\n",
        "            file_name, # text file with training data\n",
        "            output_dir=temp_gpu_path, # where it saves during \"save_every\"\n",
        "            line_by_line=False, # if using CSV file input\n",
        "            from_cache=False,\n",
        "            num_steps=10000, # takes about 5 hours on 16 gb v100 GPU for 75000\n",
        "            generate_every=1500,\n",
        "            max_grad_norm=0.5,\n",
        "            save_every=1000,\n",
        "            gradient_accumulation_steps=1,\n",
        "            save_gdrive=False, # this is an \"automated\" save which is worse than current method (IMO)\n",
        "            learning_rate=1e-3,\n",
        "        #  fp16=True, # may be relevant to set to false (even if available) for \"final\" training\n",
        "            batch_size=1, # if pushing model_size you probably want to leave this at 1\n",
        "        #  fp16_opt_level=\"O2\", # different types of FP16 are possible\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/26/2021 16:35:01 — INFO — aitextgen — Loading text from /content/V2-rename-training_script.txt with generation length of 1024.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbc0a0c199564ed288d2b9584d3ca3ec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/216768 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "11/26/2021 16:35:01 — INFO — aitextgen.TokenDataset — Encoding 216,768 sets of tokens from /content/V2-rename-training_script.txt.\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
            "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:168: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
            "  \"Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed\"\n",
            "11/26/2021 16:35:06 — INFO — pytorch_lightning.utilities.distributed — GPU available: True, used: True\n",
            "11/26/2021 16:35:06 — INFO — pytorch_lightning.utilities.distributed — TPU available: False, using: 0 TPU cores\n",
            "11/26/2021 16:35:06 — INFO — pytorch_lightning.utilities.distributed — IPU available: False, using: 0 IPUs\n",
            "11/26/2021 16:35:06 — INFO — pytorch_lightning.accelerators.gpu — LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70b2571bf40b4c1c8af16e7cfea0fad2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1819: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
            "  \"`trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m1,500 steps reached: generating sample texts.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========\n",
            ".\n",
            "\n",
            "person alpha:\n",
            "i hate to hear that.\n",
            "\n",
            "person beta:\n",
            "you are so careless, aren't you?\n",
            "\n",
            "person alpha:\n",
            "i'm feeling very sad for a few days.\n",
            "\n",
            "person beta:\n",
            "can you help me be of any of the things i need to learn?\n",
            "\n",
            "person alpha:\n",
            "for the holidays, i have lots of questions.\n",
            "\n",
            "person beta:\n",
            "yes, these days now are more important or less exciting.\n",
            "\n",
            "person alpha:\n",
            "i don't know.\n",
            "\n",
            "person beta:\n",
            "i think you're a better person, but i'm sure you will do the right thing.\n",
            "\n",
            "person alpha:\n",
            "we have a deal.\n",
            "\n",
            "person beta:\n",
            "i hope you can stay in bed for two days.\n",
            "\n",
            "person alpha:\n",
            "that's great. i'll be waiting for them soon.\n",
            "\n",
            "person beta:\n",
            "i'm glad to have a good friend.\n",
            "\n",
            "person alpha:\n",
            "i'm going to be pleased with my wife.\n",
            "\n",
            "person beta:\n",
            "do you think i can help?\n",
            "\n",
            "person alpha:\n",
            "i'm quite sure your boss is in one of the question.\n",
            "\n",
            "person beta:\n",
            "i can't go any\n",
            "==========\n",
            "\u001b[1m2,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m3,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            ".\n",
            "\n",
            "person beta:\n",
            "what's that?\n",
            "\n",
            "person alpha:\n",
            "i want to buy a rain coat and then it broke. i like the rain too much.\n",
            "\n",
            "person beta:\n",
            "it does smell a little.\n",
            "\n",
            "person alpha:\n",
            "you mean like it like this?\n",
            "\n",
            "person beta:\n",
            "yes, it's a beautiful view.\n",
            "\n",
            "person alpha:\n",
            "i like the pierre, but i don't like the winter with it.\n",
            "\n",
            "person beta:\n",
            "and don't you like the rain?\n",
            "\n",
            "person alpha:\n",
            "yes, i like the winter with my husband.\n",
            "\n",
            "person beta:\n",
            "where did you put it from?\n",
            "\n",
            "person alpha:\n",
            "my little green apples are too expensive.\n",
            "\n",
            "person beta:\n",
            " expensive, green apples are cucumbers. blow out the grapes and enjoy the rest of the hard drive.\n",
            "\n",
            "person alpha:\n",
            "all right, sir.\n",
            "\n",
            "person beta:\n",
            "i would like to buy a guitar.\n",
            "\n",
            "person alpha:\n",
            "what about this one? it's a 16 + automobile who is famous in the field.\n",
            "\n",
            "person beta:\n",
            "would you recommend some rather?\n",
            "\n",
            "person alpha:\n",
            "i recommend it. it took us\n",
            "==========\n",
            "\u001b[1m4,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m4,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "!\n",
            "\n",
            "person beta:\n",
            "yes, it's my favorite.\n",
            "\n",
            "person alpha:\n",
            "are you also like coffee?\n",
            "\n",
            "person beta:\n",
            "i want to have a coffee.\n",
            "\n",
            "person alpha:\n",
            "what about my coffee?\n",
            "\n",
            "person beta:\n",
            "i recommend coffee and sometimes you take some green coffee.\n",
            "\n",
            "person alpha:\n",
            "do you like your coffee?\n",
            "\n",
            "person beta:\n",
            "yes, but i think it's enough.\n",
            "\n",
            "person alpha:\n",
            "what is it for being served tea?\n",
            "\n",
            "person beta:\n",
            "it's been quite well but the better is the same.\n",
            "\n",
            "person alpha:\n",
            "what time do you close, what's up?\n",
            "\n",
            "person beta:\n",
            "i'm planning on going to a post office.\n",
            "\n",
            "person alpha:\n",
            "what's your flight time?\n",
            "\n",
            "person beta:\n",
            "i'm leaving at 9\n",
            "\n",
            "person alpha:\n",
            "oh, i know the twenty time.\n",
            "\n",
            "person beta:\n",
            "i see. but i can't get to the airport on that day.\n",
            "\n",
            "person alpha:\n",
            "i'm not sure yet.\n",
            "\n",
            "person beta:\n",
            "i want to post this package to the post office.\n",
            "\n",
            "person alpha:\n",
            "don't you think\n",
            "==========\n",
            "\u001b[1m5,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m6,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m6,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "!\n",
            "\n",
            "person beta:\n",
            "that's true. having a very serious boyfriend or girlfriend can be boring.\n",
            "\n",
            "person alpha:\n",
            "i can't decide what to do today.\n",
            "\n",
            "person beta:\n",
            "well. let's go swimming.\n",
            "\n",
            "person alpha:\n",
            "don't you think it's a bit cold for that?\n",
            "\n",
            "person beta:\n",
            "then why don't we go in the walk in the hills. it'd be very good for us you know.\n",
            "\n",
            "person alpha:\n",
            "car trouble center. how may i help you?\n",
            "\n",
            "person beta:\n",
            "my car won't start! stupid old car!\n",
            "\n",
            "person alpha:\n",
            "hold on, before you kick your car let's go through some possible problems.\n",
            "\n",
            "person beta:\n",
            "ok, first of all, can you turn the key in the ignition?\n",
            "\n",
            "person alpha:\n",
            "yeah! i am here with my friend and he thinks it may be the spark plug or the starter motor.\n",
            "\n",
            "person beta:\n",
            "those are possible problems, but tell me, when you turn the key, do you hear the starter motor crank?\n",
            "\n",
            "person alpha:\n",
            "yeah, it sounds like it usually does when i start the car, but nothing else happens. the\n",
            "==========\n",
            "\u001b[1m7,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m7,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            ".\n",
            "\n",
            "person alpha:\n",
            "that is exactly the best of the stuff ever. i recommend you take the bus or the stairs.\n",
            "\n",
            "person beta:\n",
            "ok, but i am going to the railway station.\n",
            "\n",
            "person alpha:\n",
            "excuse me, can i have the ticket please?\n",
            "\n",
            "person beta:\n",
            "here you are, sir.\n",
            "\n",
            "person alpha:\n",
            "yes. can i have your ticket and ticket?\n",
            "\n",
            "person beta:\n",
            "here you are. i have just come here tomorrow.\n",
            "\n",
            "person alpha:\n",
            "i want to go to china for sightseeing.what do you need?\n",
            "\n",
            "person beta:\n",
            "i need to visit china, get in.\n",
            "\n",
            "person alpha:\n",
            "okay. could you tell me how many sightseeing spots of china would be best to see these wood spots?\n",
            "\n",
            "person beta:\n",
            "will you be able to see these pictures?\n",
            "\n",
            "person alpha:\n",
            "i want to visit some places where i wish to visit china.\n",
            "\n",
            "person beta:\n",
            "well, it all depends on your journey.\n",
            "\n",
            "person alpha:\n",
            "i want to look after the trip.\n",
            "\n",
            "person beta:\n",
            "i suggest you have been looking for a trip, if\n",
            "==========\n",
            "\u001b[1m8,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m9,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n",
            "\u001b[1m9,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            " is a chinese, american people's head on the black floor, is it?\n",
            "\n",
            "person beta:\n",
            "no, but i do like a girl with good boss.\n",
            "\n",
            "person alpha:\n",
            "well. you see, but i'm not going to be a girl with that boss. i'm like that boss that you get this.\n",
            "\n",
            "person beta:\n",
            "i don't, but i like to meet the girl with talk and tell her to dinner.\n",
            "\n",
            "person alpha:\n",
            "mmm, i went to the restaurant last night. it looks like she was.\n",
            "\n",
            "person beta:\n",
            "well, i'm so sorry, that's why i'm here. we usually go to the chinese restaurant next time. it's always is comfortable.\n",
            "\n",
            "person alpha:\n",
            "have you got anything nice drink?\n",
            "\n",
            "person beta:\n",
            "thanks.\n",
            "\n",
            "person alpha:\n",
            "and i noticed that you like it, it's very nice and cheap.\n",
            "\n",
            "person beta:\n",
            "if you like the soft sleeper it much, the movie is huge.\n",
            "\n",
            "person alpha:\n",
            "that's it. it's a kind of junk food.\n",
            "\n",
            "person beta:\n",
            "this is the right places, it's usually a bit overweight.\n",
            "\n",
            "\n",
            "==========\n",
            "\u001b[1m10,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-26-2021_t-16\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DLJJolCYQQH"
      },
      "source": [
        "save_path = join(base_dir, \n",
        "                     \"FINAL-GPT2-conv-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                        dt=get_timestamp(),\n",
        "                                                        )\n",
        "                     )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUz5m0i1pbj"
      },
      "source": [
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "ai.save(save_path)\n",
        "\n",
        "print(f'saved! {get_timestamp()}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi9g2G4hK5y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Use a Trained Model for Generation\n",
        "\n",
        "If you already had a trained model from this notebook, running the next cell will copy the `pytorch_model.bin` and the `config.json`file from the specified folder in Google Drive into the Colaboratory VM. (If no `from_folder` is specified, it assumes the two files are located at the root level of your Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeznI_VeaDQn"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alooLLyx1ZGu"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# best model thus far @ 1.3B parameters and tuned for 50k steps\n",
        "# from_folder = \"/content/drive/MyDrive/Programming/AI_peter/GPT-Neo-1B-V1\"\n",
        "\n",
        "from_folder = save_path\n",
        "\n",
        "if len(from_folder) > 2:\n",
        "\n",
        "    for file in [\"pytorch_model.bin\", \"config.json\"]:\n",
        "        if from_folder:\n",
        "            copy_file_from_gdrive(file, from_folder)\n",
        "        else:\n",
        "            copy_file_from_gdrive(file)\n",
        "\n",
        "    ai = aitextgen(model_folder=from_folder, to_gpu=True)\n",
        "else:\n",
        "    ai = aitextgen(model_folder=\".\", to_gpu=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "### Generate Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd0RGDbJiDp"
      },
      "source": [
        "`generate()` without any parameters generates a single text from the loaded model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "source": [
        "ai.generate(n=3, max_length=256, \n",
        "            temperature=1.0, top_p=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSH7QgiiGi7"
      },
      "source": [
        "ai.generate(prompt=\"these days, it always seems like \", temperature=1,\n",
        "            min_length=10, batch_size =20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = ai.generate_one()`\n",
        "\n",
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "ai.generate(\n",
        "    n=3, batch_size=25, prompt=\"i just\", max_length=256, \n",
        "    temperature=1.0, top_p=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of texts to a file and sort out the samples locally on your computer. The next cell will generate `num_files` files, each with `n` texts and whatever other parameters you would pass to `generate()`. The files can then be downloaded from the Files sidebar!\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKp18dTTj402"
      },
      "source": [
        "save_loc = join(base_dir, \"generated_text_out\")\n",
        "\n",
        "os.makedirs(save_loc, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "p_list = [[\"person alpha:\"+\"\\n\", \n",
        "           \"how are you doing?\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "          [\"person alpha:\"+\"\\n\", \n",
        "           \"hello there!\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "           [\"person alpha:\"+\"\\n\", \"why does it always seem that \"],\n",
        "           [\"person beta:\" + \"\\n\"],\n",
        "]\n",
        "\n",
        "\n",
        "prompts = [\"\".join(line) for line in p_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8et1WHilo_A"
      },
      "source": [
        "from datetime import datetime\n",
        "import pprint as pp\n",
        "\n",
        "ds_date_time = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "\n",
        "base_header = \"gpt-model-textgen-{}\".format(ds_date_time)\n",
        "prompt_IDs = [base_header + \"_file-{}.txt\".format(i+1) for i in range(5, len(prompts)+11)]\n",
        "\n",
        "prompt_mng = {}\n",
        "for pid, text in zip(prompt_IDs, prompts):\n",
        "    prompt_mng[pid] = text\n",
        "pp.pprint(prompt_mng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPnqt-UVlEPR"
      },
      "source": [
        "from os.path import join\n",
        "\n",
        "for pfile, my_prompt in prompt_mng.items():\n",
        "    ai.generate_to_file(\n",
        "        n=50,\n",
        "        batch_size=5,\n",
        "        prompt=my_prompt,\n",
        "        max_length=512,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        destination_path=join(save_loc, pfile)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDdXrPOv9DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}