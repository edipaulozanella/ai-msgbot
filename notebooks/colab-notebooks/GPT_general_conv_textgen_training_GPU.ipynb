{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT- general conv - textgen training GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NK7daXR_cAU1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf1460e3e49248988aa9a816cb16598e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7dd77958d598487cbe7b6c7766f6de15",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_177bc61ff8e0488db73736889de65b9a",
              "IPY_MODEL_e4490f14b2214728acc48931fa585d4d",
              "IPY_MODEL_41dd3be667fb4e00961c582b2a14e58c"
            ]
          }
        },
        "7dd77958d598487cbe7b6c7766f6de15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "177bc61ff8e0488db73736889de65b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_90487ac8fb3146efa46bb297d23c7884",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "replacing speaker names: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d049c1a2cfa54710982e405e05dd64bc"
          }
        },
        "e4490f14b2214728acc48931fa585d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_aec1182ec20d4e16933bd793c1eb82ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 216768,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 216768,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37b0f74dfa2948d1a324182ec3538ea1"
          }
        },
        "41dd3be667fb4e00961c582b2a14e58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7142f665b0d436589ff0e6e344b4dd5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 216768/216768 [00:00&lt;00:00, 980627.07it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b72c6a50e37d4b6b8ed356fb38b2368a"
          }
        },
        "90487ac8fb3146efa46bb297d23c7884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d049c1a2cfa54710982e405e05dd64bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aec1182ec20d4e16933bd793c1eb82ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37b0f74dfa2948d1a324182ec3538ea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d7142f665b0d436589ff0e6e344b4dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b72c6a50e37d4b6b8ed356fb38b2368a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ea12a57826d4638b831d9748bb193e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2ccc2991055423dbbcdb58de9b894c4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_297f0f631420448080d13c856bb4cf6e",
              "IPY_MODEL_65b129c84e1c42619d15a25124527720",
              "IPY_MODEL_3f55d00d58d34897a50660907685013b"
            ]
          }
        },
        "d2ccc2991055423dbbcdb58de9b894c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "297f0f631420448080d13c856bb4cf6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0646eab23df04d719343697c8481f001",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfecfe75de324565b75658941b34e032"
          }
        },
        "65b129c84e1c42619d15a25124527720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b33b428e7bb047de857748ee9fcbf590",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 216768,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 216768,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c915192241e2416b9c5a89a7095478a4"
          }
        },
        "3f55d00d58d34897a50660907685013b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ebf262336054de48ad4be33c2c7f431",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 216768/216768 [00:04&lt;00:00, 46423.18it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1a5c72dbc6548ceb6c8dde97d3fff49"
          }
        },
        "0646eab23df04d719343697c8481f001": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfecfe75de324565b75658941b34e032": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b33b428e7bb047de857748ee9fcbf590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c915192241e2416b9c5a89a7095478a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ebf262336054de48ad4be33c2c7f431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1a5c72dbc6548ceb6c8dde97d3fff49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "668fb2db15b7412f8c00582ce4a7854f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_06f5b4d245424043984543406ca4ae75",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c8a01848f0cd458fbbea3332b5546dc2",
              "IPY_MODEL_e7ef47f1be4943e9826b5546403ff796",
              "IPY_MODEL_e37a45bd29f345938bdb86ce4a7d2d22"
            ]
          }
        },
        "06f5b4d245424043984543406ca4ae75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "c8a01848f0cd458fbbea3332b5546dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_145ec49946234a0dbe6f2476c298321f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Loss: 0.473 — Avg: 0.443 — GPU Mem: 16121 MB: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_951c05c895ed4d9fb62f2a9ab267647f"
          }
        },
        "e7ef47f1be4943e9826b5546403ff796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_642cb12953474b1ab91ced711ab0a906",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6dac9377a736441ba43bfbd00221ef3c"
          }
        },
        "e37a45bd29f345938bdb86ce4a7d2d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a51016a61804fe2a370536d42955700",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [2:28:00&lt;00:00,  1.13it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a745ccacd482455da1cabb8856c4648d"
          }
        },
        "145ec49946234a0dbe6f2476c298321f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "951c05c895ed4d9fb62f2a9ab267647f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "642cb12953474b1ab91ced711ab0a906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6dac9377a736441ba43bfbd00221ef3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a51016a61804fe2a370536d42955700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a745ccacd482455da1cabb8856c4648d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/main/notebooks/colab-notebooks/GPT_general_conv_textgen_training_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  aitextgen — Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU\n",
        "\n",
        "This notebook is based on the original tutorial from `aitextgen`!\n",
        "\n",
        "- For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/).\n",
        "- for `ai-msgbot` (which is using `aitextgen` for chatbot-esque purposes) you can find the project repo [here](https://github.com/pszemraj/ai-msgbot)\n",
        "\n",
        "\n",
        "_updates made by [Peter](https://peterszemraj.ch/)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4MbTt1SwId0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhbtErR7wKxC"
      },
      "source": [
        "### GPU\n",
        "\n",
        "Colaboratory uses a Nvidia K80, an Nvidia P100, an Nvidia V100, or Nvidia A100. For finetuning GPT-2 124M, any of these GPUs will be fine, but for text generation, a k80 or a P100 is ideal since they have more VRAM. \n",
        "\n",
        "- In theory: **If you receive a T4 or a V100 GPU, you can enable `fp16=True` during training for faster/more memory efficient training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgMEhF7J3ubw",
        "outputId": "ec78255b-41c7-41db-f6aa-d0638b995ba9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  2 18:30:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "### formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemDfmdgjefZ"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "_UJ8ek31Uz-r",
        "outputId": "b94063a5-9a92-47c4-ac64-ada434e9c322"
      },
      "source": [
        "\n",
        "!pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -q\n",
        "!pip install https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.71+cuda111-cp37-none-manylinux2010_x86_64.whl -q\n",
        "\n",
        "# see this issue https://github.com/googlecolab/colabtools/issues/2452 for colab A100 GPU"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |██████████████▋                 | 834.1 MB 1.3 MB/s eta 0:12:40tcmalloc: large alloc 1147494400 bytes == 0x55b8465ee000 @  0x7fe33e50c615 0x55b80cc864cc 0x55b80cd6647a 0x55b80cc892ed 0x55b80cd7ae1d 0x55b80ccfce99 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccfcd00 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cc8b039 0x55b80ccce409 0x55b80cc89c52 0x55b80ccfcc25 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8915 0x55b80cc8aafa 0x55b80ccf8c0d 0x55b80ccf79ee\n",
            "\u001b[K     |██████████████████▌             | 1055.7 MB 1.3 MB/s eta 0:09:57tcmalloc: large alloc 1434370048 bytes == 0x55b88ac44000 @  0x7fe33e50c615 0x55b80cc864cc 0x55b80cd6647a 0x55b80cc892ed 0x55b80cd7ae1d 0x55b80ccfce99 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccfcd00 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cc8b039 0x55b80ccce409 0x55b80cc89c52 0x55b80ccfcc25 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8915 0x55b80cc8aafa 0x55b80ccf8c0d 0x55b80ccf79ee\n",
            "\u001b[K     |███████████████████████▌        | 1336.2 MB 1.3 MB/s eta 0:06:26tcmalloc: large alloc 1792966656 bytes == 0x55b80fa76000 @  0x7fe33e50c615 0x55b80cc864cc 0x55b80cd6647a 0x55b80cc892ed 0x55b80cd7ae1d 0x55b80ccfce99 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccfcd00 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cc8b039 0x55b80ccce409 0x55b80cc89c52 0x55b80ccfcc25 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8915 0x55b80cc8aafa 0x55b80ccf8c0d 0x55b80ccf79ee\n",
            "\u001b[K     |█████████████████████████████▊  | 1691.1 MB 1.2 MB/s eta 0:01:48tcmalloc: large alloc 2241208320 bytes == 0x55b87a85e000 @  0x7fe33e50c615 0x55b80cc864cc 0x55b80cd6647a 0x55b80cc892ed 0x55b80cd7ae1d 0x55b80ccfce99 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccfcd00 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cd7bc66 0x55b80ccf8daf 0x55b80cc8b039 0x55b80ccce409 0x55b80cc89c52 0x55b80ccfcc25 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8915 0x55b80cc8aafa 0x55b80ccf8c0d 0x55b80ccf79ee\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 1821458432 bytes == 0x55b9001c0000 @  0x7fe33e50b1e7 0x55b80ccbc067 0x55b80cc864cc 0x55b80cd6647a 0x55b80cc892ed 0x55b80cd7ae1d 0x55b80ccfce99 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80cc8aafa 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee\n",
            "tcmalloc: large alloc 2276827136 bytes == 0x55b96cad4000 @  0x7fe33e50c615 0x55b80cc864cc 0x55b80cd6647a 0x55b80cc892ed 0x55b80cd7ae1d 0x55b80ccfce99 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf8c0d 0x55b80cc8aafa 0x55b80ccf8c0d 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee 0x55b80cc8abda 0x55b80ccf9737 0x55b80ccf79ee 0x55b80cc8b271\n",
            "\u001b[K     |████████████████████████████████| 1821.5 MB 5.9 kB/s \n",
            "\u001b[K     |████████████████████████████████| 24.6 MB 70.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 197.3 MB 77 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "fceb972f-c6b3-4aec-f4ec-aa6203368932"
      },
      "source": [
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 572 kB 14.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 63.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 524 kB 59.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 329 kB 65.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 61.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 65.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 57.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 625 kB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 53.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 71.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 71.9 MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DuC67no59vMe",
        "outputId": "50184d9e-b3a9-4d20-e846-9ffe0dafd8ef"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "### Loading GPT-2 or GPT Neo\n",
        "\n",
        "\n",
        "- A common use case is *continuing* to fine-tune a model that was originally pretrained, and then fine-tuned a little bit, but needs to be fine-tuned more for accuracy/saliency reasons or because Google cut off the runtime earlier. \n",
        "    - in this case, `load_from_folder` should be set to `True` and `load_folder_dir` points to where the model checkpoint is on your google drive.\n",
        "- **the below section describes loading an new/pretrained model from the original tutorial.**\n",
        "\n",
        "> If you're retraining a model on new text, you need to download and load the GPT-2 model into the GPU. \n",
        "\n",
        "> There are several sizes of GPT-2:\n",
        "\n",
        "    * `124M` (default): the \"small\" model, 500MB on disk.\n",
        "    * `355M` (default): the \"medium\" model, 1.5GB on disk.\n",
        "    * `774M` (default): the \"large\" model, 3GB on disk.\n",
        "\n",
        "> You can also finetune a GPT Neo model instead ([_or any textgen GPT-architecture model on huggingface_](https://huggingface.co/models?pipeline_tag=text-generation)), which is more suitable for longer texts and the base model has more recent data:\n",
        "\n",
        "* `125M`: Analogous to the GPT-2 124M model. (355M parameter model was removed)\n",
        "*  `EleutherAI/gpt-neo-1.3B` : 1.3 billion parameter model. Have yet to see this train on Colab without crashing\n",
        "\n",
        "> The next cell downloads the model and saves it in the Colaboratory VM. If the model has already been downloaded, running this cell will reload it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPPsg8XDwHvl"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXIbld-ASHL9",
        "outputId": "0d57a26b-7278-479a-bbac-d01a372b655d"
      },
      "source": [
        "model_size = \"355M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = False #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Programming/ai-msgbot/YOUR_SAVED_MODEL\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "flqSlHjMIeIw",
        "outputId": "ece306a4-94e0-4768-c5c4-1c86c29c5dfc"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "else:\n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "# Comment out the above line and uncomment the below line to use GPT Neo instead.\n",
        "# ai = aitextgen(model='distilgpt2', \n",
        "#                to_gpu=True, \n",
        "#                gradient_checkpointing=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/02/2021 18:41:12 — INFO — aitextgen — Loading model from provided weights and config in //content/drive/MyDrive/Programming/ai-msgbot/GPT2_774M_chk_175KnatQAtrivDD_100kWoW.\n",
            "12/02/2021 18:41:50 — INFO — aitextgen — GPT2 loaded with 774M parameters.\n",
            "12/02/2021 18:41:50 — INFO — aitextgen — Gradient checkpointing enabled for model training.\n",
            "12/02/2021 18:41:50 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7oQAqQHiFiX"
      },
      "source": [
        "### load training data\n",
        "\n",
        "\n",
        "- <font color=\"orange\"> combine any other data from the a \"standard\" conversational dataset and do a final pass of training (with different data) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jUWsiVOCUlLo",
        "outputId": "2fc0c0b3-f670-4157-c31e-72aab94ff778"
      },
      "source": [
        "dl_link = \"https://github.com/pszemraj/ai-msgbot/raw/main/conversation-data/Daily-Dialogues/daily_dialogue_augment.txt\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "hirJkLvbUrMb",
        "outputId": "0d9fe86b-570f-482d-c54c-53c21cb715f0"
      },
      "source": [
        "# download test image\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import os\n",
        "vm_wd = os.getcwd()\n",
        "local_name = join(vm_wd, \"training_script.txt\")\n",
        "request.urlretrieve(dl_link, local_name)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/training_script.txt', <http.client.HTTPMessage at 0x7f8395bd2a90>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cEWZnWyYX3y8",
        "outputId": "045caf9e-78b8-4e3c-8742-099c0649f43c"
      },
      "source": [
        "# adjust names in script if needed  \n",
        "import pprint as pp\n",
        "from os.path import basename\n",
        "\n",
        "def update_script_names(local_name, spkr_from=\"speaker a\", \n",
        "                        spkr_to=\"person alpha\",\n",
        "                        resp_from=\"speaker b\", resp_to=\"person beta\",\n",
        "                        verbose=False):\n",
        "    \"\"\"\n",
        "    update_script_names - if the textfile script has different names for the \n",
        "    speaker/responder than desired (i.e. it is a group conversation, and the \n",
        "    chatbot is just supposed to simulate 1:1) this function can be used to \n",
        "    standardize\n",
        "    \"\"\"\n",
        "\n",
        "    with open(local_name, 'r', encoding='utf-8', errors='ignore') as fi:\n",
        "        orig_lines = fi.readlines()\n",
        "\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    upd_lines = []\n",
        "\n",
        "    for line in tqdm(orig_lines, total=len(orig_lines), \n",
        "                     desc=\"replacing speaker names\"):\n",
        "        \n",
        "        fixline = line.replace(spkr_from, spkr_to)\n",
        "        fixline = fixline.replace(resp_from, resp_to)\n",
        "        upd_lines.append(fixline)\n",
        "\n",
        "    local_namev2 = join(vm_wd, \"V2-rename-\" + basename(local_name))\n",
        "\n",
        "    with open(local_namev2, 'w', encoding='utf-8', errors='ignore') as fo:\n",
        "        fo.writelines(upd_lines)\n",
        "\n",
        "    if verbose: pp.pprint(upd_lines[:10])\n",
        "    # return filepath\n",
        "    return local_namev2\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bf1460e3e49248988aa9a816cb16598e",
            "7dd77958d598487cbe7b6c7766f6de15",
            "177bc61ff8e0488db73736889de65b9a",
            "e4490f14b2214728acc48931fa585d4d",
            "41dd3be667fb4e00961c582b2a14e58c",
            "90487ac8fb3146efa46bb297d23c7884",
            "d049c1a2cfa54710982e405e05dd64bc",
            "aec1182ec20d4e16933bd793c1eb82ee",
            "37b0f74dfa2948d1a324182ec3538ea1",
            "d7142f665b0d436589ff0e6e344b4dd5",
            "b72c6a50e37d4b6b8ed356fb38b2368a"
          ]
        },
        "id": "6OFnPCLADfll",
        "outputId": "23b1a356-75e6-4c81-cb2b-550e95ab1558"
      },
      "source": [
        "local_name = update_script_names(local_name)\n",
        "\n",
        "file_name = local_name # update if using fn above"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf1460e3e49248988aa9a816cb16598e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "replacing speaker names:   0%|          | 0/216768 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Train / Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "REDm2euJ1kHB",
        "outputId": "01a0200d-a203-4e43-ab01-bf4c35c4480b"
      },
      "source": [
        "base_dir = \"/content/drive/MyDrive/Programming/ai-msgbot\" #@param {type:\"string\"}\n",
        "# update to yours"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sDmyL2A7uhaL",
        "outputId": "fecc9943-d7b3-4319-b843-26e1f77d38f3"
      },
      "source": [
        "import gc, os\n",
        "from os.path import join\n",
        "from datetime import datetime\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime(\"%b-%d-%Y_t-%H\")\n",
        "\n",
        "temp_gpu_path = join(base_dir, \n",
        "                     \"GPT2-conversational-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                            dt=get_timestamp(),\n",
        "                                                            )\n",
        "                     )\n",
        "os.makedirs(temp_gpu_path, exist_ok=True)\n",
        "gc.collect()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzDXk_iGn-wF"
      },
      "source": [
        "### example outputs\n",
        "\n",
        "- this cell had its outputs cleared before notebook was posted.\n",
        "- the below is an example of what the outputs during training should look like.\n",
        "\n",
        "```\n",
        "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1819: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
        "  \"`trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7.\"\n",
        "1,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-25-2021_t-04\n",
        "1,500 steps reached: generating sample texts.\n",
        "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
        "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
        "==========\n",
        "\n",
        "\n",
        "person alpha:\n",
        "do you enjoy the movie?\n",
        "\n",
        "person beta:\n",
        "yes, i love the film is based in george r. r. martin scotland\n",
        "\n",
        "person alpha:\n",
        "oh cool, how does it get to be one?\n",
        "\n",
        "person beta:\n",
        "well it was a break between a single.\n",
        "\n",
        "person alpha:\n",
        "i really like the show but i don't know much about it, i do not know much about it\n",
        "\n",
        "person beta:\n",
        "i know there are many episodes for the show, and the show has to be the highest grosss in the show though.\n",
        "\n",
        "person alpha:\n",
        "i've been a new game of thrones, or twice a year\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1pdV5lovKg"
      },
      "source": [
        "### t r a i n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7ea12a57826d4638b831d9748bb193e7",
            "d2ccc2991055423dbbcdb58de9b894c4",
            "297f0f631420448080d13c856bb4cf6e",
            "65b129c84e1c42619d15a25124527720",
            "3f55d00d58d34897a50660907685013b",
            "0646eab23df04d719343697c8481f001",
            "cfecfe75de324565b75658941b34e032",
            "b33b428e7bb047de857748ee9fcbf590",
            "c915192241e2416b9c5a89a7095478a4",
            "1ebf262336054de48ad4be33c2c7f431",
            "c1a5c72dbc6548ceb6c8dde97d3fff49",
            "668fb2db15b7412f8c00582ce4a7854f",
            "06f5b4d245424043984543406ca4ae75",
            "c8a01848f0cd458fbbea3332b5546dc2",
            "e7ef47f1be4943e9826b5546403ff796",
            "e37a45bd29f345938bdb86ce4a7d2d22",
            "145ec49946234a0dbe6f2476c298321f",
            "951c05c895ed4d9fb62f2a9ab267647f",
            "642cb12953474b1ab91ced711ab0a906",
            "6dac9377a736441ba43bfbd00221ef3c",
            "2a51016a61804fe2a370536d42955700",
            "a745ccacd482455da1cabb8856c4648d"
          ]
        },
        "id": "aeXshJM-Cuaf",
        "outputId": "5a052f9d-5c21-44fa-cd16-a0e6b8d63b72"
      },
      "source": [
        "# DO NOT USE WARMUP STEPS\n",
        "\n",
        "ai.train(\n",
        "            file_name, # text file with training data\n",
        "            output_dir=temp_gpu_path, # where it saves during \"save_every\"\n",
        "            line_by_line=False, # if using CSV file input\n",
        "            from_cache=False,\n",
        "            num_steps=10000, # takes about 5 hours on 16 gb v100 GPU for 75000\n",
        "            generate_every=500,\n",
        "            max_grad_norm=0.5,\n",
        "            save_every=500,\n",
        "            gradient_accumulation_steps=1,\n",
        "            save_gdrive=False, # this is an \"automated\" save which is worse than current method (IMO)\n",
        "            learning_rate=1e-3,\n",
        "        #  fp16=True, # may be relevant to set to false (even if available) for \"final\" training\n",
        "            batch_size=1, # if pushing model_size you probably want to leave this at 1\n",
        "        #  fp16_opt_level=\"O2\", # different types of FP16 are possible\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/02/2021 18:42:04 — INFO — aitextgen — Loading text from /content/V2-rename-training_script.txt with generation length of 1024.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ea12a57826d4638b831d9748bb193e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/216768 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/02/2021 18:42:04 — INFO — aitextgen.TokenDataset — Encoding 216,768 sets of tokens from /content/V2-rename-training_script.txt.\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:148: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.\n",
            "  f\"Setting `Trainer(checkpoint_callback={checkpoint_callback})` is deprecated in v1.5 and will \"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:91: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:168: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.\n",
            "  \"Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed\"\n",
            "12/02/2021 18:42:09 — INFO — pytorch_lightning.utilities.distributed — GPU available: True, used: True\n",
            "12/02/2021 18:42:09 — INFO — pytorch_lightning.utilities.distributed — TPU available: False, using: 0 TPU cores\n",
            "12/02/2021 18:42:09 — INFO — pytorch_lightning.utilities.distributed — IPU available: False, using: 0 IPUs\n",
            "12/02/2021 18:42:09 — INFO — pytorch_lightning.accelerators.gpu — LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "668fb2db15b7412f8c00582ce4a7854f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1819: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
            "  \"`trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7.\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m1,500 steps reached: generating sample texts.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========\n",
            ". i have made an early in the morning. i'll have to go over the weekend.\n",
            "\n",
            "person beta:\n",
            "you know how to use a little bit, and then, it's still too far away.\n",
            "\n",
            "person alpha:\n",
            "good idea! i'll see you tomorrow night tomorrow night tomorrow night.\n",
            "\n",
            "person beta:\n",
            "see you then!\n",
            "\n",
            "person alpha:\n",
            "i am sorry, i'm going to have to pay my time off my room.\n",
            "\n",
            "person beta:\n",
            "that's ok. do you have any furnished boxes available for room tonight?\n",
            "\n",
            "person alpha:\n",
            "i don't have one for a room, so it's just very formal.\n",
            "\n",
            "person beta:\n",
            "how about that?\n",
            "\n",
            "person alpha:\n",
            "well, the sky is open now.\n",
            "\n",
            "person beta:\n",
            "i would like to buy two exquisite watches for tonight.\n",
            "\n",
            "person alpha:\n",
            "ok. you'll have it tomorrow night!\n",
            "\n",
            "person beta:\n",
            "i'd love it.\n",
            "\n",
            "person alpha:\n",
            "yes, what time would you like your table?\n",
            "\n",
            "person beta:\n",
            "at 7:30 pm. do you have a good night camera?\n",
            "\n",
            "person alpha:\n",
            "sorry, i don't have\n",
            "==========\n",
            "\u001b[1m2,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m3,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            " to keep a neck to london.\n",
            "\n",
            "person alpha:\n",
            "you are suspected of a bit. do you have a problem with you?\n",
            "\n",
            "person beta:\n",
            "of course, i do. when i came too, i just moved out.\n",
            "\n",
            "person alpha:\n",
            "so did i tell you when you saw the movie?\n",
            "\n",
            "person beta:\n",
            "i saw the movie. i saw that movie at the theater in the theater.\n",
            "\n",
            "person alpha:\n",
            "yes, i heard that movie. would you like to come in with me to check the kitchen?\n",
            "\n",
            "person beta:\n",
            "yes, of course. what did you want to see?\n",
            "\n",
            "person alpha:\n",
            "the movie sounds too good. did you want to go with me tomorrow?\n",
            "\n",
            "person beta:\n",
            "yes, i did. i want to go to the movies.\n",
            "\n",
            "person alpha:\n",
            "the movie sounds like fun. what did you see?\n",
            "\n",
            "person beta:\n",
            "i wanted to go camping, and i want to buy some records.\n",
            "\n",
            "person alpha:\n",
            "whose? what are you going to do tonight?\n",
            "\n",
            "person beta:\n",
            "i'm going to my hotel to beijing.\n",
            "\n",
            "person alpha:\n",
            "i'm going to stay in\n",
            "==========\n",
            "\u001b[1m4,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m4,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            " a for you, you have to check the signs to the world affairs on every one.\n",
            "\n",
            "person alpha:\n",
            "is this the right counter for having an increase in speed?\n",
            "\n",
            "person beta:\n",
            "yes, it's quite a bit. i'll get on next step in about 3 to three days.\n",
            "\n",
            "person alpha:\n",
            "do you think i should take further exam for this major?\n",
            "\n",
            "person beta:\n",
            "no, i'll be free.\n",
            "\n",
            "person alpha:\n",
            "i'm checking out the application form.\n",
            "\n",
            "person beta:\n",
            "excuse me, driver, you have kept us waiting for 15 minutes. can't we start the tour right now? we'll have to drive to the gate east.\n",
            "\n",
            "person alpha:\n",
            "i'm sorry, sir. i think we can't get to the bottom of the car.\n",
            "\n",
            "person beta:\n",
            "don't worry about it, i lost.\n",
            "\n",
            "person alpha:\n",
            "yes, i'm afraid we are on the high way.\n",
            "\n",
            "person beta:\n",
            "the car's packed against the clutch.\n",
            "\n",
            "person alpha:\n",
            "which way is this?\n",
            "\n",
            "person beta:\n",
            "i'm not sure.let's return on a train.\n",
            "\n",
            "person alpha:\n",
            "\n",
            "==========\n",
            "\u001b[1m5,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m6,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m6,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            ", why don't you try to see your doctor?\n",
            "\n",
            "person alpha:\n",
            "okay. i will go ahead.\n",
            "\n",
            "person beta:\n",
            "boring? isn't that a chain restaurant?\n",
            "\n",
            "person alpha:\n",
            "yes, i'm american. what can i do for you?\n",
            "\n",
            "person beta:\n",
            "i'm from american. here is my card.\n",
            "\n",
            "person alpha:\n",
            "can you please give me some details?\n",
            "\n",
            "person beta:\n",
            "yes, you can. i'm looking for a pair so i can't find it now.\n",
            "\n",
            "person alpha:\n",
            "do you a second hand now?\n",
            "\n",
            "person beta:\n",
            "sure, can you tell me how to make a reservation this evening?\n",
            "\n",
            "person alpha:\n",
            "yes, we can do that. we can take a reservation at the same time.\n",
            "\n",
            "person beta:\n",
            "i understand. i hope you can find it there.\n",
            "\n",
            "person alpha:\n",
            "the waitresses next time are on the 9th floor.\n",
            "\n",
            "person beta:\n",
            "yes, i see.\n",
            "\n",
            "person alpha:\n",
            "what a wonderful day! the sun is on the thirteenth of the month.\n",
            "\n",
            "person beta:\n",
            "i'm so glad that this is\n",
            "==========\n",
            "\u001b[1m7,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m7,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            ", i want to do some sit-up in the kitchen, long-term working conditions.\n",
            "\n",
            "person beta:\n",
            "may i see your doctor, please?\n",
            "\n",
            "person alpha:\n",
            "of course, i have this this afternoon.\n",
            "\n",
            "person beta:\n",
            "what's it, dear?\n",
            "\n",
            "person alpha:\n",
            "it's ready to go.\n",
            "\n",
            "person beta:\n",
            "then what do you think of the fine cap? do i look good or what?\n",
            "\n",
            "person alpha:\n",
            "i look at myself in the ear, i just have a couple of final love up there.\n",
            "\n",
            "person beta:\n",
            "i know what you mean. it sounds ideal, but i was looking for an awful burglar.\n",
            "\n",
            "person alpha:\n",
            "it is not only that there's nothing left.\n",
            "\n",
            "person beta:\n",
            "do you have an outside view?\n",
            "\n",
            "person alpha:\n",
            "i guess that's my favorite.\n",
            "\n",
            "person beta:\n",
            "when did you begin writing?\n",
            "\n",
            "person alpha:\n",
            "i started writing margin articles for fun, and eventually was asked to write a book, so i was pretty lucky.\n",
            "\n",
            "person beta:\n",
            "are you writing a book now?\n",
            "\n",
            "person alpha:\n",
            "yes, it's about a group\n",
            "==========\n",
            "\u001b[1m8,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m9,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n",
            "\u001b[1m9,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            " you better.\n",
            "\n",
            "person beta:\n",
            "that is very kind of you, mary's father is responsible for the entire wedding ceremony. it looks like she is going to be barbara's birthday soon.\n",
            "\n",
            "person alpha:\n",
            "maybe. what about doing over there?\n",
            "\n",
            "person beta:\n",
            "i am going to church with my parents, is that right?\n",
            "\n",
            "person alpha:\n",
            "i guess it is a holiday present to the united states, we should take on more clothes!\n",
            "\n",
            "person beta:\n",
            "what do they think of all the universe?\n",
            "\n",
            "person alpha:\n",
            "they need to set up tables, read them, from cosmetics to another and ready-made ones, and they're pretty strict. we also set up preparing when the procedures arrives.\n",
            "\n",
            "person beta:\n",
            "wow, that's pretty much the same! hey, and richreds are such a lovely child!\n",
            "\n",
            "person alpha:\n",
            "i like the joke. hey that means we can make on display.\n",
            "\n",
            "person beta:\n",
            "that's a great idea. i'm all year't know and all the sizes will begin to paint all.\n",
            "\n",
            "person alpha:\n",
            "are you going to take a job?\n",
            "\n",
            "person beta:\n",
            "yes, i\n",
            "==========\n",
            "\u001b[1m10,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Dec-02-2021_t-18\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DLJJolCYQQH"
      },
      "source": [
        "from os.path import join\n",
        "save_path = join(base_dir, \n",
        "                     \"FINAL-GPT2-conv-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                        dt=get_timestamp(),\n",
        "                                                        )\n",
        "                     )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUz5m0i1pbj"
      },
      "source": [
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "ai.save(save_path)\n",
        "\n",
        "print(f'saved! {get_timestamp()}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi9g2G4hK5y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Use a Trained Model for Generation\n",
        "\n",
        "If you already had a trained model from this notebook, running the next cell will copy the `pytorch_model.bin` and the `config.json`file from the specified folder in Google Drive into the Colaboratory VM. (If no `from_folder` is specified, it assumes the two files are located at the root level of your Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeznI_VeaDQn"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alooLLyx1ZGu"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# best model thus far @ 1.3B parameters and tuned for 50k steps\n",
        "# from_folder = \"/content/drive/MyDrive/Programming/AI_peter/GPT-Neo-1B-V1\"\n",
        "\n",
        "from_folder = save_path\n",
        "\n",
        "if len(from_folder) > 2:\n",
        "\n",
        "    for file in [\"pytorch_model.bin\", \"config.json\"]:\n",
        "        if from_folder:\n",
        "            copy_file_from_gdrive(file, from_folder)\n",
        "        else:\n",
        "            copy_file_from_gdrive(file)\n",
        "\n",
        "    ai = aitextgen(model_folder=from_folder, to_gpu=True)\n",
        "else:\n",
        "    ai = aitextgen(model_folder=\".\", to_gpu=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "### Generate Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd0RGDbJiDp"
      },
      "source": [
        "`generate()` without any parameters generates a single text from the loaded model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "source": [
        "ai.generate(n=3, max_length=256, \n",
        "            temperature=1.0, top_p=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSH7QgiiGi7"
      },
      "source": [
        "ai.generate(prompt=\"these days, it always seems like \", temperature=1,\n",
        "            min_length=10, batch_size =20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = ai.generate_one()`\n",
        "\n",
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "ai.generate(\n",
        "    n=3, batch_size=25, prompt=\"i just\", max_length=256, \n",
        "    temperature=1.0, top_p=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of texts to a file and sort out the samples locally on your computer. The next cell will generate `num_files` files, each with `n` texts and whatever other parameters you would pass to `generate()`. The files can then be downloaded from the Files sidebar!\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKp18dTTj402"
      },
      "source": [
        "save_loc = join(base_dir, \"generated_text_out\")\n",
        "\n",
        "os.makedirs(save_loc, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "p_list = [[\"person alpha:\"+\"\\n\", \n",
        "           \"how are you doing?\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "          [\"person alpha:\"+\"\\n\", \n",
        "           \"hello there!\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "           [\"person alpha:\"+\"\\n\", \"why does it always seem that \"],\n",
        "           [\"person beta:\" + \"\\n\"],\n",
        "]\n",
        "\n",
        "\n",
        "prompts = [\"\".join(line) for line in p_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8et1WHilo_A"
      },
      "source": [
        "from datetime import datetime\n",
        "import pprint as pp\n",
        "\n",
        "ds_date_time = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "\n",
        "base_header = \"gpt-model-textgen-{}\".format(ds_date_time)\n",
        "prompt_IDs = [base_header + \"_file-{}.txt\".format(i+1) for i in range(5, len(prompts)+11)]\n",
        "\n",
        "prompt_mng = {}\n",
        "for pid, text in zip(prompt_IDs, prompts):\n",
        "    prompt_mng[pid] = text\n",
        "pp.pprint(prompt_mng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPnqt-UVlEPR"
      },
      "source": [
        "from os.path import join\n",
        "\n",
        "for pfile, my_prompt in prompt_mng.items():\n",
        "    ai.generate_to_file(\n",
        "        n=50,\n",
        "        batch_size=5,\n",
        "        prompt=my_prompt,\n",
        "        max_length=512,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        destination_path=join(save_loc, pfile)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDdXrPOv9DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}