{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT2- general conv - textgen training GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/main/colab-notebooks/GPT2_general_conv_textgen_training_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  aitextgen — Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU\n",
        "\n",
        "This notebook is based on the original tutorial from `aitextgen`!\n",
        "\n",
        "- For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/).\n",
        "- for `ai-msgbot` (which is using `aitextgen` for chatbot-esque purposes) you can find the project repo [here](https://github.com/pszemraj/ai-msgbot)\n",
        "\n",
        "\n",
        "_updates made by [Peter](https://peterszemraj.ch/)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4MbTt1SwId0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhbtErR7wKxC"
      },
      "source": [
        "### GPU\n",
        "\n",
        "Colaboratory uses a Nvidia K80, an Nvidia P100, an Nvidia V100, or Nvidia A100. For finetuning GPT-2 124M, any of these GPUs will be fine, but for text generation, a k80 or a P100 is ideal since they have more VRAM. \n",
        "\n",
        "- In theory: **If you receive a T4 or a V100 GPU, you can enable `fp16=True` during training for faster/more memory efficient training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgMEhF7J3ubw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989db2e9-ed24-4ee8-a29e-be796f0ee423"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 24 11:22:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "### formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemDfmdgjefZ"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UJ8ek31Uz-r"
      },
      "source": [
        "# update torch in case using a A100 GPU\n",
        "# may take 5+ minutes\n",
        "!pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_"
      },
      "source": [
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuC67no59vMe"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "### Loading GPT-2 or GPT Neo\n",
        "\n",
        "\n",
        "- A common use case is *continuing* to fine-tune a model that was originally pretrained, and then fine-tuned a little bit, but needs to be fine-tuned more for accuracy/saliency reasons or because Google cut off the runtime earlier. \n",
        "    - in this case, `load_from_folder` should be set to `True` and `load_folder_dir` points to where the model checkpoint is on your google drive.\n",
        "- **the below section describes loading an new/pretrained model from the original tutorial.**\n",
        "\n",
        "> If you're retraining a model on new text, you need to download and load the GPT-2 model into the GPU. \n",
        "\n",
        "> There are several sizes of GPT-2:\n",
        "\n",
        "    * `124M` (default): the \"small\" model, 500MB on disk.\n",
        "    * `355M` (default): the \"medium\" model, 1.5GB on disk.\n",
        "    * `774M` (default): the \"large\" model, 3GB on disk.\n",
        "\n",
        "> You can also finetune a GPT Neo model instead ([_or any textgen GPT-architecture model on huggingface_](https://huggingface.co/models?pipeline_tag=text-generation)), which is more suitable for longer texts and the base model has more recent data:\n",
        "\n",
        "* `125M`: Analogous to the GPT-2 124M model. (355M parameter model was removed)\n",
        "*  `EleutherAI/gpt-neo-1.3B` : 1.3 billion parameter model. Have yet to see this train on Colab without crashing\n",
        "\n",
        "> The next cell downloads the model and saves it in the Colaboratory VM. If the model has already been downloaded, running this cell will reload it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPPsg8XDwHvl"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AXIbld-ASHL9"
      },
      "source": [
        "model_size = \"774M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = True #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Programming/AI_peter/gpt2-25KtriviaQA100KnaturalQA50Kdailydialogues-774M\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flqSlHjMIeIw"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "else:\n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "# Comment out the above line and uncomment the below line to use GPT Neo instead.\n",
        "# ai = aitextgen(model='distilgpt2', \n",
        "#                to_gpu=True, \n",
        "#                gradient_checkpointing=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7oQAqQHiFiX"
      },
      "source": [
        "### load training data\n",
        "\n",
        "\n",
        "- <font color=\"orange\"> combine any other data from the a \"standard\" conversational dataset and do a final pass of training (with different data) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jUWsiVOCUlLo"
      },
      "source": [
        "dl_link = \"https://github.com/pszemraj/ai-msgbot/raw/main/conversation-data/wizard-of-wikipedia/ScriptParse-wow-train-kilt.txt\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hirJkLvbUrMb"
      },
      "source": [
        "# download test image\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import os\n",
        "vm_wd = os.getcwd()\n",
        "local_name = join(vm_wd, \"training_script.txt\")\n",
        "request.urlretrieve(dl_link, local_name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEWZnWyYX3y8"
      },
      "source": [
        "# adjust names in script if needed  \n",
        "import pprint as pp\n",
        "from os.path import basename\n",
        "\n",
        "def update_script_names(local_name, spkr_from=\"speaker a\", spkr_to=\"person beta\",\n",
        "                        resp_from=\"speaker b\", resp_to=\"nancy sellers\",\n",
        "                        verbose=False):\n",
        "    \"\"\"\n",
        "    update_script_names - if the textfile script has different names for the \n",
        "    speaker/responder than desired (i.e. it is a group conversation, and the \n",
        "    chatbot is just supposed to simulate 1:1) this function can be used to \n",
        "    standardize\n",
        "    \"\"\"\n",
        "\n",
        "    with open(local_name, 'r', encoding='utf-8', errors='ignore') as fi:\n",
        "        orig_lines = fi.readlines()\n",
        "\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    upd_lines = []\n",
        "\n",
        "    for line in tqdm(orig_lines, total=len(orig_lines), \n",
        "                     desc=\"replacing speaker names\"):\n",
        "        \n",
        "        fixline = line.replace(spkr_from, spkr_to)\n",
        "        fixline = fixline.replace(resp_from, resp_to)\n",
        "        upd_lines.append(fixline)\n",
        "\n",
        "    local_namev2 = join(vm_wd, \"V2-rename-\" + basename(local_name))\n",
        "\n",
        "    with open(local_namev2, 'w', encoding='utf-8', errors='ignore') as fo:\n",
        "        fo.writelines(upd_lines)\n",
        "\n",
        "    if verbose: pp.pprint(upd_lines[:10])\n",
        "    # return filepath\n",
        "    return local_namev2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "file_name = local_name # update if using fn above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Train / Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "REDm2euJ1kHB"
      },
      "source": [
        "base_dir = \"/content/drive/MyDrive/Programming/ai-msgbot\" #@param {type:\"string\"}\n",
        "# update to yours"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDmyL2A7uhaL"
      },
      "source": [
        "import gc, os\n",
        "from os.path import join\n",
        "from datetime import datetime\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime(\"%b-%d-%Y_t-%H\")\n",
        "\n",
        "temp_gpu_path = join(base_dir, \n",
        "                     \"GPT2-conversational-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                            dt=get_timestamp(),\n",
        "                                                            )\n",
        "                     )\n",
        "os.makedirs(temp_gpu_path, exist_ok=True)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzDXk_iGn-wF"
      },
      "source": [
        "### example outputs\n",
        "\n",
        "- this cell had its outputs cleared before notebook was posted.\n",
        "- the below is an example of what the outputs during training should look like.\n",
        "\n",
        "```\n",
        "1,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-355M-Nov-23-2021_t-04\n",
        "1,500 steps reached: generating sample texts.\n",
        "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
        "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
        "==========\n",
        ".\n",
        "\n",
        "person alpha:\n",
        "i love to read. reading. reading requires creativity. it's one of the intricacies of decoding symbols and improving knowledge, sharing information through wires and wires.\n",
        "is reading still a good thing to read?\n",
        "not really. reading software allows creativity, refinement and development. it's also a great tool for sharing information and sharing information over the past.\n",
        "can you give example of reading strategies?\n",
        "\n",
        "person beta:\n",
        "its is. reading produces creativity. it's my favorite form of reading, it's used on high shelfs.\n",
        "\n",
        "person alpha:\n",
        "do you like to eat at a restaurant? i'm a big fan of cooking and watching my favorite meal.\n",
        "i love cooking, it is the combination of grilling over an open fire.\n",
        "i love cooking, it is so good! it's my favorite meal of the day. i like to add to my food in my house.\n",
        "it is a great meal too, i like to add many different ways to cooking.\n",
        "i like to add different ways to cooking.\n",
        "i agree, it is the most common type of cooking.\n",
        "\n",
        "person beta:\n",
        "i like to add different techniques and ingredients and ingredients.\n",
        "\n",
        "person alpha:\n",
        "\n",
        "==========\n",
        "2,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-355M-Nov-23-2021_t-04\n",
        "3,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-355M-Nov-23-2021_t-04\n",
        "3,000 steps reached: generating sample texts.\n",
        "==========\n",
        "\n",
        "\n",
        "person alpha:\n",
        "i love to cook. cooking is a great art. you can bake!\n",
        "i love cooking too. cooking as a whole is an art.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1pdV5lovKg"
      },
      "source": [
        "### t r a i n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf"
      },
      "source": [
        "# DO NOT USE WARMUP STEPS\n",
        "\n",
        "ai.train(file_name,\n",
        "         output_dir=temp_gpu_path, # where it saves during \"save_every\"\n",
        "         line_by_line=False, # if using CSV file input\n",
        "         from_cache=False,\n",
        "         num_steps=100000, # takes about 5 hours on 16 gb v100 GPU for 75000\n",
        "         generate_every=1500,\n",
        "         max_grad_norm=0.5,\n",
        "         save_every=1000,\n",
        "         gradient_accumulation_steps=1,\n",
        "         save_gdrive=False, # this is an \"automated\" save which is worse than current method (IMO)\n",
        "         learning_rate=1e-3,\n",
        "        #  fp16=True, # may be relevant to set to false (even if available) for \"final\" training\n",
        "         batch_size=1, # if pushing model_size you probably want to leave this at 1\n",
        "        #  fp16_opt_level=\"O2\", # different types of FP16 are possible\n",
        "        #  amp_backend='apex'\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DLJJolCYQQH"
      },
      "source": [
        "save_path = join(base_dir, \n",
        "                     \"FINAL-GPT2-conv-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                        dt=get_timestamp(),\n",
        "                                                        )\n",
        "                     )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUz5m0i1pbj"
      },
      "source": [
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "ai.save(save_path)\n",
        "\n",
        "print(f'saved! {get_timestamp()}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi9g2G4hK5y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Use a Trained Model for Generation\n",
        "\n",
        "If you already had a trained model from this notebook, running the next cell will copy the `pytorch_model.bin` and the `config.json`file from the specified folder in Google Drive into the Colaboratory VM. (If no `from_folder` is specified, it assumes the two files are located at the root level of your Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeznI_VeaDQn"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alooLLyx1ZGu"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# best model thus far @ 1.3B parameters and tuned for 50k steps\n",
        "# from_folder = \"/content/drive/MyDrive/Programming/AI_peter/GPT-Neo-1B-V1\"\n",
        "\n",
        "from_folder = save_path\n",
        "\n",
        "if len(from_folder) > 2:\n",
        "\n",
        "    for file in [\"pytorch_model.bin\", \"config.json\"]:\n",
        "        if from_folder:\n",
        "            copy_file_from_gdrive(file, from_folder)\n",
        "        else:\n",
        "            copy_file_from_gdrive(file)\n",
        "\n",
        "    ai = aitextgen(model_folder=from_folder, to_gpu=True)\n",
        "else:\n",
        "    ai = aitextgen(model_folder=\".\", to_gpu=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "### Generate Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd0RGDbJiDp"
      },
      "source": [
        "`generate()` without any parameters generates a single text from the loaded model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "source": [
        "ai.generate(n=3, max_length=256, \n",
        "            temperature=1.0, top_p=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSH7QgiiGi7"
      },
      "source": [
        "ai.generate(prompt=\"these days, it always seems like \", temperature=1,\n",
        "            min_length=10, batch_size =20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = ai.generate_one()`\n",
        "\n",
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "ai.generate(\n",
        "    n=3, batch_size=25, prompt=\"i just\", max_length=256, \n",
        "    temperature=1.0, top_p=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of texts to a file and sort out the samples locally on your computer. The next cell will generate `num_files` files, each with `n` texts and whatever other parameters you would pass to `generate()`. The files can then be downloaded from the Files sidebar!\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKp18dTTj402"
      },
      "source": [
        "save_loc = join(base_dir, \"generated_text_out\")\n",
        "\n",
        "os.makedirs(save_loc, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "p_list = [[\"person alpha:\"+\"\\n\", \n",
        "           \"how are you doing?\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "          [\"person alpha:\"+\"\\n\", \n",
        "           \"hello there!\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "           [\"person alpha:\"+\"\\n\", \"why does it always seem that \"],\n",
        "           [\"person beta:\" + \"\\n\"],\n",
        "]\n",
        "\n",
        "\n",
        "prompts = [\"\".join(line) for line in p_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8et1WHilo_A"
      },
      "source": [
        "from datetime import datetime\n",
        "import pprint as pp\n",
        "\n",
        "ds_date_time = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "\n",
        "base_header = \"gpt-dailydialogue-textgen-{}\".format(ds_date_time)\n",
        "prompt_IDs = [base_header + \"_file-{}.txt\".format(i+1) for i in range(5, len(prompts)+11)]\n",
        "\n",
        "prompt_mng = {}\n",
        "for pid, text in zip(prompt_IDs, prompts):\n",
        "    prompt_mng[pid] = text\n",
        "pp.pprint(prompt_mng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPnqt-UVlEPR"
      },
      "source": [
        "from os.path import join\n",
        "\n",
        "for pfile, my_prompt in prompt_mng.items():\n",
        "    ai.generate_to_file(\n",
        "        n=50,\n",
        "        batch_size=5,\n",
        "        prompt=my_prompt,\n",
        "        max_length=512,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        destination_path=join(save_loc, pfile)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDdXrPOv9DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}