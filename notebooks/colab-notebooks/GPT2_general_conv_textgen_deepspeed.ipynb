{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GPT2- general conv - textgen-deepspeed.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NK7daXR_cAU1"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "47d6f70da3cb4e24a4193a4088bf075a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_da13b9a2f42a4e4cbdfcbd1bfcbfa76f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2cdb2b9a1eeb4f0696bb5e4cdbf0ba50",
              "IPY_MODEL_4e792bdc1700490b93c5d981ba0f31a0",
              "IPY_MODEL_4483de3675ee4a06bf6fc8dfde67b0fd"
            ]
          }
        },
        "da13b9a2f42a4e4cbdfcbd1bfcbfa76f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2cdb2b9a1eeb4f0696bb5e4cdbf0ba50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_76a143af6baf4d3fa13593f1b1695f6e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "replacing speaker names: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ededd46df80341ae90e3f58b65e92620"
          }
        },
        "4e792bdc1700490b93c5d981ba0f31a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f531057bef4427599a887a9a6cc721e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 262550,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 262550,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_033b344c82974ae485d3d31dfc82193e"
          }
        },
        "4483de3675ee4a06bf6fc8dfde67b0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95358ec087ee48ffbedaedf0c0b1e737",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 262550/262550 [00:00&lt;00:00, 1183836.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cfd2fa2b72554368a7d8419983b34957"
          }
        },
        "76a143af6baf4d3fa13593f1b1695f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ededd46df80341ae90e3f58b65e92620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f531057bef4427599a887a9a6cc721e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "033b344c82974ae485d3d31dfc82193e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95358ec087ee48ffbedaedf0c0b1e737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cfd2fa2b72554368a7d8419983b34957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fecc190b1eb946c38899cb259d4e72dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4952a748aed246b09dc970be721fd589",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5debdf5f99749449f767005fc3690e2",
              "IPY_MODEL_d582730616fa4f3d971316ccbd5e4bcc",
              "IPY_MODEL_07e7491d26a54d2a9d08f9b5e58ffbdf"
            ]
          }
        },
        "4952a748aed246b09dc970be721fd589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "c5debdf5f99749449f767005fc3690e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_525d22c449ae40aaa96364ea801bb5f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d8b30c478d042f28bb9d906490baeee"
          }
        },
        "d582730616fa4f3d971316ccbd5e4bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dd04c017d2cf4b749b58a51402c384a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 262550,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 262550,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e351b52ad6a4f76af1d1a98d90b2602"
          }
        },
        "07e7491d26a54d2a9d08f9b5e58ffbdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8d27553896b54cc89dee49d01d8f48df",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 262550/262550 [00:03&lt;00:00, 70451.93it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce189b9aad774f0eaa68fae88d33653d"
          }
        },
        "525d22c449ae40aaa96364ea801bb5f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d8b30c478d042f28bb9d906490baeee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dd04c017d2cf4b749b58a51402c384a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e351b52ad6a4f76af1d1a98d90b2602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d27553896b54cc89dee49d01d8f48df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce189b9aad774f0eaa68fae88d33653d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a34365db4d654c71bf5a737a31bce585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8f414ceddfdf4a93bcc6dd48ffd96bfe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_79bd1471eb304476815e9d38593be002",
              "IPY_MODEL_af86416c5dc6416fb1171da562494d5f",
              "IPY_MODEL_dba3833eaaed41c48aea8ae8f09208a0"
            ]
          }
        },
        "8f414ceddfdf4a93bcc6dd48ffd96bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "79bd1471eb304476815e9d38593be002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4ab31be35fb48a29ab47c129b7afe17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  1%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99e18c8a0c0241c083563e37325c6900"
          }
        },
        "af86416c5dc6416fb1171da562494d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_53115460f9af4e01ba30e40eb2f873c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 50000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 700,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69e8ab4fe019400e9d70bc92380ba17b"
          }
        },
        "dba3833eaaed41c48aea8ae8f09208a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2ad798a0dfc3467e8c6f9b111fec5562",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 700/50000 [03:53&lt;4:33:59,  3.00it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eec08564509346cfaaf357112db4aab5"
          }
        },
        "a4ab31be35fb48a29ab47c129b7afe17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99e18c8a0c0241c083563e37325c6900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "53115460f9af4e01ba30e40eb2f873c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69e8ab4fe019400e9d70bc92380ba17b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ad798a0dfc3467e8c6f9b111fec5562": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eec08564509346cfaaf357112db4aab5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/main/notebooks/colab-notebooks/GPT2_general_conv_textgen_deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "##  aitextgen — Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU\n",
        "\n",
        "This notebook is based on the original tutorial from `aitextgen`. **What is different vs. any other one is that this one installs a clone of a _branch_ oi aitextgen that has actually been updated recently.Deepspeed seems to work on this one.**\n",
        "\n",
        "- For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/).\n",
        "- for `ai-msgbot` (which is using `aitextgen` for chatbot-esque purposes) you can find the project repo [here](https://github.com/pszemraj/ai-msgbot)\n",
        "\n",
        "\n",
        "_updates made by [Peter](https://peterszemraj.ch/)_\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4MbTt1SwId0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhbtErR7wKxC"
      },
      "source": [
        "### GPU\n",
        "\n",
        "Colaboratory uses a Nvidia K80, an Nvidia P100, an Nvidia V100, or Nvidia A100. For finetuning GPT-2 124M, any of these GPUs will be fine, but for text generation, a k80 or a P100 is ideal since they have more VRAM. \n",
        "\n",
        "- In theory: **If you receive a T4 or a V100 GPU, you can enable `fp16=True` during training for faster/more memory efficient training.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAQC46o2zeMo"
      },
      "source": [
        "# !sudo apt-get purge nvidia*\n",
        "# !udo add-apt-repository ppa:graphics-drivers/ppa\n",
        "# !sudo apt-get update\n",
        "# !ubuntu-drivers devices\n",
        "# !sudo apt-get install nvidia-driver-460"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgMEhF7J3ubw",
        "outputId": "6cad6e0f-5ec6-467f-999a-48034ffea690"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 15 03:18:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    56W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av56_NzY2tUk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjFoflTs1f7A",
        "outputId": "1c6ac7cd-7fe7-40de-af91-b6f5480819e2"
      },
      "source": [
        "!nvcc --version\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "### formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemDfmdgjefZ"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_UJ8ek31Uz-r",
        "outputId": "e7153330-2c17-4fb3-d5c1-e591456c1dfa"
      },
      "source": [
        "# update torch in case using a A100 GPU\n",
        "# may take 5+ minutes\n",
        "\n",
        "# !pip3 install -q torch==1.9.1+cu111 torchtext torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html -q\n",
        "\n",
        "!pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html -q\n",
        "!pip install https://storage.googleapis.com/jax-releases/cuda111/jaxlib-0.1.71+cuda111-cp37-none-manylinux2010_x86_64.whl -q\n",
        "\n",
        "# see this issue https://github.com/googlecolab/colabtools/issues/2452"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "939bf1c2-ac9d-48a3-a403-897a9e9510c5"
      },
      "source": [
        "# !pip install -q aitextgen\n",
        "!pip install https://github.com/minimaxir/aitextgen/archive/trainer.zip -q\n",
        "# pip install https://github.com/user/repository/archive/branch.zip\n",
        "!pip install -q deepspeed\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DuC67no59vMe",
        "outputId": "f4f5bc49-5ff1-491a-d35d-c35d37791c85"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "### Loading GPT-2 or GPT Neo\n",
        "\n",
        "\n",
        "- A common use case is *continuing* to fine-tune a model that was originally pretrained, and then fine-tuned a little bit, but needs to be fine-tuned more for accuracy/saliency reasons or because Google cut off the runtime earlier. \n",
        "    - in this case, `load_from_folder` should be set to `True` and `load_folder_dir` points to where the model checkpoint is on your google drive.\n",
        "- **the below section describes loading an new/pretrained model from the original tutorial.**\n",
        "\n",
        "> If you're retraining a model on new text, you need to download and load the GPT-2 model into the GPU. \n",
        "\n",
        "> There are several sizes of GPT-2:\n",
        "\n",
        "    * `124M` (default): the \"small\" model, 500MB on disk.\n",
        "    * `355M` (default): the \"medium\" model, 1.5GB on disk.\n",
        "    * `774M` (default): the \"large\" model, 3GB on disk.\n",
        "\n",
        "> You can also finetune a GPT Neo model instead ([_or any textgen GPT-architecture model on huggingface_](https://huggingface.co/models?pipeline_tag=text-generation)), which is more suitable for longer texts and the base model has more recent data:\n",
        "\n",
        "* `125M`: Analogous to the GPT-2 124M model. (355M parameter model was removed)\n",
        "*  `EleutherAI/gpt-neo-1.3B` : 1.3 billion parameter model. Have yet to see this train on Colab without crashing\n",
        "\n",
        "**the largest model trained on Colab on \"standard settings\" is [GPT-Neo 1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B) as of Nov 27**\n",
        "\n",
        "> The next cell downloads the model and saves it in the Colaboratory VM. If the model has already been downloaded, running this cell will reload it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPPsg8XDwHvl"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXIbld-ASHL9",
        "cellView": "form",
        "outputId": "6585ae61-d46e-43cc-980e-f7ffdd5242f9"
      },
      "source": [
        "model_size = \"774M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = False #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Programming/ai-msgbot/ur-model-to-load\" #@param {type:\"string\"}\n",
        "custom_model = \"gpt2-large\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "flqSlHjMIeIw",
        "outputId": "d04b474c-416a-462e-b0e2-c524a60ba80a"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "elif len(custom_model) > 0:\n",
        "    model_size = custom_model.split('/')[-1]\n",
        "    ai = aitextgen(model=custom_model, \n",
        "                   to_gpu=True, \n",
        "                   to_fp16=True,\n",
        "\n",
        "                   gradient_checkpointing=True)\n",
        "else:\n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=True,\n",
        "                   to_fp16=True,\n",
        "                   gradient_checkpointing=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/15/2021 03:18:33 — INFO — aitextgen — Loading gpt2-large model from /aitextgen.\n",
            "12/15/2021 03:18:45 — INFO — aitextgen — GPT2 loaded with 774M parameters.\n",
            "12/15/2021 03:18:45 — INFO — aitextgen — Gradient checkpointing enabled for model training.\n",
            "12/15/2021 03:18:45 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n",
            "12/15/2021 03:18:45 — WARNING — aitextgen — Currently, FP16 text generation results in random output. You may want to avoid using to_fp16 for the time being.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7oQAqQHiFiX"
      },
      "source": [
        "### load training data\n",
        "\n",
        "\n",
        "- <font color=\"orange\"> combine any other data from the a \"standard\" conversational dataset and do a final pass of training (with different data) </font>\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "WoW\n",
        "https://github.com/pszemraj/ai-msgbot/raw/main/conversation-data/wizard-of-wikipedia/ScriptParse-wow-train-kilt.txt\n",
        "\n",
        "Daily Dialogues\n",
        "https://github.com/pszemraj/ai-msgbot/raw/main/conversation-data/Daily-Dialogues/daily_dialogue_augment.txt\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jUWsiVOCUlLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3ade2975-8499-4089-fb34-686ea10d0db9"
      },
      "source": [
        "dl_link = \"https://www.dropbox.com/s/b54gwbxl4e0p4sn/ScriptParse-wow-train-kilt_3.txt?dl=1\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hirJkLvbUrMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5c1dcf3-499d-427a-baf9-175be5dae64b"
      },
      "source": [
        "# download test image\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import os\n",
        "vm_wd = os.getcwd()\n",
        "local_name = join(vm_wd, \"training_script.txt\")\n",
        "request.urlretrieve(dl_link, local_name)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/training_script.txt', <http.client.HTTPMessage at 0x7fec38e1fa50>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEWZnWyYX3y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "28a7fdc5-e8ad-401e-e7b3-05025365979b"
      },
      "source": [
        "# adjust names in script if needed  \n",
        "import pprint as pp\n",
        "from os.path import basename\n",
        "\n",
        "def update_script_names(local_name, spkr_from=\"speaker a\", \n",
        "                        spkr_to=\"person alpha\",\n",
        "                        resp_from=\"speaker b\", resp_to=\"person beta\",\n",
        "                        verbose=False):\n",
        "    \"\"\"\n",
        "    update_script_names - if the textfile script has different names for the \n",
        "    speaker/responder than desired (i.e. it is a group conversation, and the \n",
        "    chatbot is just supposed to simulate 1:1) this function can be used to \n",
        "    standardize\n",
        "    \"\"\"\n",
        "\n",
        "    with open(local_name, 'r', encoding='utf-8', errors='ignore') as fi:\n",
        "        orig_lines = fi.readlines()\n",
        "\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    upd_lines = []\n",
        "\n",
        "    for line in tqdm(orig_lines, total=len(orig_lines), \n",
        "                     desc=\"replacing speaker names\"):\n",
        "        \n",
        "        fixline = line.replace(spkr_from, spkr_to)\n",
        "        fixline = fixline.replace(resp_from, resp_to)\n",
        "        upd_lines.append(fixline)\n",
        "\n",
        "    local_namev2 = join(vm_wd, \"V2-rename-\" + basename(local_name))\n",
        "\n",
        "    with open(local_namev2, 'w', encoding='utf-8', errors='ignore') as fo:\n",
        "        fo.writelines(upd_lines)\n",
        "\n",
        "    if verbose: pp.pprint(upd_lines[:10])\n",
        "    # return filepath\n",
        "    return local_namev2\n",
        "\n",
        "\n",
        "def preview_script(file_path, num_lines:int=20):\n",
        "    with open(local_name, 'r', encoding='utf-8', errors='ignore') as fi:\n",
        "        script_lines = fi.readlines()\n",
        "\n",
        "    print(f\"A preview of the first {num_lines} lines of {file_path} is: \\n\")\n",
        "    pp.pprint(script_lines[:num_lines])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "47d6f70da3cb4e24a4193a4088bf075a",
            "da13b9a2f42a4e4cbdfcbd1bfcbfa76f",
            "2cdb2b9a1eeb4f0696bb5e4cdbf0ba50",
            "4e792bdc1700490b93c5d981ba0f31a0",
            "4483de3675ee4a06bf6fc8dfde67b0fd",
            "76a143af6baf4d3fa13593f1b1695f6e",
            "ededd46df80341ae90e3f58b65e92620",
            "5f531057bef4427599a887a9a6cc721e",
            "033b344c82974ae485d3d31dfc82193e",
            "95358ec087ee48ffbedaedf0c0b1e737",
            "cfd2fa2b72554368a7d8419983b34957"
          ]
        },
        "outputId": "638152ff-b0ca-4edb-f513-f039fbf38134"
      },
      "source": [
        "local_name = update_script_names(local_name)\n",
        "\n",
        "file_name = local_name # update if using fn above\n",
        "preview_script(file_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47d6f70da3cb4e24a4193a4088bf075a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "replacing speaker names:   0%|          | 0/262550 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A preview of the first 20 lines of /content/V2-rename-training_script.txt is: \n",
            "\n",
            "['that is true. i think i am pretty good at visualization. are you?\\n',\n",
            " '\\n',\n",
            " 'person beta:\\n',\n",
            " 'yeah i would say so. it is fun to talk about because not everyone has the '\n",
            " \"same interpretations, you can't really control it down to one.\\n\",\n",
            " '\\n',\n",
            " 'person alpha:\\n',\n",
            " 'i really enjoy a cappuccino, with all that warm steamed milk and double '\n",
            " \"espresso...it really starts off my day. do you like cappuccino's?\\n\",\n",
            " '\\n',\n",
            " 'person beta:\\n',\n",
            " \"love them. though now it's almond milk for me! and having a cappucino in \"\n",
            " 'italy is pretty darn close to heaven ;-)\\n',\n",
            " '\\n',\n",
            " 'person beta:\\n',\n",
            " \"well, i've never had one in italy, but would love it! i like cappuccinos \"\n",
            " 'with cream too, maybe a dab of cinnamon as well.\\n',\n",
            " '\\n',\n",
            " 'person alpha:\\n',\n",
            " 'i like the color blue, i even named my dog blue!\\n',\n",
            " '\\n',\n",
            " 'person beta:\\n',\n",
            " \"that's pretty cool. i like blue from blue's clues, haha. blue is one of the \"\n",
            " 'three primary colors of pigments in painting.\\n',\n",
            " '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "## Train / Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change **unless the model size grows beyond 1B params**\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "REDm2euJ1kHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b4d0eaa2-04a2-4cef-eb49-1f576b751812"
      },
      "source": [
        "base_dir = \"/content/drive/MyDrive/Programming/ai-msgbot\" #@param {type:\"string\"}\n",
        "# update to yours"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDmyL2A7uhaL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e5d31a2-6bac-4e27-b0c7-133474c191d9"
      },
      "source": [
        "import gc, os\n",
        "from os.path import join\n",
        "from datetime import datetime\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.now().strftime(\"%b-%d-%Y_t-%H\")\n",
        "\n",
        "temp_gpu_path = join(base_dir, \n",
        "                     \"GPT2-conversational-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                            dt=get_timestamp(),\n",
        "                                                            )\n",
        "                     )\n",
        "os.makedirs(temp_gpu_path, exist_ok=True)\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzDXk_iGn-wF"
      },
      "source": [
        "### example outputs\n",
        "\n",
        "- this cell had its outputs cleared before notebook was posted.\n",
        "- the below is an example of what the outputs during training should look like.\n",
        "\n",
        "```\n",
        "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:1819: LightningDeprecationWarning: `trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7. Use `ProgressBarBase.get_metrics` instead.\n",
        "  \"`trainer.progress_bar_dict` is deprecated in v1.5 and will be removed in v1.7.\"\n",
        "1,000 steps reached: saving model to //content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-774M-Nov-25-2021_t-04\n",
        "1,500 steps reached: generating sample texts.\n",
        "/usr/local/lib/python3.7/dist-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
        "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
        "==========\n",
        "\n",
        "\n",
        "person alpha:\n",
        "do you enjoy the movie?\n",
        "\n",
        "person beta:\n",
        "yes, i love the film is based in george r. r. martin scotland\n",
        "\n",
        "person alpha:\n",
        "oh cool, how does it get to be one?\n",
        "\n",
        "person beta:\n",
        "well it was a break between a single.\n",
        "\n",
        "person alpha:\n",
        "i really like the show but i don't know much about it, i do not know much about it\n",
        "\n",
        "person beta:\n",
        "i know there are many episodes for the show, and the show has to be the highest grosss in the show though.\n",
        "\n",
        "person alpha:\n",
        "i've been a new game of thrones, or twice a year\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "(train_data: str | TokenDataset, output_dir: str = \"trained_model\", fp16: bool = False, fp16_opt_level: str = \"O1\", n_gpu: int = - 1, tpu_cores: int = 0, max_grad_norm: float = 0.5, gradient_accumulation_steps: int = 1, seed: int | None = None, learning_rate: float = 0.001, weight_decay: float = 0.05, adam_epsilon: float = 1e-8, warmup_steps: int = 0, num_steps: int = 5000, save_every: int = 1000, generate_every: int = 1000, n_generate: int = 1, loggers: List | None = None, batch_size: int = 1, num_workers: int | None = None, benchmark: bool = True, avg_loss_smoothing: float = 0.01, save_gdrive: bool = False, run_id: str = <Expression>, progress_bar_refresh_rate: int = 20, freeze_layers: bool = False, num_layers_freeze: int | None = None, use_deepspeed: bool = False, **kwargs) -> None\n",
        "```\n"
      ],
      "metadata": {
        "id": "VrOonVzN94nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aitextgen.TokenDataset import TokenDataset\n",
        "\n",
        "data = TokenDataset(file_name)\n",
        "\n",
        "# try ai.train_pt() later"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fecc190b1eb946c38899cb259d4e72dd",
            "4952a748aed246b09dc970be721fd589",
            "c5debdf5f99749449f767005fc3690e2",
            "d582730616fa4f3d971316ccbd5e4bcc",
            "07e7491d26a54d2a9d08f9b5e58ffbdf",
            "525d22c449ae40aaa96364ea801bb5f3",
            "3d8b30c478d042f28bb9d906490baeee",
            "dd04c017d2cf4b749b58a51402c384a0",
            "4e351b52ad6a4f76af1d1a98d90b2602",
            "8d27553896b54cc89dee49d01d8f48df",
            "ce189b9aad774f0eaa68fae88d33653d"
          ]
        },
        "id": "XB0yI5AKAmDo",
        "outputId": "1877e4f3-1ff3-4114-b9fd-909454b055a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fecc190b1eb946c38899cb259d4e72dd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/262550 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "12/15/2021 03:18:54 — INFO — aitextgen.TokenDataset — Encoding 262,550 sets of tokens from /content/V2-rename-training_script.txt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk1pdV5lovKg"
      },
      "source": [
        "### t r a i n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "a34365db4d654c71bf5a737a31bce585",
            "8f414ceddfdf4a93bcc6dd48ffd96bfe",
            "79bd1471eb304476815e9d38593be002",
            "af86416c5dc6416fb1171da562494d5f",
            "dba3833eaaed41c48aea8ae8f09208a0",
            "a4ab31be35fb48a29ab47c129b7afe17",
            "99e18c8a0c0241c083563e37325c6900",
            "53115460f9af4e01ba30e40eb2f873c9",
            "69e8ab4fe019400e9d70bc92380ba17b",
            "2ad798a0dfc3467e8c6f9b111fec5562",
            "eec08564509346cfaaf357112db4aab5"
          ]
        },
        "outputId": "386b06b8-2685-485e-88e8-7404917a25b4"
      },
      "source": [
        "# DO NOT USE WARMUP STEPS\n",
        "# ai.train_pt()\n",
        "ai.train(\n",
        "            train_data=data,\n",
        "            output_dir=temp_gpu_path, # where it saves during \"save_every\"\n",
        "            line_by_line=False, # if using CSV file input\n",
        "            from_cache=False,\n",
        "            num_steps=50000, # takes about 5 hours on 16 gb v100 GPU for 75000\n",
        "            generate_every=1500,\n",
        "            max_grad_norm=0.5,\n",
        "            save_every=1000,\n",
        "            gradient_accumulation_steps=1,\n",
        "            save_gdrive=False, # this is an \"automated\" save which is worse than current method (IMO)\n",
        "            learning_rate=1e-4,\n",
        "            # fp16_opt_level=\"O2\", # different types of FP16 are possible\n",
        "         \n",
        "            # learning_rate=1e-3,\n",
        "            use_deepspeed=True,\n",
        "            # fp16=True, # may be relevant to set to false (even if available) for \"final\" training\n",
        "            batch_size=1, # if pushing model_size you probably want to leave this at 1\n",
        "            freeze_layers= True, # whether to change weights on ALL layers or not\n",
        "            num_layers_freeze = 28, # look up how many layers your model has\n",
        "        #  fp16_opt_level=\"O2\", # different types of FP16 are possible\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "***** Running training *****\n",
            "  Num examples = 2288881\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 50000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a34365db4d654c71bf5a737a31bce585",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/50000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/Programming/ai-msgbot/GPT2-conversational-gpt2-large-Dec-15-2021_t-03/checkpoint-500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 1.4697, 'learning_rate': 0.0001, 'epoch': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DLJJolCYQQH"
      },
      "source": [
        "save_path = join(base_dir, \n",
        "                     \"FINAL-GPT2-conv-{sz}-{dt}\".format(sz=model_size,\n",
        "                                                        dt=get_timestamp(),\n",
        "                                                        )\n",
        "                     )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUz5m0i1pbj"
      },
      "source": [
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "ai.save(save_path)\n",
        "\n",
        "print(f'saved! {get_timestamp()}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpi9g2G4hK5y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "## Use a Trained Model for Generation\n",
        "\n",
        "If you already had a trained model from this notebook, running the next cell will copy the `pytorch_model.bin` and the `config.json`file from the specified folder in Google Drive into the Colaboratory VM. (If no `from_folder` is specified, it assumes the two files are located at the root level of your Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeznI_VeaDQn"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alooLLyx1ZGu"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# best model thus far @ 1.3B parameters and tuned for 50k steps\n",
        "# from_folder = \"/content/drive/MyDrive/Programming/AI_peter/GPT-Neo-1B-V1\"\n",
        "\n",
        "from_folder = save_path\n",
        "\n",
        "if len(from_folder) > 2:\n",
        "\n",
        "    for file in [\"pytorch_model.bin\", \"config.json\"]:\n",
        "        if from_folder:\n",
        "            copy_file_from_gdrive(file, from_folder)\n",
        "        else:\n",
        "            copy_file_from_gdrive(file)\n",
        "\n",
        "    ai = aitextgen(model_folder=from_folder, to_gpu=True)\n",
        "else:\n",
        "    ai = aitextgen(model_folder=\".\", to_gpu=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "### Generate Text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd0RGDbJiDp"
      },
      "source": [
        "`generate()` without any parameters generates a single text from the loaded model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "source": [
        "ai.generate(n=3, max_length=256, \n",
        "            temperature=1.0, top_p=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSH7QgiiGi7"
      },
      "source": [
        "ai.generate(prompt=\"these days, it always seems like \", temperature=1,\n",
        "            min_length=10, batch_size =20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = ai.generate_one()`\n",
        "\n",
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "ai.generate(\n",
        "    n=3, batch_size=25, prompt=\"i just\", max_length=256, \n",
        "    temperature=1.0, top_p=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of texts to a file and sort out the samples locally on your computer. The next cell will generate `num_files` files, each with `n` texts and whatever other parameters you would pass to `generate()`. The files can then be downloaded from the Files sidebar!\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKp18dTTj402"
      },
      "source": [
        "save_loc = join(base_dir, \"generated_text_out\")\n",
        "\n",
        "os.makedirs(save_loc, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "p_list = [[\"person alpha:\"+\"\\n\", \n",
        "           \"how are you doing?\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "          [\"person alpha:\"+\"\\n\", \n",
        "           \"hello there!\"+\"\\n\", \"\\n\", \n",
        "           \"person beta:\" + \"\\n\"], \n",
        "           [\"person alpha:\"+\"\\n\", \"why does it always seem that \"],\n",
        "           [\"person beta:\" + \"\\n\"],\n",
        "]\n",
        "\n",
        "\n",
        "prompts = [\"\".join(line) for line in p_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8et1WHilo_A"
      },
      "source": [
        "from datetime import datetime\n",
        "import pprint as pp\n",
        "\n",
        "ds_date_time = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "\n",
        "base_header = \"gpt-model-textgen-{}\".format(ds_date_time)\n",
        "prompt_IDs = [base_header + \"_file-{}.txt\".format(i+1) for i in range(5, len(prompts)+11)]\n",
        "\n",
        "prompt_mng = {}\n",
        "for pid, text in zip(prompt_IDs, prompts):\n",
        "    prompt_mng[pid] = text\n",
        "pp.pprint(prompt_mng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPnqt-UVlEPR"
      },
      "source": [
        "from os.path import join\n",
        "\n",
        "for pfile, my_prompt in prompt_mng.items():\n",
        "    ai.generate_to_file(\n",
        "        n=50,\n",
        "        batch_size=5,\n",
        "        prompt=my_prompt,\n",
        "        max_length=512,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        destination_path=join(save_loc, pfile)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leDdXrPOv9DG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}