{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gridsearch_GPT_params.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPZDblqNwugm5ZuPrq0p2bk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45e2a546738b42ddaf055ba1f9249890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a3313d78f814a72ac53ab1df92bec49",
              "IPY_MODEL_6dd3a391bf334ae5ae157f336e7f08a3",
              "IPY_MODEL_3545d2d319944bad902ed2292110e39d"
            ],
            "layout": "IPY_MODEL_0722cd453c734893ade54cf13af16dcc"
          }
        },
        "8a3313d78f814a72ac53ab1df92bec49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d1fc1e2ee0f4940aa61f5b0a1463e67",
            "placeholder": "​",
            "style": "IPY_MODEL_ff2c9526948340698003bc006bc308d6",
            "value": "completing grid search...:  56%"
          }
        },
        "6dd3a391bf334ae5ae157f336e7f08a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f16c92fb2fed46e7b3a21440fe099f6b",
            "max": 4375,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9f4f82eb9ad4aeeab0d8b0b018721bf",
            "value": 2447
          }
        },
        "3545d2d319944bad902ed2292110e39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc65a41119f64e50a44cec4b91bde10d",
            "placeholder": "​",
            "style": "IPY_MODEL_59d764c1510c4d858c2bd91062509c1e",
            "value": " 2447/4375 [12:46:22&lt;10:39:14, 19.89s/it]"
          }
        },
        "0722cd453c734893ade54cf13af16dcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1fc1e2ee0f4940aa61f5b0a1463e67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff2c9526948340698003bc006bc308d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f16c92fb2fed46e7b3a21440fe099f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f4f82eb9ad4aeeab0d8b0b018721bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc65a41119f64e50a44cec4b91bde10d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d764c1510c4d858c2bd91062509c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/main/notebooks/colab-notebooks/hyperparameters/gridsearch_GPT_params.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88KhaHME-ZVq"
      },
      "source": [
        "# Text Generation Chatbot: Grid Search for Parameters\n",
        "\n",
        "- optimal \"text bot\" hyperparamters are not really known, nor is there a right answer (_i.e. there is no \"ground truth\" dataset of 'reasonable responses to X prompt'_)\n",
        "- proposal: iterate through \"reasonable\" parameter ranges, record output, compute sentence coherence scoring on outputs\n",
        "- evaluate results once finished"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPqNVijEZ6dU",
        "outputId": "2139e6e4-7351-4fd0-c843-08cee243f45d"
      },
      "source": [
        "!pip install -U -q pandas\n",
        "!pip uninstall -y -q pyarrow\n",
        "!pip install -U -q pyarrow\n",
        "!pip install -U -q openpyxl\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.3 MB 28.6 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 25.6 MB 73.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 242 kB 35.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "## formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8U_usB0yPkh"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Nn3e175uDlXC",
        "outputId": "4f85bf14-8cd2-40db-dd55-b9144b309d49"
      },
      "source": [
        "# !pip3 install -q torch==1.9.1+cpu  -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -q torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_mh0LUOiyZdc",
        "outputId": "f1740581-7ae4-4c75-c2e0-de99a03ddd1e"
      },
      "source": [
        "%%capture\n",
        "!pip install -U tqdm\n",
        "!pip install clean-text\n",
        "!pip install -q torchtext\n",
        "\n",
        "import argparse\n",
        "import gc\n",
        "import os\n",
        "import pprint as pp\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from cleantext import clean\n",
        "warnings.filterwarnings(action=\"ignore\", message=\".*gradient_checkpointing*\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_AuKgHyDktV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "cc0fc5c5-34eb-49e6-e420-0dea3dcfefc0"
      },
      "source": [
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 572 kB 37.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 43.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 524 kB 53.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 72.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 31.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 329 kB 70.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 69.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 65.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 64.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 73.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 71.4 MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMtCgKhZ_YW2"
      },
      "source": [
        "# gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "DuC67no59vMe",
        "outputId": "1a7f8a71-4c78-4671-dfc8-ca72367b25fb"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNhVl3R7yimM"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9I5TpuFZJc"
      },
      "source": [
        "## query_gpt_peter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "8-6N3jcWyjrV",
        "outputId": "adff7c3a-4808-4b92-ad0f-43ed8857040f"
      },
      "source": [
        "# should inherit main ai object\n",
        "def query_gpt_peter(\n",
        "                    prompt_msg: str,\n",
        "                    speaker=\"persona alpha\",\n",
        "                    responder=\"person beta\",\n",
        "                    kparam=100,\n",
        "                    temp=0.7,\n",
        "                    nuc_range=0.8,\n",
        "                    verbose=False,\n",
        "                    use_gpu=False,  \n",
        "                    ):\n",
        "\n",
        "\n",
        "    p_list = []\n",
        "    if speaker is not None:\n",
        "        p_list.append(speaker.lower() + \":\" + \"\\n\")  # write prompt as the speaker\n",
        "    p_list.append(prompt_msg.lower() + \"\\n\")\n",
        "    p_list.append(\"\\n\")\n",
        "    p_list.append(responder.lower() + \":\" + \"\\n\")\n",
        "    this_prompt = \"\".join(p_list)\n",
        "    if verbose:\n",
        "        print(\"overall prompt:\\n\")\n",
        "        pp.pprint(this_prompt, indent=4)\n",
        "    this_result = ai.generate(\n",
        "        n=1,\n",
        "        top_k=kparam,\n",
        "        batch_size=512,\n",
        "        max_length=128,\n",
        "        min_length=16,\n",
        "        prompt=this_prompt,\n",
        "        temperature=temp,\n",
        "        top_p=nuc_range,\n",
        "        do_sample=True,\n",
        "        return_as_list=True,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        this_result = str(this_result[0]).split(\"\\n\")\n",
        "        res_out = [str(ele).strip() for ele in this_result]\n",
        "        p_out = [str(ele).strip() for ele in p_list]\n",
        "        diff_list = list(\n",
        "            set(res_out).difference(p_out)\n",
        "        )  # remove prior prompts for the response\n",
        "        this_result = [\n",
        "            str(msg)\n",
        "            for msg in diff_list\n",
        "            if (\":\" not in str(msg))\n",
        "            and (\"szemr\" not in str(msg))\n",
        "            and (\"peter\" not in str(msg))\n",
        "        ]  # remove all names\n",
        "        if not isinstance(this_result, list):\n",
        "            list(this_result)\n",
        "        output = str(this_result[0]).strip()\n",
        "        # add second line of output if first is too short (subjective)\n",
        "        if len(output) < 15 and len(this_result) > 1:\n",
        "            output = output + \" \" + str(this_result[1]).strip()\n",
        "    except:\n",
        "        output = \"<bro, there was an error. try again>\"\n",
        "\n",
        "    p_list.append(output + \"\\n\")\n",
        "    p_list.append(\"\\n\")\n",
        "\n",
        "    model_responses = {\"out_text\": output, \"full_conv\": p_list}\n",
        "\n",
        "    return model_responses\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBIX9RcX1FmQ"
      },
      "source": [
        "## generic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SmXqCHgC1jyE",
        "outputId": "2a705c9a-bd0e-4f57-e586-6b4c2bc35ada"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# clears cells in jupyter \n",
        "\n",
        "def isnotebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'Shell':\n",
        "            return True  # Colab\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter\n",
        "\n",
        "def clear_jupyter_cell():\n",
        "    is_jupyter = isnotebook()\n",
        "\n",
        "    if is_jupyter:\n",
        "        clear_output(wait=True)\n",
        "    else:\n",
        "        print(\"not in a jupyter notebook\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "uEmQyYkq1GpY",
        "outputId": "4eb860ea-7e33-42cd-e11f-4888ac38d3ac"
      },
      "source": [
        "\n",
        "def create_folder(directory):\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "\n",
        "def shorten_title(title_text, max_no=35):\n",
        "    if len(title_text) < max_no:\n",
        "        return title_text\n",
        "    else:\n",
        "        return title_text[:max_no] + \"...\"\n",
        "\n",
        "\n",
        "def cleantxt_wrap(ugly_text):\n",
        "    # a wrapper for clean text with options different than default\n",
        "\n",
        "    # https://pypi.org/project/clean-text/\n",
        "    cleaned_text = clean(\n",
        "        ugly_text,\n",
        "        fix_unicode=True,  # fix various unicode errors\n",
        "        to_ascii=True,  # transliterate to closest ASCII representation\n",
        "        lower=True,  # lowercase text\n",
        "        no_line_breaks=True,  # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,  # replace all URLs with a special token\n",
        "        no_emails=True,  # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,  # replace all phone numbers with a special token\n",
        "        no_numbers=False,  # replace all numbers with a special token\n",
        "        no_digits=False,  # replace all digits with a special token\n",
        "        no_currency_symbols=True,  # replace all currency symbols with a special token\n",
        "        no_punct=True,  # remove punctuations\n",
        "        replace_with_punct=\"\",  # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"<NUM>\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"<CUR>\",\n",
        "        lang=\"en\",  # set to 'de' for German special handling\n",
        "    )\n",
        "\n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qJnA9sEpHhn"
      },
      "source": [
        "### dropbox \n",
        "\n",
        "\n",
        "- [tutorial](https://python.plainenglish.io/automate-your-pdf-upload-to-dropbox-python-script-bdacc2c721f6)\n",
        "- [api docs](https://dropbox-sdk-python.readthedocs.io/en/latest/api/dropbox.html?highlight=files_upload#dropbox.dropbox_client.Dropbox.files_upload)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oz_GboGqZrwa",
        "outputId": "13b37775-0e63-4eb2-b601-ae5db30db0c1"
      },
      "source": [
        "dropbox_subfolder = \"GPT text gen - gridsearch - 1pt3B WoW_DD\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8rOS5ZVZ1qPJ",
        "outputId": "4f5b4380-c06c-4111-97ed-fb9bcf4952a6"
      },
      "source": [
        "!pip install -q dropbox\n",
        "import dropbox"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 24.1 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30 kB 35.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 51 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 61 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 71 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 81 kB 26.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 92 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 102 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 112 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 122 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 133 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 143 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 153 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 163 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 174 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 184 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 194 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 204 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 215 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 225 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 235 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 245 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 256 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 266 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 276 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 286 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 296 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 307 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 317 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 327 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 337 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 348 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 358 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 368 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 378 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 389 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 399 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 409 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 419 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 430 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 440 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 450 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 460 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 471 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 481 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 491 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 501 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 512 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 522 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 532 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 542 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 552 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 563 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 573 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 580 kB 27.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 69.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "ErlmBoWaW5EZ",
        "outputId": "7fe02cff-1325-4f4e-9af0-5d3c93f8bddf"
      },
      "source": [
        "token = \"create-your-own\"\n",
        "\n",
        "dbx = dropbox.Dropbox(token)\n",
        "pp.pprint(dbx.users_get_current_account(), compact=True, indent=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/01/2021 04:31:21 — INFO — dropbox — Request to users/get_current_account\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FullAccount(account_id='dbid:AAAYVJX_n2JYa7yh6anfANmwSJ7qkr-u8Do', account_type=AccountType('pro', None), country='CH', disabled=False, email='peterszemraj@gmail.com', email_verified=True, is_paired=False, locale='en', name=Name(abbreviated_name='PS', display_name='Peter Szemraj', familiar_name='Peter', given_name='Peter', surname='Szemraj'), profile_photo_url='https://dl-web.dropbox.com/account_photo/get/dbaphid%3AAACl3aYwbYAysBHXpGdGBXZj6IYsXocikxc?size=128x128&vers=1628569648991', referral_link='https://www.dropbox.com/referrals/AAAVOEoeLmg4Nw7hRIA9Wsn8OMf2_EdGbik?src=app9-10920512', root_info=UserRootInfo(home_namespace_id='9560958', root_namespace_id='9560958'), team=NOT_SET, team_member_id=NOT_SET)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QPLVcnGz1oxA",
        "outputId": "6e9a5b79-df83-4e45-cce2-71ed78c8c46d"
      },
      "source": [
        "def get_size_mb(path2file, verbose=False):\n",
        "\n",
        "    file_stats = os.stat(path2file)\n",
        "\n",
        "    file_size_mb = {file_stats.st_size / (1024 * 1024)}\n",
        "    if verbose: print(f'File Size in MegaBytes is {file_size_mb}')\n",
        "    return round(list(file_size_mb)[0],2) # returns rounded to 2 decimals"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "vrV1mVSibJIy",
        "outputId": "e4d6cf49-0e94-4f9d-d138-89a830e7823a"
      },
      "source": [
        "from os.path import join, basename, dirname\n",
        "import time, random\n",
        "from google.colab import files\n",
        "\n",
        "def put_in_dropbox(\n",
        "    vm_path, subfolder=dropbox_subfolder, no_printout=True, ncalls=0, max_calls=3\n",
        "):\n",
        "    if ncalls > max_calls:\n",
        "        return \"failed saving to DropBox - {} tries\".format(ncalls)\n",
        "    elif get_size_mb(vm_path) > 155:\n",
        "        files.download(vm_path)\n",
        "        return \"file size of {} too big to put in DB, downloading\".format(basename(vm_path))\n",
        "    # for an item on the colab machine on path, upload to dropbox app folder at\n",
        "    # subfolder/\"filename\"\n",
        "    base_filename = basename(vm_path)\n",
        "    db_path = \"/{}/{}\".format(subfolder, base_filename)\n",
        "    try:\n",
        "        with open(vm_path, \"rb\") as f:\n",
        "            dbx.files_upload(f.read(), path=db_path, autorename=True, mute=no_printout)\n",
        "    except:\n",
        "        print(\n",
        "            \"WARNING - unable to post in dropbox, retry no. {} - \".format(ncalls + 1),\n",
        "            datetime.now(),\n",
        "        )\n",
        "        time.sleep(random.randint(1, 3))  # small delay before trying again\n",
        "        put_in_dropbox(vm_path, ncalls=ncalls + 1)  # recursion for retrying"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "# load model file\n",
        "\n",
        "\n",
        "- <font color=\"orange\"> IMPORTANT: while not validated _at the time of writing_, it is somewhat important to consider what optimal hyperparameter behavior _means_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXIbld-ASHL9",
        "outputId": "f3df1241-dcfd-4086-ac26-d43c134c3e50"
      },
      "source": [
        "model_size = \"355M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = True #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Programming/ai-msgbot/GPTneo_conv_33kWoW_18kDD\" #@param {type:\"string\"}\n",
        "do_grad_chkpt = False #@param {type:\"boolean\"}\n",
        "use_gpu = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "flqSlHjMIeIw",
        "outputId": "d810d694-aa9d-4798-96e3-57bb2d7a1f2d"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=use_gpu,\n",
        "                   gradient_checkpointing=do_grad_chkpt)\n",
        "else:\n",
        "    # load new \n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=use_gpu,\n",
        "                   gradient_checkpointing=do_grad_chkpt)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/01/2021 04:31:22 — INFO — aitextgen — Loading model from provided weights and config in //content/drive/MyDrive/Programming/ai-msgbot/GPTneo_conv_33kWoW_18kDD.\n",
            "12/01/2021 04:33:19 — INFO — aitextgen — GPTNeo loaded with 1315M parameters.\n",
            "12/01/2021 04:33:19 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "OWFcmCxs2szt",
        "outputId": "6575bd98-139d-4828-9a9a-d5719eebc038"
      },
      "source": [
        "dropbox_subfolder = dropbox_subfolder + \"_\" + os.path.basename(load_folder_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zmoaSxGq5gN"
      },
      "source": [
        "# go through loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5np1MlFD2LRb",
        "outputId": "17211c64-8be6-4599-957a-263e2302e82c"
      },
      "source": [
        "temp_srch = list(range(50, 100, 2))\n",
        "temp_srch = [float(item)/100 for item in temp_srch]\n",
        "top_p_srch = list(range(50, 100,2)) \n",
        "top_p_srch = [float(item)/100 for item in top_p_srch]\n",
        "topk_srch = list(range(25, 200, 25))\n",
        "estimate_comp = len(temp_srch)*len(top_p_srch)*len(topk_srch)\n",
        "print(\"anticipate approx {} calculations\\n\\n\".format(estimate_comp))\n",
        "\n",
        "grid_df = pd.DataFrame(columns=[\"temp\", \"topk\", \"top_p\", \"prompt\", \"speaker\",\n",
        "                                \"model_response\"])\n",
        "\n",
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anticipate approx 4375 calculations\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 0 entries\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   temp            0 non-null      object\n",
            " 1   topk            0 non-null      object\n",
            " 2   top_p           0 non-null      object\n",
            " 3   prompt          0 non-null      object\n",
            " 4   speaker         0 non-null      object\n",
            " 5   model_response  0 non-null      object\n",
            "dtypes: object(6)\n",
            "memory usage: 0.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6FR-spt_LW"
      },
      "source": [
        "**question lists:**\n",
        "\n",
        "1. [medium - 40 small talk q's](https://medium.com/twyla-ai/40-small-talk-questions-your-chatbot-needs-to-know-and-why-it-matters-63caf03347f6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "k23F0B3x4MVw",
        "outputId": "f29ba77b-a450-4b4b-8c70-1290cf3d62bb"
      },
      "source": [
        "import random\n",
        "random = random.SystemRandom() # big boy random\n",
        "\n",
        "n = 3\n",
        "print(f\"\\nrandom {n} values from temp list are \\n\", random.sample(temp_srch, k=n))\n",
        "print(f\"\\nrandom {n} values from top_p list are \\n\", random.sample(top_p_srch, k=n))\n",
        "print(f\"\\nrandom {n} values from topk list are \\n\", random.sample(topk_srch, k=n))\n",
        "\n",
        "\n",
        "# shuffle lists for search. going thru in ~random~ order enables being able\n",
        "# to somewhat look at the results during\n",
        "\n",
        "random.shuffle(temp_srch)\n",
        "random.shuffle(top_p_srch)\n",
        "\n",
        "# a series of general questions that users may ask to test model response\n",
        "generic_openers = [\n",
        "                    'How are you?',\n",
        "                    'What’s up?',\n",
        "                    'Good morning',\n",
        "                    'Tell me something',\n",
        "                    'Ok',\n",
        "                    'Yes',\n",
        "                    'I’ll do that now',\n",
        "                    'Hello',\n",
        "                    'Thank you',\n",
        "                    'Goodbye',\n",
        "                    'How can you help me?',\n",
        "                    'what can you do?',\n",
        "                    'Hi, my name is……',\n",
        "                    'Happy birthday!',\n",
        "                    'I have a question, can you help me?',\n",
        "                    'Do you know a joke?',\n",
        "                    'Do you love me?',\n",
        "                    'I love you',\n",
        "                    'Will you marry me?',\n",
        "                    'Are you single?',\n",
        "                    'Do you like people?',\n",
        "                    'Does Santa Claus exist?',\n",
        "                    'Are you part of the Matrix?',\n",
        "                    'You’re cute.',\n",
        "                    'Do you have a hobby?',\n",
        "                    'You’re clever',\n",
        "                    'Tell me about your personality',\n",
        "                    'You’re annoying',\n",
        "                    'you suck',\n",
        "                    'I want to speak to a human now.',\n",
        "                    'Don’t you speak English?!',\n",
        "                    'I want the answer NOW!',\n",
        "                    'Are you human?',\n",
        "                    'Are you a robot?',\n",
        "                    'What is your name?',\n",
        "                    'How old are you?',\n",
        "                    'What’s your age?',\n",
        "                    'What day is it today?',\n",
        "                    'Who made you?',\n",
        "                    'Which languages can you speak?',\n",
        "                    'What is your mother’s name?',\n",
        "                    'Where do you live?',\n",
        "                    'What’s the weather like today?',\n",
        "                    'Are you expensive?',\n",
        "                    'Do you get smarter?',\n",
        "                    'how old am I?',\n",
        "                    'Did I win the lottery last night?',\n",
        "                    'Will it fit in my shipping container?'\n",
        "                    'Can I ask the same question in different ways and will you understand all of them correctly?',\n",
        "                    'Can you tell me about yourself?'\n",
        "                   \n",
        "                ]\n",
        "\n",
        "test_speakers = [\n",
        "                 \"emilie szemraj\",\n",
        "                 \"michael lebens\",\n",
        "                 'jonas',\n",
        "                 \"jonathan\",\n",
        "                 \"joost\",\n",
        "                 \"kevin\",\n",
        "                 \"peter\",\n",
        "                 \"caio\",\n",
        "                 \"arnold schwarzenegger\",\n",
        "                 \"person alpha\", # some datasets use this in the script directly\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "random 3 values from temp list are \n",
            " [0.62, 0.92, 0.56]\n",
            "\n",
            "random 3 values from top_p list are \n",
            " [0.74, 0.98, 0.68]\n",
            "\n",
            "random 3 values from topk list are \n",
            " [100, 75, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOpEvDf1QfZR"
      },
      "source": [
        "conversation starters taken from [this parade article](https://parade.com/969981/parade/conversation-starters/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "45e2a546738b42ddaf055ba1f9249890",
            "8a3313d78f814a72ac53ab1df92bec49",
            "6dd3a391bf334ae5ae157f336e7f08a3",
            "3545d2d319944bad902ed2292110e39d",
            "0722cd453c734893ade54cf13af16dcc",
            "1d1fc1e2ee0f4940aa61f5b0a1463e67",
            "ff2c9526948340698003bc006bc308d6",
            "f16c92fb2fed46e7b3a21440fe099f6b",
            "d9f4f82eb9ad4aeeab0d8b0b018721bf",
            "dc65a41119f64e50a44cec4b91bde10d",
            "59d764c1510c4d858c2bd91062509c1e"
          ]
        },
        "id": "Bt6Lacky4GmK",
        "outputId": "42f36e2c-ff8d-416e-efa6-35bd008cdd8a"
      },
      "source": [
        "pbar = tqdm(total=estimate_comp, desc=\"completing grid search...\")\n",
        "\n",
        "for this_temp in temp_srch:\n",
        "    ind = temp_srch.index(this_temp)\n",
        "\n",
        "    if ind > 0 and ind % 5 == 0:\n",
        "        # save progress\n",
        "        timestamp = datetime.now().strftime(\"%b-%d-%Y_%H-%M\")\n",
        "        grid_df.reset_index(drop=True, inplace=True)\n",
        "        out_xl = \"GPT-gridsearch-{}.xlsx\".format(timestamp)\n",
        "        out_ftr = \"GPT-gridsearch-{}.ftr\".format(timestamp)\n",
        "        grid_df.to_excel(out_xl)\n",
        "        grid_df.to_feather(out_ftr)\n",
        "        put_in_dropbox(out_xl)\n",
        "        put_in_dropbox(out_ftr)\n",
        "\n",
        "    for this_top_p in top_p_srch:\n",
        "        for this_topk in topk_srch:\n",
        "\n",
        "            tst_spkr = random.sample(test_speakers, k=1)[0]\n",
        "            tst_prompt = random.sample(generic_openers, k=1)[0]\n",
        "\n",
        "            model_out = query_gpt_peter(\n",
        "                                        prompt_msg=tst_prompt,\n",
        "                                        speaker=tst_spkr,\n",
        "                                        kparam=this_topk,\n",
        "                                        temp=this_temp,\n",
        "                                        nuc_range=this_top_p,\n",
        "                                        verbose=False,\n",
        "                                        )\n",
        "            this_resp = model_out[\"out_text\"]\n",
        "            df_ind = len(grid_df)\n",
        "            # [\"temp\", \"topk\", \"top_p\", \"prompt\", \"speaker\", \"model_response\"]\n",
        "            grid_df.loc[df_ind] = [this_temp, this_topk, \n",
        "                                   this_top_p, tst_prompt,\n",
        "                                   tst_spkr, this_resp]\n",
        "            pbar.update(1)\n",
        "\n",
        "pbar.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45e2a546738b42ddaf055ba1f9249890",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "completing grid search...:   0%|          | 0/4375 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12/01/2021 08:59:17 — INFO — dropbox — Request to files/upload\n",
            "12/01/2021 08:59:17 — INFO — dropbox — Request to files/upload\n",
            "12/01/2021 13:28:00 — INFO — dropbox — Request to files/upload\n",
            "12/01/2021 13:28:01 — INFO — dropbox — Request to files/upload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBsKz-yk8VeQ"
      },
      "source": [
        "## save final results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FIzcf678Vi2"
      },
      "source": [
        "timestamp = datetime.now().strftime(\"%b-%d-%Y_%H-%M\")\n",
        "grid_df.reset_index(drop=True, inplace=True)\n",
        "out_xl = \"GPT-Peter774M-FINAL-gridsearch-{}.xlsx\".format(timestamp)\n",
        "out_ftr = \"GPT-Peter774M-FINAL-gridsearch-{}.ftr\".format(timestamp)\n",
        "grid_df.to_excel(out_xl)\n",
        "grid_df.to_feather(out_ftr)\n",
        "put_in_dropbox(out_xl)\n",
        "put_in_dropbox(out_ftr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Azv9Z2ZAsHw"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A19WObvdAtUa"
      },
      "source": [
        "error_string = \"<bro, there was an error. try again>\"\n",
        "\n",
        "df_analysis = grid_df.copy().convert_dtypes()\n",
        "df_analysis = df_analysis[df_analysis[\"model_response\"] != error_string]\n",
        "df_analysis.reset_index(drop=True, inplace=True)\n",
        "df_analysis.info(verbose=True) # overview"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIR9dYifF_Uh"
      },
      "source": [
        "assume the outputs are a single sentence and then use a [sentence-scoring algorithm on them](https://github.com/simonepri/lm-scorer) via `lm-scorer` package.\n",
        "\n",
        "- [here](https://colab.research.google.com/github/simonepri/lm-scorer/blob/master/examples/lm_scorer.ipynb) is an example notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPciEe8CGA-D"
      },
      "source": [
        "!pip install -q lm-scorer\n",
        "\n",
        "import torch\n",
        "from lm_scorer.models.auto import AutoLMScorer as LMScorer\n",
        "\n",
        "# Available models\n",
        "list(LMScorer.supported_model_names())\n",
        "# => [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", distilgpt2\"]\n",
        "\n",
        "# Load model to cpu or cuda\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 1\n",
        "scorer = LMScorer.from_pretrained(\"gpt2-large\", device=device,\n",
        "                                  batch_size=batch_size)\n",
        "print(\"loaded - \", datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLaDdHizHIil"
      },
      "source": [
        "# example\n",
        "\n",
        "# Compute sentence score as the geometric mean of tokens' probabilities\n",
        "scorer.sentence_score(\"I like this package.\", reduce=\"gmean\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqhFW7PHnWq"
      },
      "source": [
        "# create helper fn\n",
        "\n",
        "def score_text_blob(gen_text: str, method=\"gmean\", verbose=False):\n",
        "    # for method 'mean', \n",
        "    try:\n",
        "        return scorer.sentence_score(gen_text, reduce=method)\n",
        "    except:\n",
        "        if verbose: print(f\"failed computing scores for {gen_text}\")\n",
        "        return 0 # score sentences that failed at 0 - assumption = too undecipherable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1arelYZBM0Wb"
      },
      "source": [
        "### apply and see results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie1WA7-hXaNF"
      },
      "source": [
        "df_analysis.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZZt72ZGM3I7"
      },
      "source": [
        "import time\n",
        "st = time.time()\n",
        "df_analysis[\"model_response\"] = df_analysis[\"model_response\"].apply(str)\n",
        "df_analysis[\"model_response\"] = df_analysis[\"model_response\"].apply(cleantxt_wrap)\n",
        "df_analysis[\"geom_score\"] = df_analysis[\"model_response\"].apply(score_text_blob)\n",
        "df_analysis[\"prod_score\"] = df_analysis[\"model_response\"].apply(score_text_blob, \n",
        "                                                                args=(\"prod\",))\n",
        "df_analysis[\"mean_score\"] = df_analysis[\"model_response\"].apply(score_text_blob, \n",
        "                                                                args=(\"mean\",))\n",
        "df_analysis[\"hmean_score\"] = df_analysis[\"model_response\"].apply(score_text_blob, \n",
        "                                                                args=(\"hmean\",))\n",
        "rt = (time.time() - st) / 60 # minutes\n",
        "print(f\"the total runtime was {rt} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VFewiDThHgv"
      },
      "source": [
        "## basic - plotly\n",
        "\n",
        "- [plotly express docs](https://plotly.com/python/plotly-express/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUlZRKzrWMno"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig_temp = px.histogram(df_analysis, x=\"mean_score\", color=\"topk\",\n",
        "                        marginal=\"box\", # or violin, rug\n",
        "                        hover_data=df_analysis.columns, template=\"plotly_dark\",\n",
        "                        title=\"Text Gen Mean Score Dist - various top_k values\",\n",
        "                        height=720, width=int(720*1.613))\n",
        "fig_temp.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztTFat5Ed0cf"
      },
      "source": [
        "fig_temp = px.histogram(df_analysis, x=\"prod_score\", color=\"topk\",\n",
        "                        marginal=\"box\", # or violin, rug\n",
        "                        hover_data=df_analysis.columns, template=\"ggplot2\",\n",
        "                        title=\"Product Score Dist for different top_k values\",\n",
        "                        log_y=True, \n",
        "                        height=720, width=int(720*1.613))\n",
        "fig_temp.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47h82DB1feLb"
      },
      "source": [
        "df_analysis.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe9DhFcaes5C"
      },
      "source": [
        "px.scatter_matrix(df_analysis, template=\"plotly_dark\", \n",
        "                    dimensions=['temp', 'topk', 'top_p','geom_score', \n",
        "                                'mean_score', 'hmean_score', 'prod_score'],\n",
        "                    height=1080, width=int(1080*1.613),\n",
        "                    title=\"scatter matrix - all quant vars\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637mHFl588-N"
      },
      "source": [
        "## autoviz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AR99vX70vu9X"
      },
      "source": [
        "save_av = True #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Xe1p0D9IhG"
      },
      "source": [
        "%%capture\n",
        "!pip install -U autoviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIFwVxNA-ssc"
      },
      "source": [
        "from autoviz.AutoViz_Class import AutoViz_Class\n",
        "\n",
        "AV = AutoViz_Class()\n",
        "filename = \"for_autoviz.csv\"\n",
        "\n",
        "df_analysis.to_csv(filename, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xClTiYHi-ztz"
      },
      "source": [
        "sep = \",\"\n",
        "dft = AV.AutoViz(\n",
        "    filename,\n",
        "    sep=\",\",\n",
        "    depVar=\"prod_score\",\n",
        "    dfte=None,\n",
        "    header=0,\n",
        "    verbose=2,\n",
        "    lowess=True,\n",
        "    chart_format=\"svg\",\n",
        "    max_rows_analyzed=150000,\n",
        "    max_cols_analyzed=30,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeWQCS31pKOB"
      },
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "autopath = Path(\"/content/AutoViz_Plots\")\n",
        "av_arc = \"GPT-Model Hyperparam Opt_autoviz_plots_{}\".format(timestamp)\n",
        "shutil.make_archive(av_arc, \"gztar\", autopath)\n",
        "\n",
        "if save_av: put_in_dropbox(av_arc + \".tar.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQzqDKSB4cym"
      },
      "source": [
        "## sweetviz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLPiuLqN4cyy"
      },
      "source": [
        "!pip install -q sweetviz\n",
        "import sweetviz as sv\n",
        "\n",
        "df_sv = df_analysis.copy().convert_dtypes()\n",
        "\n",
        "feature_config = sv.FeatureConfig(skip=['prompt', 'speaker', 'model_response',],\n",
        "                                  force_num=['temp', 'topk', 'top_p',\n",
        "                                             'geom_score', 'prod_score'],\n",
        "                                  )\n",
        "\n",
        "\n",
        "\n",
        "df_sv = df_sv[['temp', 'topk', 'top_p', 'geom_score', 'prod_score']]\n",
        "\n",
        "df_sv = df_sv.astype(float)\n",
        "\n",
        "df_sv[\"model_response\"] = df_analysis[\"model_response\"].astype(str)\n",
        "df_sv[\"model_response\"] = df_sv[\"model_response\"].astype(\"string\")\n",
        "df_sv.info()\n",
        "# df_sv[\"topk\"] = df_sv[\"topk\"].\n",
        "\n",
        "  astype(float)\n",
        "# df_sv[\"topk\"] = df_sv[\"topk\"].astype(float)\n",
        "# arameters are skip, force_cat, force_num and force_text. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awgOo3WM4cyz"
      },
      "source": [
        "my_report = sv.analyze(df_sv, \n",
        "                       target_feat=\"prod_score\")\n",
        "\n",
        "sv_eda_name = \"GPT-Model Hyperparam Opt-SWEETVIZ-EDA-{}.html\".format(timestamp)\n",
        "\n",
        "my_report.show_html(filepath=sv_eda_name,\n",
        "                    layout='vertical',) # Default arguments will generate to \"SWEE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r85YSapL4cyz"
      },
      "source": [
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(sv_eda_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUAn8N4Y4cy0"
      },
      "source": [
        "put_in_dropbox(sv_eda_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Hq36wgDGF8"
      },
      "source": [
        "## data table + export "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUT20qJODJ6Z"
      },
      "source": [
        "from google.colab import *\n",
        "\n",
        "\n",
        "data_table.DataTable(df_analysis, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vdKLWUyDK2L"
      },
      "source": [
        "df_analysis.reset_index(drop=True, inplace=True)\n",
        "data_header = \"GPT-Model Hyperparam Analysis w Metrics - {}\".format(timestamp)\n",
        "df_analysis.to_excel(data_header + \".xlsx\")\n",
        "put_in_dropbox(data_header + \".xlsx\")\n",
        "\n",
        "df_analysis.to_feather(data_header + \".ftr\")\n",
        "put_in_dropbox(data_header + \".xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HJpIS6V_UPi"
      },
      "source": [
        "## pandas profiling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-22dBq03naW"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTwrpwVk_Vax"
      },
      "source": [
        "!pip uninstall -y -q pandas-profiling\n",
        "!pip install -U -q pandas-profiling\n",
        "\n",
        "import numpy as np\n",
        "from pandas_profiling import ProfileReport"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fge_Euam6iii"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9WDV2R__gR9"
      },
      "source": [
        "df_profile = df_analysis[['temp', 'topk', 'top_p',\n",
        "                          'prod_score', 'mean_score',\n",
        "                          'hmean_score', 'model_response']]\n",
        "profile = ProfileReport(df_profile, dark_mode=True,\n",
        "                        title=\"GPT-Model Hyperparam Opt (Sentence Score)\",\n",
        "                        minimal=True,\n",
        "                        )\n",
        "\n",
        "\n",
        "profile.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Y8l96jBIbG"
      },
      "source": [
        "\n",
        "pd_eda_name = \"GPT-Model Hyperparam Opt-EDA-{}.html\".format(timestamp)\n",
        "profile.to_file(pd_eda_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfDiMaVDTeK4"
      },
      "source": [
        "put_in_dropbox(pd_eda_name)\n",
        "print(\"saved - \", datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}