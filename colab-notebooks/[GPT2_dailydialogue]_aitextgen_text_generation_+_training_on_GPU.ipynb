{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[GPT2-dailydialogue] aitextgen - text generation + training on GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NK7daXR_cAU1"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab847ac4b97243349740f5eccc0c08ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_449c5b81809c462bb3a1a7092d2cab6f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b19763f947ca4cd9a49d408fd58b5b63",
              "IPY_MODEL_8e82657dd9964a208af8c06f4e5aef4e",
              "IPY_MODEL_62c623f96a524c4b92011ebf9bcba37a"
            ]
          }
        },
        "449c5b81809c462bb3a1a7092d2cab6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b19763f947ca4cd9a49d408fd58b5b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28f4dd7717be4a6e9cfa7f4e30a6c5cf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "replacing speaker names: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1529e4683dcd44e6a61858db230da23d"
          }
        },
        "8e82657dd9964a208af8c06f4e5aef4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1de183df7b54b6cae7e11d30dcd471e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 216768,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 216768,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_250916529271410e8205203143686467"
          }
        },
        "62c623f96a524c4b92011ebf9bcba37a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83bdf9b6b46f4034808ea7f2ad9b89c0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 216768/216768 [00:00&lt;00:00, 930878.85it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8eef2ca0e82f439fbacf71f1e526b16e"
          }
        },
        "28f4dd7717be4a6e9cfa7f4e30a6c5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1529e4683dcd44e6a61858db230da23d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1de183df7b54b6cae7e11d30dcd471e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "250916529271410e8205203143686467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83bdf9b6b46f4034808ea7f2ad9b89c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8eef2ca0e82f439fbacf71f1e526b16e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "87d965f445fd4172864957d557efb351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9db391b200eb4704a0595634806798d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57dde068500746c2aae246e5af40d071",
              "IPY_MODEL_b3ad1439c90649f8a4c0fdcc8c6b8abd",
              "IPY_MODEL_8e0f38b141a245cd882b4112d90e6437"
            ]
          }
        },
        "9db391b200eb4704a0595634806798d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "57dde068500746c2aae246e5af40d071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2f651908e908431abc6c04028342318a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_90a55e405b274febab74e3fbe4830d6a"
          }
        },
        "b3ad1439c90649f8a4c0fdcc8c6b8abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_40cc7bff21074f0c806c26415c0953a7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 216768,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 216768,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9cdf3376f3b04d1ba6f7b98afec6a762"
          }
        },
        "8e0f38b141a245cd882b4112d90e6437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a8a3142595d490ea09bc4f000cf06e4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 216768/216768 [00:02&lt;00:00, 77906.06it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98f87b8bd1c54034befea163f0b843a0"
          }
        },
        "2f651908e908431abc6c04028342318a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "90a55e405b274febab74e3fbe4830d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40cc7bff21074f0c806c26415c0953a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9cdf3376f3b04d1ba6f7b98afec6a762": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a8a3142595d490ea09bc4f000cf06e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98f87b8bd1c54034befea163f0b843a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8ce9a6c7fda4d12ac806649c822d75b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_46af5b2857b54abba1e3196d84f788dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_14e9ef3e8d4a432db42d111bfac2ef47",
              "IPY_MODEL_6e48e606a1584b8b9bdf79f99c99908c",
              "IPY_MODEL_2059415d9d564e98834cb583bde9250d"
            ]
          }
        },
        "46af5b2857b54abba1e3196d84f788dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "14e9ef3e8d4a432db42d111bfac2ef47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d7b7ca59499f4b4a8d356072d47dd4f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Loss: 1.230 — Avg: 1.232 — GPU Mem: 14087 MB:   9%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97d0d005509d40dbaa85d28bceff2d29"
          }
        },
        "6e48e606a1584b8b9bdf79f99c99908c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_06eb4c9af6284798bf508c2fe9e96f11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 75000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6920,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00b968d7fced4bc6b0a7b7788e3d8a31"
          }
        },
        "2059415d9d564e98834cb583bde9250d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d5299c65dc34715996f809966bbcbd4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6920/75000 [25:24&lt;4:09:59,  4.54it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5adbb27d8cd42ab9739851d7ba150f7"
          }
        },
        "d7b7ca59499f4b4a8d356072d47dd4f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97d0d005509d40dbaa85d28bceff2d29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06eb4c9af6284798bf508c2fe9e96f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00b968d7fced4bc6b0a7b7788e3d8a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d5299c65dc34715996f809966bbcbd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5adbb27d8cd42ab9739851d7ba150f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/main/colab-notebooks/%5BGPT2_dailydialogue%5D_aitextgen_text_generation_%2B_training_on_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  aitextgen — Train a GPT-2 (or GPT Neo) Text-Generating Model w/ GPU\n",
        "\n",
        "by [Max Woolf](https://minimaxir.com)\n",
        "\n",
        "*Last updated: May 16th, 2021 (aitextgen v0.5.2)*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Colaboratory** using `aitextgen`!\n",
        "\n",
        "For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/).\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgMEhF7J3ubw",
        "outputId": "416af75a-1095-48c1-b777-be184e4041c7"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 18 08:19:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    49W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "## formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CemDfmdgjefZ"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "_UJ8ek31Uz-r",
        "outputId": "eee6435d-1095-4656-dc22-c1ecf7c16198"
      },
      "source": [
        "# update torch in case using a A100 GPU\n",
        "!pip3 install torch==1.9.1+cu111 -f -q https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "!pip3 install cudatoolkit"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: -q\n",
            "Collecting https://download.pytorch.org/whl/torch_stable.html\n",
            "  Downloading https://download.pytorch.org/whl/torch_stable.html (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 508 kB/s \n",
            "\u001b[31m  ERROR: Cannot unpack file /tmp/pip-unpack-6qk9i48m/torch_stable.html (downloaded from /tmp/pip-req-build-5vzjx2vt, content-type: text/html); cannot detect archive format\u001b[0m\n",
            "\u001b[31mERROR: Cannot determine archive format of /tmp/pip-req-build-5vzjx2vt\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement cudatoolkit (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for cudatoolkit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "c80cba42-1af8-4d14-c68f-2e0c3821124c"
      },
      "source": [
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DuC67no59vMe",
        "outputId": "297761af-d169-4dbf-e2f1-4cd8846e0e1d"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses a Nvidia P4, an Nvidia T4, an Nvidia P100, or an Nvidia V100. For finetuning GPT-2 124M, any of these GPUs will be fine, but for text generation, a T4 or a P100 is ideal since they have more VRAM. **If you receive a T4 or a V100 GPU, you can enable `fp16=True` during training for faster/more memory efficient training.**\n",
        "\n",
        "You can verify which GPU is active by running the cell below. If you want to try for a different GPU, go to **Runtime -> Factory Reset Runtime**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "sUmTooTW3osf",
        "outputId": "c33a05fe-2721-4aad-c663-4432a6f1f7e1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 18 08:19:23 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    49W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "## Loading GPT-2 or GPT Neo\n",
        "\n",
        "If you're retraining a model on new text, you need to download and load the GPT-2 model into the GPU. \n",
        "\n",
        "There are several sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M` (default): the \"medium\" model, 1.5GB on disk.\n",
        "* `774M` (default): the \"large\" model, 3GB on disk.\n",
        "\n",
        "You can also finetune a GPT Neo model instead, which is more suitable for longer texts and the base model has more recent data:\n",
        "\n",
        "* `125M`: Analogous to the GPT-2 124M model.\n",
        "* `350M`: Analogous to the GPT-2 355M model\n",
        "\n",
        "The next cell downloads the model and saves it in the Colaboratory VM. If the model has already been downloaded, running this cell will reload it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXIbld-ASHL9",
        "outputId": "c9777b62-5e89-47e4-c754-e13037949727"
      },
      "source": [
        "model_size = \"355M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = True #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Programming/AI_peter/gpt2_dailydialogue_gpu_355M\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "flqSlHjMIeIw",
        "outputId": "baa4b6ce-30b0-42a6-cab9-43c9850f8769"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=True,\n",
        "                   gradient_checkpointing=True)\n",
        "else:\n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=True,\n",
        "                gradient_checkpointing=True)\n",
        "# ai = aitextgen(tf_gpt2=\"124M\", to_gpu=True)\n",
        "# https://huggingface.co/distilgpt2\n",
        "# Comment out the above line and uncomment the below line to use GPT Neo instead.\n",
        "# model_size\n",
        "# ai = aitextgen(model='gpt2-medium', \n",
        "#                to_gpu=True, \n",
        "#                gradient_checkpointing=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/18/2021 08:19:24 — INFO — aitextgen — Loading model from provided weights and config in //content/drive/MyDrive/Programming/AI_peter/gpt2_dailydialogue_gpu_355M.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n",
            "10/18/2021 08:19:28 — INFO — aitextgen — GPT2 loaded with 354M parameters.\n",
            "10/18/2021 08:19:28 — INFO — aitextgen — Gradient checkpointing enabled for model training.\n",
            "10/18/2021 08:19:28 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7oQAqQHiFiX"
      },
      "source": [
        "# load training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jUWsiVOCUlLo",
        "outputId": "5684b910-de94-454b-f977-026acf608c1b"
      },
      "source": [
        "dl_link = \"https://www.dropbox.com/s/zngfhi9zw438fxk/daily_dialogue_augment.txt?dl=1\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hirJkLvbUrMb",
        "outputId": "9b5cad3e-b62a-4f17-8b28-c0ee1a35edc8"
      },
      "source": [
        "# download test image\n",
        "from urllib import request\n",
        "from os.path import join\n",
        "import os\n",
        "vm_wd = os.getcwd()\n",
        "local_name = join(vm_wd, \"dailydialogue.txt\")\n",
        "request.urlretrieve(dl_link, local_name)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/dailydialogue.txt', <http.client.HTTPMessage at 0x7f3e3c3361d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ab847ac4b97243349740f5eccc0c08ca",
            "449c5b81809c462bb3a1a7092d2cab6f",
            "b19763f947ca4cd9a49d408fd58b5b63",
            "8e82657dd9964a208af8c06f4e5aef4e",
            "62c623f96a524c4b92011ebf9bcba37a",
            "28f4dd7717be4a6e9cfa7f4e30a6c5cf",
            "1529e4683dcd44e6a61858db230da23d",
            "d1de183df7b54b6cae7e11d30dcd471e",
            "250916529271410e8205203143686467",
            "83bdf9b6b46f4034808ea7f2ad9b89c0",
            "8eef2ca0e82f439fbacf71f1e526b16e"
          ]
        },
        "id": "cEWZnWyYX3y8",
        "outputId": "b17a515f-98d7-41e5-e3fb-e9b7e5ff690f"
      },
      "source": [
        "# adjust names \n",
        "\n",
        "\n",
        "with open(local_name, 'r', encoding='utf-8', errors='ignore') as fi:\n",
        "    orig_lines = fi.readlines()\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "upd_lines = []\n",
        "\n",
        "for line in tqdm(orig_lines, total=len(orig_lines), desc=\"replacing speaker names\"):\n",
        "    fixline = line.replace(\"speaker a\", \"john smith\")\n",
        "    fixline = fixline.replace(\"speaker b\", \"nancy sellers\")\n",
        "    upd_lines.append(fixline)\n",
        "\n",
        "local_namev2 = join(vm_wd, \"V2dailydialogue.txt\")\n",
        "\n",
        "with open(local_namev2, 'w', encoding='utf-8', errors='ignore') as fo:\n",
        "    fo.writelines(upd_lines)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab847ac4b97243349740f5eccc0c08ca",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "replacing speaker names:   0%|          | 0/216768 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "Is-lpOMrZGyE",
        "outputId": "089180e5-2ad3-427c-a3a0-25e6506831b8"
      },
      "source": [
        "import pprint as pp\n",
        "\n",
        "pp.pprint(upd_lines[:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['john smith:\\n',\n",
            " 'say, jim, how about going for a few beers after dinner?\\n',\n",
            " '\\n',\n",
            " 'nancy sellers:\\n',\n",
            " 'you know that is tempting but is really not good for our fitness.\\n',\n",
            " '\\n',\n",
            " 'john smith:\\n',\n",
            " 'what do you mean? it will help us to relax.\\n',\n",
            " '\\n',\n",
            " 'nancy sellers:\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6OFnPCLADfll",
        "outputId": "2ae63558-bd0d-4fcd-b447-c8b6f47983cd"
      },
      "source": [
        "file_name = local_namev2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "If your text file is large (>10MB), it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM.\n",
        "\n",
        "Additionally, you may want to consider [compressing the dataset to a cache first](https://docs.aitextgen.io/dataset/) on your local computer, then uploading the resulting `dataset_cache.tar.gz` and setting the `file_name`in the previous cell to that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-Z6okFD8VKtS",
        "outputId": "005252a2-ab59-449f-b11f-cd4b85b36691"
      },
      "source": [
        "# copy_file_from_gdrive(local_name)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "# Train / Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2 in aitextgen. It runs for `num_steps`, and a progress bar will appear to show training progress, current loss (the lower the better the model), and average loss (to give a sense on loss trajectory).\n",
        "\n",
        "The model will be saved every `save_every` steps in `trained_model` by default, and when training completes. If you mounted your Google Drive, the model will _also_ be saved there in a unique folder.\n",
        "\n",
        "The training might time out after 4ish hours; if you did not mount to Google Drive, make sure you end training and save the results so you don't lose them! (if this happens frequently, you may want to consider using [Colab Pro](https://colab.research.google.com/signup))\n",
        "\n",
        "Important parameters for `train()`:\n",
        "\n",
        "- **`line_by_line`**: Set this to `True` if the input text file is a single-column CSV, with one record per row. aitextgen will automatically process it optimally.\n",
        "- **`from_cache`**: If you compressed your dataset locally (as noted in the previous section) and are using that cache file, set this to `True`.\n",
        "- **`num_steps`**: Number of steps to train the model for.\n",
        "- **`generate_every`**: Interval of steps to generate example text from the model; good for qualitatively validating training.\n",
        "- **`save_every`**: Interval of steps to save the model: the model will be saved in the VM to `/trained_model`.\n",
        "- **`save_gdrive`**: Set this to `True` to copy the model to a unique folder in your Google Drive, if you have mounted it in the earlier cells\n",
        "- **`fp16`**: Enables half-precision training for faster/more memory-efficient training. Only works on a T4 or V100 GPU.\n",
        "\n",
        "Here are other important parameters for `train()` that are useful but you likely do not need to change.\n",
        "\n",
        "- **`learning_rate`**: Learning rate of the model training.\n",
        "- **`batch_size`**: Batch size of the model training; setting it too high will cause the GPU to go OOM. (if using `fp16`, you can increase the batch size more safely)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sDmyL2A7uhaL",
        "outputId": "7763a871-3361-40f9-9ab7-bb924508481c"
      },
      "source": [
        "import gc, os\n",
        "from os.path import join\n",
        "\n",
        "base_dir = \"/content/drive/MyDrive/Programming/AI_peter\"\n",
        "temp_gpu_path = join(base_dir, \"gp2_dailydialogue_{}\".format(model_size))\n",
        "os.makedirs(temp_gpu_path, exist_ok=True)\n",
        "gc.collect()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "87d965f445fd4172864957d557efb351",
            "9db391b200eb4704a0595634806798d3",
            "57dde068500746c2aae246e5af40d071",
            "b3ad1439c90649f8a4c0fdcc8c6b8abd",
            "8e0f38b141a245cd882b4112d90e6437",
            "2f651908e908431abc6c04028342318a",
            "90a55e405b274febab74e3fbe4830d6a",
            "40cc7bff21074f0c806c26415c0953a7",
            "9cdf3376f3b04d1ba6f7b98afec6a762",
            "2a8a3142595d490ea09bc4f000cf06e4",
            "98f87b8bd1c54034befea163f0b843a0",
            "f8ce9a6c7fda4d12ac806649c822d75b",
            "46af5b2857b54abba1e3196d84f788dd",
            "14e9ef3e8d4a432db42d111bfac2ef47",
            "6e48e606a1584b8b9bdf79f99c99908c",
            "2059415d9d564e98834cb583bde9250d",
            "d7b7ca59499f4b4a8d356072d47dd4f4",
            "97d0d005509d40dbaa85d28bceff2d29",
            "06eb4c9af6284798bf508c2fe9e96f11",
            "00b968d7fced4bc6b0a7b7788e3d8a31",
            "2d5299c65dc34715996f809966bbcbd4",
            "a5adbb27d8cd42ab9739851d7ba150f7"
          ]
        },
        "id": "aeXshJM-Cuaf",
        "outputId": "71d467b4-2dfd-4b84-eecd-70f9a587272d"
      },
      "source": [
        "# DO NOT USE WARMUP STEPS\n",
        "\n",
        "ai.train(file_name,\n",
        "         output_dir=temp_gpu_path,\n",
        "         line_by_line=False,\n",
        "         from_cache=False,\n",
        "         num_steps=75000, # takes about 5 hours on 16 gb GPU for 75000\n",
        "         generate_every=1500,\n",
        "         max_grad_norm=0.8,\n",
        "         save_every=1000,\n",
        "         gradient_accumulation_steps=1,\n",
        "         save_gdrive=False,\n",
        "         learning_rate=1e-3,\n",
        "         fp16=True,\n",
        "         batch_size=1, \n",
        "         fp16_opt_level=\"O1\",\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/18/2021 08:19:36 — INFO — aitextgen — Loading text from /content/V2dailydialogue.txt with generation length of 1024.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87d965f445fd4172864957d557efb351",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/216768 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/18/2021 08:19:36 — INFO — aitextgen.TokenDataset — Encoding 216,768 sets of tokens from /content/V2dailydialogue.txt.\n",
            "10/18/2021 08:19:39 — WARNING — aitextgen — pytorch_model.bin already exists in //content/drive/MyDrive/Programming/AI_peter/gp2_dailydialogue_355M and will be overwritten!\n",
            "10/18/2021 08:19:39 — INFO — pytorch_lightning.trainer.connectors.accelerator_connector — Using native 16bit precision.\n",
            "10/18/2021 08:19:39 — INFO — pytorch_lightning.utilities.distributed — GPU available: True, used: True\n",
            "10/18/2021 08:19:39 — INFO — pytorch_lightning.utilities.distributed — TPU available: False, using: 0 TPU cores\n",
            "10/18/2021 08:19:39 — INFO — pytorch_lightning.utilities.distributed — IPU available: False, using: 0 IPUs\n",
            "10/18/2021 08:19:39 — INFO — pytorch_lightning.accelerators.gpu — LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8ce9a6c7fda4d12ac806649c822d75b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/75000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/precision/precision_plugin.py:141: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  torch.nn.utils.clip_grad_norm_(parameters, clip_val)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1,000 steps reached: saving model to //content/drive/MyDrive/Programming/AI_peter/gp2_dailydialogue_355M\u001b[0m\n",
            "\u001b[1m1,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            ". i think i'll be going to go to work right away.\n",
            "\n",
            "john smith:\n",
            "excuse me, sir. could i have your name?\n",
            "\n",
            "nancy sellers:\n",
            "here's your card, sir.\n",
            "\n",
            "john smith:\n",
            "thank you. what's your bill for?\n",
            "\n",
            "nancy sellers:\n",
            "do you need your account number?\n",
            "\n",
            "john smith:\n",
            "yes, here's your bill.\n",
            "\n",
            "nancy sellers:\n",
            "here's your bill, i know your bill here.\n",
            "\n",
            "john smith:\n",
            "thank you.\n",
            "\n",
            "nancy sellers:\n",
            "hello, sir. can you make a reservation?\n",
            "\n",
            "john smith:\n",
            "sure, how dare you?\n",
            "\n",
            "nancy sellers:\n",
            "i think i will be right back.\n",
            "\n",
            "john smith:\n",
            "ok, here is a bill.\n",
            "\n",
            "nancy sellers:\n",
            "would you like to make a reservation for friday?\n",
            "\n",
            "john smith:\n",
            "sure. we have many plans for friday is a birthday for you.\n",
            "\n",
            "nancy sellers:\n",
            "if that's the case, we'll pick you up at the last moment.\n",
            "\n",
            "john smith:\n",
            "so, i'll have a good\n",
            "==========\n",
            "\u001b[1m2,000 steps reached: saving model to //content/drive/MyDrive/Programming/AI_peter/gp2_dailydialogue_355M\u001b[0m\n",
            "\u001b[1m3,000 steps reached: saving model to //content/drive/MyDrive/Programming/AI_peter/gp2_dailydialogue_355M\u001b[0m\n",
            "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            " to make a quick decision.\n",
            "\n",
            "nancy sellers:\n",
            "i'd love to get a parking meter.\n",
            "\n",
            "john smith:\n",
            "here, give me a look at here... it looks like a parking lot is crowded.\n",
            "\n",
            "nancy sellers:\n",
            "i was just wondering if we can turn right at this traffic jam.\n",
            "\n",
            "john smith:\n",
            "ok, let me see you. you can take a walk in the parking lot.\n",
            "\n",
            "nancy sellers:\n",
            "where will we get the parking lot?\n",
            "\n",
            "john smith:\n",
            "in taichung, near suzieurich.\n",
            "\n",
            "nancy sellers:\n",
            "can we help you with your bags?\n",
            "\n",
            "john smith:\n",
            "yes, i'd like to check out a lot of stuff.\n",
            "\n",
            "nancy sellers:\n",
            "sure. here you are.\n",
            "\n",
            "john smith:\n",
            "what do you want to do?\n",
            "\n",
            "nancy sellers:\n",
            "i'd like to check out some taiwanese ones.\n",
            "\n",
            "john smith:\n",
            "i'm sure it will be fine.\n",
            "\n",
            "nancy sellers:\n",
            "i'm sorry, but i'm not sure what i can do.\n",
            "\n",
            "john smith:\n",
            "well, tell me.\n",
            "==========\n",
            "\u001b[1m4,000 steps reached: saving model to //content/drive/MyDrive/Programming/AI_peter/gp2_dailydialogue_355M\u001b[0m\n",
            "\u001b[1m4,500 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            "-taking isn't the fault.\n",
            "\n",
            "john smith:\n",
            "i'm afraid i have no choice. i still have a choice.\n",
            "\n",
            "nancy sellers:\n",
            "i don't mind. i'll take care of myself.\n",
            "\n",
            "john smith:\n",
            "i'm doing great, thank you.\n",
            "\n",
            "nancy sellers:\n",
            "i'm happy to be of help.\n",
            "\n",
            "john smith:\n",
            "well, don't mention me.\n",
            "\n",
            "nancy sellers:\n",
            "i'm not sure if i can do it well, there are too many options.\n",
            "\n",
            "john smith:\n",
            "i don't want to ask anyone for help for help.\n",
            "\n",
            "nancy sellers:\n",
            "i like to ask his former friends.\n",
            "\n",
            "john smith:\n",
            "i like to ask them for help.\n",
            "\n",
            "nancy sellers:\n",
            "have you been doing your homework?\n",
            "\n",
            "john smith:\n",
            "yes, but i'm not in the mood to learn anything new.\n",
            "\n",
            "nancy sellers:\n",
            "but you're going to have a party tonight?\n",
            "\n",
            "john smith:\n",
            "yes, i'm going to have a party.\n",
            "\n",
            "nancy sellers:\n",
            "i'm going to the party too.\n",
            "\n",
            "john smith:\n",
            "\n",
            "==========\n",
            "\u001b[1m5,000 steps reached: saving model to //content/drive/MyDrive/Programming/AI_peter/gp2_dailydialogue_355M\u001b[0m\n",
            "\u001b[1m6,000 steps reached: saving model to //content/drive/MyDrive/Programming/AI_peter/gp2_dailydialogue_355M\u001b[0m\n",
            "\u001b[1m6,000 steps reached: generating sample texts.\u001b[0m\n",
            "==========\n",
            ":\n",
            "what? you are a little short.\n",
            "\n",
            "nancy sellers:\n",
            "i'm not sure. i thought i'd go to the store to see if there is any other small problem with it.\n",
            "\n",
            "john smith:\n",
            "good afternoon, i am looking for a pan pan.\n",
            "\n",
            "nancy sellers:\n",
            "is this a particular one?\n",
            "\n",
            "john smith:\n",
            "no, this is the latest style.\n",
            "\n",
            "nancy sellers:\n",
            "this one is very nice.\n",
            "\n",
            "john smith:\n",
            "i'd be happy to get it.\n",
            "\n",
            "nancy sellers:\n",
            "what style is it?\n",
            "\n",
            "john smith:\n",
            "it's very lightweight.\n",
            "\n",
            "nancy sellers:\n",
            "i want to try it on.\n",
            "\n",
            "john smith:\n",
            "this is the pan.\n",
            "\n",
            "nancy sellers:\n",
            "i'm happy to hear that.\n",
            "\n",
            "john smith:\n",
            "i'm happy that this is a very nice pan.\n",
            "\n",
            "nancy sellers:\n",
            "yes, very lightweight. it's very hot.\n",
            "\n",
            "john smith:\n",
            "i've found some similar similar work here.\n",
            "\n",
            "nancy sellers:\n",
            "this one is very nice.\n",
            "\n",
            "john smith:\n",
            "what's it,\n",
            "==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DLJJolCYQQH"
      },
      "source": [
        "save_path = \"/content/drive/MyDrive/Programming/AI_peter/gpt2_dailydialogue_gpu_{}\".format(model_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgUz5m0i1pbj"
      },
      "source": [
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "ai.save(save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L"
      },
      "source": [
        "\n",
        "# Use a Train Model for Generation\n",
        "\n",
        "If you already had a trained model from this notebook, running the next cell will copy the `pytorch_model.bin` and the `config.json`file from the specified folder in Google Drive into the Colaboratory VM. (If no `from_folder` is specified, it assumes the two files are located at the root level of your Google Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeznI_VeaDQn"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alooLLyx1ZGu"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD"
      },
      "source": [
        "# best model thus far @ 1.3B parameters and tuned for 50k steps\n",
        "# from_folder = \"/content/drive/MyDrive/Programming/AI_peter/GPT-Neo-1B-V1\"\n",
        "\n",
        "from_folder = save_path\n",
        "\n",
        "if len(from_folder) > 2:\n",
        "\n",
        "    for file in [\"pytorch_model.bin\", \"config.json\"]:\n",
        "        if from_folder:\n",
        "            copy_file_from_gdrive(file, from_folder)\n",
        "        else:\n",
        "            copy_file_from_gdrive(file)\n",
        "\n",
        "    ai = aitextgen(model_folder=from_folder, to_gpu=True)\n",
        "else:\n",
        "    ai = aitextgen(model_folder=\".\", to_gpu=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generate Text From The Trained Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cd0RGDbJiDp"
      },
      "source": [
        "`generate()` without any parameters generates a single text from the loaded model to the console."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL"
      },
      "source": [
        "ai.generate(n=3, max_length=256, \n",
        "            temperature=1.0, top_p=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fSH7QgiiGi7"
      },
      "source": [
        "ai.generate(prompt=\"give me a good pickup line!\", temperature=1,\n",
        "            min_length=10, batch_size =20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = ai.generate_one()`\n",
        "\n",
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N"
      },
      "source": [
        "ai.generate(\n",
        "    n=3, batch_size=25, prompt=\"i just\", max_length=256, \n",
        "    temperature=1.0, top_p=0.9\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2"
      },
      "source": [
        "For bulk generation, you can generate a large amount of texts to a file and sort out the samples locally on your computer. The next cell will generate `num_files` files, each with `n` texts and whatever other parameters you would pass to `generate()`. The files can then be downloaded from the Files sidebar!\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "rKp18dTTj402"
      },
      "source": [
        "save_loc = \"/content/drive/MyDrive/Programming/AI_peter/output_files\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0"
      },
      "source": [
        "p_list = [[\"nancy powell:\"+\"\\n\", \n",
        "           \"how are you doing?\"+\"\\n\", \"\\n\", \n",
        "           \"john smith:\" + \"\\n\"], \n",
        "           [\"nancy powell:\"+\"\\n\", \"why is \"],\n",
        "           [\"can you help me with my homework?\"+\"\\n\", \"\\n\", \n",
        "            \"john smith:\" + \"\\n\"],\n",
        "           [\"john smith:\" + \"\\n\"],\n",
        "           [\"Hey I’m meeting the astrophysics professor via zoom after school any tips?\"+\"\\n\",\n",
        "            \"\\n\", \"john smith:\" + \"\\n\"], \n",
        "]\n",
        "\n",
        "\n",
        "prompts = [\"\".join(line) for line in p_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8et1WHilo_A"
      },
      "source": [
        "from datetime import datetime\n",
        "import pprint as pp\n",
        "\n",
        "ds_date_time = datetime.now().strftime(\"%m.%d.%Y\")\n",
        "\n",
        "base_header = \"gpt-dailydialogue-textgen-{}\".format(ds_date_time)\n",
        "prompt_IDs = [base_header + \"_file-{}.txt\".format(i+1) for i in range(5, len(prompts)+11)]\n",
        "\n",
        "prompt_mng = {}\n",
        "for pid, text in zip(prompt_IDs, prompts):\n",
        "    prompt_mng[pid] = text\n",
        "pp.pprint(prompt_mng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPnqt-UVlEPR"
      },
      "source": [
        "from os.path import join\n",
        "\n",
        "for pfile, my_prompt in prompt_mng.items():\n",
        "    ai.generate_to_file(\n",
        "        n=50,\n",
        "        batch_size=5,\n",
        "        prompt=my_prompt,\n",
        "        max_length=512,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "        destination_path=join(save_loc, pfile)\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}