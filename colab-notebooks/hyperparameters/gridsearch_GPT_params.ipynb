{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gridsearch_GPT_params.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "88KhaHME-ZVq",
        "NK7daXR_cAU1",
        "KNhVl3R7yimM",
        "2VFewiDThHgv",
        "637mHFl588-N"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM8AR0HQBiWj8NEjaXFZXg1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a294874fa3145389f0fcf1853d38148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_11db70df106c462090c99144cea46411",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_66a8fc7874644670b91a64d75ada22e6",
              "IPY_MODEL_c3288c1c89664e30929d367f6a7e0557",
              "IPY_MODEL_929ab4baebf7461ea006b74c787b6c3c"
            ]
          }
        },
        "11db70df106c462090c99144cea46411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "66a8fc7874644670b91a64d75ada22e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ebb94c4c60ae466ca11c767ec41a9a13",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "completing grid search...:  12%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c12fed11efe4bbf84b9dad63b6c5c5a"
          }
        },
        "c3288c1c89664e30929d367f6a7e0557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_845a338a99cf47b797f2c8e520102edd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 5000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 578,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d7904ff8b7a49d6930f6c927c61f494"
          }
        },
        "929ab4baebf7461ea006b74c787b6c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_34be3b9d58de49149129e920f7fe4bc1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 578/5000 [1:56:21&lt;14:46:15, 12.03s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22e3102462144dc4a46485c4395d3678"
          }
        },
        "ebb94c4c60ae466ca11c767ec41a9a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c12fed11efe4bbf84b9dad63b6c5c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "845a338a99cf47b797f2c8e520102edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d7904ff8b7a49d6930f6c927c61f494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34be3b9d58de49149129e920f7fe4bc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22e3102462144dc4a46485c4395d3678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pszemraj/ai-msgbot/blob/main/colab-notebooks/hyperparameters/gridsearch_GPT_params.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88KhaHME-ZVq"
      },
      "source": [
        "# Grid Search for Parameters\n",
        "\n",
        "- optimal \"text bot\" hyperparamters are not really known, nor is there a right answer\n",
        "- proposal: iterate through \"reasonable\" parameter ranges, record output, compute sentence coherence scoring on outputs\n",
        "- evaluate results once finished"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPqNVijEZ6dU",
        "outputId": "40117ec9-dbc9-4e8f-9b6a-df5972feb6c6"
      },
      "source": [
        "!pip install -U -q pandas\n",
        "!pip uninstall -y -q pyarrow\n",
        "!pip install -U -q pyarrow\n",
        "!pip install -U -q openpyxl\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.3 MB 23.8 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 23.6 MB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 242 kB 29.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7daXR_cAU1"
      },
      "source": [
        "## formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztfEh86cDCR"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "# colab formatting\n",
        "def set_css():\n",
        "    display(\n",
        "        HTML(\n",
        "            \"\"\"\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  \"\"\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "get_ipython().events.register(\"pre_run_cell\", set_css)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8U_usB0yPkh"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Nn3e175uDlXC",
        "outputId": "b44931af-6d47-47cf-82e0-3edd1684cce4"
      },
      "source": [
        "!pip3 install -q torch==1.9.1+cpu  -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 175.4 MB 11 kB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 8.6 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 22.5 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.9.1+cpu which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "_mh0LUOiyZdc",
        "outputId": "93057101-6fd4-4567-90e7-c0f1c63fb58b"
      },
      "source": [
        "%%capture\n",
        "!pip install -U tqdm\n",
        "!pip install clean-text\n",
        "\n",
        "import argparse\n",
        "import gc\n",
        "import os\n",
        "import pprint as pp\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from cleantext import clean\n",
        "warnings.filterwarnings(action=\"ignore\", message=\".*gradient_checkpointing*\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_AuKgHyDktV"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "KBkpRgBCBS2_",
        "outputId": "582b62bc-fb6d-448e-9326-944217e514d2"
      },
      "source": [
        "!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 572 kB 28.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 925 kB 54.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 58.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 282 kB 57.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 73.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 71.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 67.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 55.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 73.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 68.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 69.9 MB/s \n",
            "\u001b[?25h  Building wheel for aitextgen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMtCgKhZ_YW2"
      },
      "source": [
        "# gdrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DuC67no59vMe",
        "outputId": "6f312edc-4504-454f-c8cd-936fffe085c4"
      },
      "source": [
        "mount_gdrive()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNhVl3R7yimM"
      },
      "source": [
        "# functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q9I5TpuFZJc"
      },
      "source": [
        "## query_gpt_peter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8-6N3jcWyjrV",
        "outputId": "d8238b3a-effc-4c9e-e623-46db68c59321"
      },
      "source": [
        "# should inherit main ai object\n",
        "def query_gpt_peter(\n",
        "                    prompt_msg: str,\n",
        "                    speaker=\"calvin miller\",\n",
        "                    responder=\"peter szemraj\",\n",
        "                    kparam=100,\n",
        "                    temp=0.7,\n",
        "                    nuc_range=0.8,\n",
        "                    verbose=False,\n",
        "                    use_gpu=False,  \n",
        "                    ):\n",
        "\n",
        "\n",
        "    p_list = []\n",
        "    if speaker is not None:\n",
        "        p_list.append(speaker.lower() + \":\" + \"\\n\")  # write prompt as the speaker\n",
        "    p_list.append(prompt_msg.lower() + \"\\n\")\n",
        "    p_list.append(\"\\n\")\n",
        "    p_list.append(responder.lower() + \":\" + \"\\n\")\n",
        "    this_prompt = \"\".join(p_list)\n",
        "    if verbose:\n",
        "        print(\"overall prompt:\\n\")\n",
        "        pp.pprint(this_prompt, indent=4)\n",
        "    this_result = ai.generate(\n",
        "        n=1,\n",
        "        top_k=kparam,\n",
        "        batch_size=512,\n",
        "        max_length=128,\n",
        "        min_length=16,\n",
        "        prompt=this_prompt,\n",
        "        temperature=temp,\n",
        "        top_p=nuc_range,\n",
        "        do_sample=True,\n",
        "        return_as_list=True,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        this_result = str(this_result[0]).split(\"\\n\")\n",
        "        res_out = [str(ele).strip() for ele in this_result]\n",
        "        p_out = [str(ele).strip() for ele in p_list]\n",
        "        diff_list = list(\n",
        "            set(res_out).difference(p_out)\n",
        "        )  # remove prior prompts for the response\n",
        "        this_result = [\n",
        "            str(msg)\n",
        "            for msg in diff_list\n",
        "            if (\":\" not in str(msg))\n",
        "            and (\"szemr\" not in str(msg))\n",
        "            and (\"peter\" not in str(msg))\n",
        "        ]  # remove all names\n",
        "        if not isinstance(this_result, list):\n",
        "            list(this_result)\n",
        "        output = str(this_result[0]).strip()\n",
        "        # add second line of output if first is too short (subjective)\n",
        "        if len(output) < 15 and len(this_result) > 1:\n",
        "            output = output + \" \" + str(this_result[1]).strip()\n",
        "    except:\n",
        "        output = \"<bro, there was an error. try again>\"\n",
        "\n",
        "    p_list.append(output + \"\\n\")\n",
        "    p_list.append(\"\\n\")\n",
        "\n",
        "    model_responses = {\"out_text\": output, \"full_conv\": p_list}\n",
        "\n",
        "    return model_responses\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBIX9RcX1FmQ"
      },
      "source": [
        "## generic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SmXqCHgC1jyE",
        "outputId": "2bb4c829-e1ef-4fa4-d09a-08e694d5f2f1"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# clears cells in jupyter \n",
        "\n",
        "def isnotebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'Shell':\n",
        "            return True  # Colab\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter\n",
        "\n",
        "def clear_jupyter_cell():\n",
        "    is_jupyter = isnotebook()\n",
        "\n",
        "    if is_jupyter:\n",
        "        clear_output(wait=True)\n",
        "    else:\n",
        "        print(\"not in a jupyter notebook\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uEmQyYkq1GpY",
        "outputId": "50978ee9-87fb-4288-d2f8-ba7719113097"
      },
      "source": [
        "\n",
        "def create_folder(directory):\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "\n",
        "def shorten_title(title_text, max_no=35):\n",
        "    if len(title_text) < max_no:\n",
        "        return title_text\n",
        "    else:\n",
        "        return title_text[:max_no] + \"...\"\n",
        "\n",
        "\n",
        "def cleantxt_wrap(ugly_text):\n",
        "    # a wrapper for clean text with options different than default\n",
        "\n",
        "    # https://pypi.org/project/clean-text/\n",
        "    cleaned_text = clean(\n",
        "        ugly_text,\n",
        "        fix_unicode=True,  # fix various unicode errors\n",
        "        to_ascii=True,  # transliterate to closest ASCII representation\n",
        "        lower=True,  # lowercase text\n",
        "        no_line_breaks=True,  # fully strip line breaks as opposed to only normalizing them\n",
        "        no_urls=True,  # replace all URLs with a special token\n",
        "        no_emails=True,  # replace all email addresses with a special token\n",
        "        no_phone_numbers=True,  # replace all phone numbers with a special token\n",
        "        no_numbers=False,  # replace all numbers with a special token\n",
        "        no_digits=False,  # replace all digits with a special token\n",
        "        no_currency_symbols=True,  # replace all currency symbols with a special token\n",
        "        no_punct=True,  # remove punctuations\n",
        "        replace_with_punct=\"\",  # instead of removing punctuations you may replace them\n",
        "        replace_with_url=\"<URL>\",\n",
        "        replace_with_email=\"<EMAIL>\",\n",
        "        replace_with_phone_number=\"<PHONE>\",\n",
        "        replace_with_number=\"<NUM>\",\n",
        "        replace_with_digit=\"0\",\n",
        "        replace_with_currency_symbol=\"<CUR>\",\n",
        "        lang=\"en\",  # set to 'de' for German special handling\n",
        "    )\n",
        "\n",
        "    return cleaned_text"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qJnA9sEpHhn"
      },
      "source": [
        "### dropbox \n",
        "\n",
        "\n",
        "- [tutorial](https://python.plainenglish.io/automate-your-pdf-upload-to-dropbox-python-script-bdacc2c721f6)\n",
        "- [api docs](https://dropbox-sdk-python.readthedocs.io/en/latest/api/dropbox.html?highlight=files_upload#dropbox.dropbox_client.Dropbox.files_upload)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oz_GboGqZrwa",
        "outputId": "ecbaf7a8-b1c3-4112-b57e-60df6082ed81"
      },
      "source": [
        "dropbox_subfolder = \"GPT text gen - gridsearch v2\"  # @param {type:\"string\"}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8rOS5ZVZ1qPJ",
        "outputId": "7f794055-fdad-4022-989d-8de310d67110"
      },
      "source": [
        "!pip install -q dropbox\n",
        "import dropbox"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▋                               | 10 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40 kB 36.9 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 51 kB 38.2 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 61 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 71 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 81 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 92 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 102 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 112 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 122 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 133 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 143 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 153 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 163 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 174 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 184 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 194 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 204 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 215 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 225 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 235 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 245 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 256 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 266 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 276 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 286 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 296 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 307 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 317 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 327 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 337 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 348 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 358 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 368 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 378 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 389 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 399 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 409 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 419 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 430 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 440 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 450 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 460 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 471 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 481 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 491 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 501 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 512 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 522 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 532 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 542 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 552 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 563 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 573 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 577 kB 27.2 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |██                              | 10 kB 34.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 42.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30 kB 50.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40 kB 56.6 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 51 kB 60.2 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 61 kB 64.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 71 kB 67.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 81 kB 68.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 92 kB 71.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 102 kB 74.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 112 kB 74.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 122 kB 74.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 133 kB 74.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 143 kB 74.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 153 kB 74.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 160 kB 74.0 MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |██████▋                         | 10 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 20 kB 39.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 30 kB 47.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 40 kB 53.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 49 kB 6.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ErlmBoWaW5EZ",
        "outputId": "514c623b-e8fc-4b84-f05a-02c8692ca85a"
      },
      "source": [
        "token = \"7JM1V7L7-0kAAAAAAAAAAc1mKnyh-G-4YwfL2o9WNJ2Tdh2JJslf_U5IxGrgb-J-\"\n",
        "\n",
        "dbx = dropbox.Dropbox(token)\n",
        "pp.pprint(dbx.users_get_current_account(), compact=True, indent=4)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/21/2021 22:53:30 — INFO — dropbox — Request to users/get_current_account\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FullAccount(account_id='dbid:AAAYVJX_n2JYa7yh6anfANmwSJ7qkr-u8Do', account_type=AccountType('pro', None), country='CH', disabled=False, email='peterszemraj@gmail.com', email_verified=True, is_paired=False, locale='en', name=Name(abbreviated_name='PS', display_name='Peter Szemraj', familiar_name='Peter', given_name='Peter', surname='Szemraj'), profile_photo_url='https://dl-web.dropbox.com/account_photo/get/dbaphid%3AAACl3aYwbYAysBHXpGdGBXZj6IYsXocikxc?size=128x128&vers=1628569648991', referral_link='https://www.dropbox.com/referrals/AAAVOEoeLmg4Nw7hRIA9Wsn8OMf2_EdGbik?src=app9-10920512', root_info=UserRootInfo(home_namespace_id='9560958', root_namespace_id='9560958'), team=NOT_SET, team_member_id=NOT_SET)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QPLVcnGz1oxA",
        "outputId": "b816db1b-5e6b-4c2b-cefa-23aa64a22b95"
      },
      "source": [
        "def get_size_mb(path2file, verbose=False):\n",
        "\n",
        "    file_stats = os.stat(path2file)\n",
        "\n",
        "    file_size_mb = {file_stats.st_size / (1024 * 1024)}\n",
        "    if verbose: print(f'File Size in MegaBytes is {file_size_mb}')\n",
        "    return round(list(file_size_mb)[0],2) # returns rounded to 2 decimals"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vrV1mVSibJIy",
        "outputId": "691ab877-ca92-4487-9858-d62dbf3f5699"
      },
      "source": [
        "from os.path import join, basename, dirname\n",
        "import time, random\n",
        "from google.colab import files\n",
        "\n",
        "def put_in_dropbox(\n",
        "    vm_path, subfolder=dropbox_subfolder, no_printout=True, ncalls=0, max_calls=3\n",
        "):\n",
        "    if ncalls > max_calls:\n",
        "        return \"failed saving to DropBox - {} tries\".format(ncalls)\n",
        "    elif get_size_mb(vm_path) > 155:\n",
        "        files.download(vm_path)\n",
        "        return \"file size of {} too big to put in DB, downloading\".format(basename(vm_path))\n",
        "    # for an item on the colab machine on path, upload to dropbox app folder at\n",
        "    # subfolder/\"filename\"\n",
        "    base_filename = basename(vm_path)\n",
        "    db_path = \"/{}/{}\".format(subfolder, base_filename)\n",
        "    try:\n",
        "        with open(vm_path, \"rb\") as f:\n",
        "            dbx.files_upload(f.read(), path=db_path, autorename=True, mute=no_printout)\n",
        "    except:\n",
        "        print(\n",
        "            \"WARNING - unable to post in dropbox, retry no. {} - \".format(ncalls + 1),\n",
        "            datetime.now(),\n",
        "        )\n",
        "        time.sleep(random.randint(1, 3))  # small delay before trying again\n",
        "        put_in_dropbox(vm_path, ncalls=ncalls + 1)  # recursion for retrying"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRhgNvsH4Wn"
      },
      "source": [
        "# load model file\n",
        "\n",
        "\n",
        "- <font color=\"orange\"> IMPORTANT: while not validated _at the time of writing_, it is somewhat important to consider if optimal hyperparameter behavior "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "AXIbld-ASHL9",
        "outputId": "745e5d89-b8a1-4ce5-c9d1-e77916211178"
      },
      "source": [
        "model_size = \"774M\" #@param [\"355M\", \"774M\"]\n",
        "load_from_folder = True #@param {type:\"boolean\"}\n",
        "load_folder_dir = \"/content/drive/MyDrive/Programming/AI_peter/gpt2_std_gpu_774M\" #@param {type:\"string\"}\n",
        "do_grad_chkpt = False #@param {type:\"boolean\"}\n",
        "use_gpu = False #@param {type:\"boolean\"}\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "flqSlHjMIeIw",
        "outputId": "3a0cb9d3-d480-4ec0-dddd-7c3460450075"
      },
      "source": [
        "if load_from_folder:\n",
        "    ai = aitextgen(model_folder=load_folder_dir, to_gpu=use_gpu,\n",
        "                   gradient_checkpointing=do_grad_chkpt)\n",
        "else:\n",
        "    # load new \n",
        "    ai = aitextgen(tf_gpt2=model_size, to_gpu=use_gpu,\n",
        "                   gradient_checkpointing=do_grad_chkpt)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/21/2021 22:53:31 — INFO — aitextgen — Loading model from provided weights and config in //content/drive/MyDrive/Programming/AI_peter/gpt2_std_gpu_774M.\n",
            "10/21/2021 22:54:13 — INFO — aitextgen — GPT2 loaded with 774M parameters.\n",
            "10/21/2021 22:54:13 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zmoaSxGq5gN"
      },
      "source": [
        "# go through loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5np1MlFD2LRb",
        "outputId": "71710d0a-6d9b-4773-8b1e-092ad1e981ed"
      },
      "source": [
        "temp_srch = list(range(50, 100, 1))\n",
        "temp_srch = [float(item)/100 for item in temp_srch]\n",
        "top_p_srch = list(range(50, 100,2)) \n",
        "top_p_srch = [float(item)/100 for item in top_p_srch]\n",
        "topk_srch = list(range(50, 150, 25))\n",
        "estimate_comp = len(temp_srch)*len(top_p_srch)*len(topk_srch)\n",
        "print(\"anticipate approx {} calculations\\n\\n\".format(estimate_comp))\n",
        "\n",
        "grid_df = pd.DataFrame(columns=[\"temp\", \"topk\", \"top_p\", \"prompt\", \"speaker\",\n",
        "                                \"model_response\"])\n",
        "\n",
        "grid_df.info()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anticipate approx 5000 calculations\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 0 entries\n",
            "Data columns (total 6 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   temp            0 non-null      object\n",
            " 1   topk            0 non-null      object\n",
            " 2   top_p           0 non-null      object\n",
            " 3   prompt          0 non-null      object\n",
            " 4   speaker         0 non-null      object\n",
            " 5   model_response  0 non-null      object\n",
            "dtypes: object(6)\n",
            "memory usage: 0.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "k23F0B3x4MVw",
        "outputId": "3fd1abc6-27c5-4fe5-d20f-da66a4ee4a9d"
      },
      "source": [
        "import random\n",
        "n = 3\n",
        "print(f\"\\nrandom {n} values from temp list are \\n\", random.sample(temp_srch, k=n))\n",
        "print(f\"\\nrandom {n} values from top_p list are \\n\", random.sample(top_p_srch, k=n))\n",
        "print(f\"\\nrandom {n} values from topk list are \\n\", random.sample(topk_srch, k=n))\n",
        "\n",
        "\n",
        "# shuffle lists for search. going thru in ~random~ order enables being able\n",
        "# to somewhat look at the results during\n",
        "\n",
        "random.shuffle(temp_srch)\n",
        "random.shuffle(top_p_srch)\n",
        "\n",
        "\n",
        "generic_openers = [\n",
        "                   \"what's up?\",\n",
        "                   \"heyyyyyyyy\",\n",
        "                   \"u up?\",\n",
        "                   \"Have you done anything exciting lately?\",\n",
        "                   \"hey, how are you?\",\n",
        "                   \"Are you having a good time?\",\n",
        "                   \"Why did you come tonight?\",\n",
        "                   \"What are you going to do this weekend?\",\n",
        "                   \"Wow, it’s so hot/cold in here.\",\n",
        "                   \"What is a controversial opinion you have?\",\n",
        "                   \"What do you normally do for fun?\",\n",
        "                   \"What’s your biggest pet peeve?\",\n",
        "                   \"Do you like to cook?\",\n",
        "                   \"What’s the most interesting thing you’ve worked on lately?\",\n",
        "                   \"Is a hot dog a sandwich?\",\n",
        "                   \"What does your brother Bilo have?\",\n",
        "                   \"Are you a morning person or a night owl?\",\n",
        "                   \"How are your parents doing?\",\n",
        "                   \"How can you tell when someone is lying?\",\n",
        "                   \"what she orda?\"\n",
        "                ]\n",
        "\n",
        "test_speakers = [\n",
        "                 \"andrew e^x weatherly\",\n",
        "                 \"emilie szemraj\",\n",
        "                 \"christopher szemraj\",\n",
        "                 \"michael lebens\",\n",
        "                 'jonas meirer',\n",
        "                 \"lucia pasara\",\n",
        "                 \"jonathan\",\n",
        "                 \"joost\",\n",
        "                 \"jocelyn terle\",\n",
        "                 \"kevin ta\",\n",
        "                 \"andres\",\n",
        "                 'david \"pumpen\" wissel',\n",
        "                 \"stefan schopf\", \n",
        "                 \"caio dorea\",\n",
        "]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "random 3 values from temp list are \n",
            " [0.6, 0.53, 0.78]\n",
            "\n",
            "random 3 values from top_p list are \n",
            " [0.62, 0.64, 0.78]\n",
            "\n",
            "random 3 values from topk list are \n",
            " [75, 100, 50]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOpEvDf1QfZR"
      },
      "source": [
        "conversation starters taken from [this parade article](https://parade.com/969981/parade/conversation-starters/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "5a294874fa3145389f0fcf1853d38148",
            "11db70df106c462090c99144cea46411",
            "66a8fc7874644670b91a64d75ada22e6",
            "c3288c1c89664e30929d367f6a7e0557",
            "929ab4baebf7461ea006b74c787b6c3c",
            "ebb94c4c60ae466ca11c767ec41a9a13",
            "6c12fed11efe4bbf84b9dad63b6c5c5a",
            "845a338a99cf47b797f2c8e520102edd",
            "2d7904ff8b7a49d6930f6c927c61f494",
            "34be3b9d58de49149129e920f7fe4bc1",
            "22e3102462144dc4a46485c4395d3678"
          ]
        },
        "id": "Bt6Lacky4GmK",
        "outputId": "0e766a71-52cc-4001-83f5-e2cf6c6c24de"
      },
      "source": [
        "pbar = tqdm(total=estimate_comp, desc=\"completing grid search...\")\n",
        "\n",
        "for this_temp in temp_srch:\n",
        "    ind = temp_srch.index(this_temp)\n",
        "\n",
        "    if ind > 0 and ind % 5 == 0:\n",
        "        # save progress\n",
        "        timestamp = datetime.now().strftime(\"%b-%d-%Y_%H-%M\")\n",
        "        grid_df.reset_index(drop=True, inplace=True)\n",
        "        out_xl = \"GPT-Peter774M-gridsearch-{}.xlsx\".format(timestamp)\n",
        "        out_ftr = \"GPT-Peter774M-gridsearch-{}.ftr\".format(timestamp)\n",
        "        grid_df.to_excel(out_xl)\n",
        "        grid_df.to_feather(out_ftr)\n",
        "        put_in_dropbox(out_xl)\n",
        "        put_in_dropbox(out_ftr)\n",
        "\n",
        "    for this_top_p in top_p_srch:\n",
        "        for this_topk in topk_srch:\n",
        "\n",
        "            tst_spkr = random.sample(test_speakers, k=1)[0]\n",
        "            tst_prompt = random.sample(generic_openers, k=1)[0]\n",
        "\n",
        "            model_out = query_gpt_peter(\n",
        "                                        prompt_msg=tst_prompt,\n",
        "                                        speaker=tst_spkr,\n",
        "                                        kparam=this_topk,\n",
        "                                        temp=this_temp,\n",
        "                                        nuc_range=this_top_p,\n",
        "                                        verbose=False,\n",
        "                                        )\n",
        "            this_resp = model_out[\"out_text\"]\n",
        "            df_ind = len(grid_df)\n",
        "            # [\"temp\", \"topk\", \"top_p\", \"prompt\", \"speaker\", \"model_response\"]\n",
        "            grid_df.loc[df_ind] = [this_temp, this_topk, this_top_p, tst_prompt,\n",
        "                                   tst_spkr, this_resp]\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "pbar.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a294874fa3145389f0fcf1853d38148",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "completing grid search...:   0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/22/2021 00:43:47 — INFO — dropbox — Request to files/upload\n",
            "10/22/2021 00:43:48 — INFO — dropbox — Request to files/upload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBsKz-yk8VeQ"
      },
      "source": [
        "## save final results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FIzcf678Vi2"
      },
      "source": [
        "timestamp = datetime.now().strftime(\"%b-%d-%Y_%H-%M\")\n",
        "grid_df.reset_index(drop=True, inplace=True)\n",
        "out_xl = \"GPT-Peter774M-FINAL-gridsearch-{}.xlsx\".format(timestamp)\n",
        "out_ftr = \"GPT-Peter774M-FINAL-gridsearch-{}.ftr\".format(timestamp)\n",
        "grid_df.to_excel(out_xl)\n",
        "grid_df.to_feather(out_ftr)\n",
        "put_in_dropbox(out_xl)\n",
        "put_in_dropbox(out_ftr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Azv9Z2ZAsHw"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A19WObvdAtUa"
      },
      "source": [
        "error_string = \"<bro, there was an error. try again>\"\n",
        "\n",
        "df_analysis = grid_df.copy().convert_dtypes()\n",
        "df_analysis = df_analysis[df_analysis[\"model_response\"] != error_string]\n",
        "df_analysis.reset_index(drop=True, inplace=True)\n",
        "df_analysis.info(verbose=True) # overview"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIR9dYifF_Uh"
      },
      "source": [
        "assume the outputs are a single sentence and then use a [sentence-scoring algorithm on them](https://github.com/simonepri/lm-scorer) via `lm-scorer` package.\n",
        "\n",
        "- [here](https://colab.research.google.com/github/simonepri/lm-scorer/blob/master/examples/lm_scorer.ipynb) is an example notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPciEe8CGA-D"
      },
      "source": [
        "!pip install -q lm-scorer\n",
        "\n",
        "import torch\n",
        "from lm_scorer.models.auto import AutoLMScorer as LMScorer\n",
        "\n",
        "# Available models\n",
        "list(LMScorer.supported_model_names())\n",
        "# => [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\", distilgpt2\"]\n",
        "\n",
        "# Load model to cpu or cuda\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 1\n",
        "scorer = LMScorer.from_pretrained(\"gpt2-large\", device=device,\n",
        "                                  batch_size=batch_size)\n",
        "print(\"loaded - \", datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLaDdHizHIil"
      },
      "source": [
        "# example\n",
        "\n",
        "# Compute sentence score as the geometric mean of tokens' probabilities\n",
        "scorer.sentence_score(\"I like this package.\", reduce=\"gmean\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqhFW7PHnWq"
      },
      "source": [
        "# create helper fn\n",
        "\n",
        "def score_text_blob(gen_text: str, method=\"gmean\", verbose=False):\n",
        "    # for method 'mean', \n",
        "    try:\n",
        "        return scorer.sentence_score(gen_text, reduce=method)\n",
        "    except:\n",
        "        if verbose: print(f\"failed computing scores for {gen_text}\")\n",
        "        return 0 # score sentences that failed at 0 - assumption = too undecipherable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1arelYZBM0Wb"
      },
      "source": [
        "### apply and see results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie1WA7-hXaNF"
      },
      "source": [
        "df_analysis.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZZt72ZGM3I7"
      },
      "source": [
        "import time\n",
        "st = time.time()\n",
        "df_analysis[\"model_response\"] = df_analysis[\"model_response\"].apply(str)\n",
        "df_analysis[\"model_response\"] = df_analysis[\"model_response\"].apply(cleantxt_wrap)\n",
        "df_analysis[\"geom_score\"] = df_analysis[\"model_response\"].apply(score_text_blob)\n",
        "df_analysis[\"prod_score\"] = df_analysis[\"model_response\"].apply(score_text_blob, \n",
        "                                                                args=(\"prod\",))\n",
        "df_analysis[\"mean_score\"] = df_analysis[\"model_response\"].apply(score_text_blob, \n",
        "                                                                args=(\"mean\",))\n",
        "df_analysis[\"hmean_score\"] = df_analysis[\"model_response\"].apply(score_text_blob, \n",
        "                                                                args=(\"hmean\",))\n",
        "rt = (time.time() - st) / 60 # minutes\n",
        "print(f\"the total runtime was {rt} minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VFewiDThHgv"
      },
      "source": [
        "## basic - plotly\n",
        "\n",
        "- [plotly express docs](https://plotly.com/python/plotly-express/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUlZRKzrWMno"
      },
      "source": [
        "fig_temp = px.histogram(df_analysis, x=\"mean_score\", color=\"topk\",\n",
        "                        marginal=\"box\", # or violin, rug\n",
        "                        hover_data=df_analysis.columns, template=\"plotly_dark\",\n",
        "                        title=\"Text Gen Mean Score Dist - various top_k values\",\n",
        "                        height=720, width=int(720*1.613))\n",
        "fig_temp.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztTFat5Ed0cf"
      },
      "source": [
        "fig_temp = px.histogram(df_analysis, x=\"prod_score\", color=\"topk\",\n",
        "                        marginal=\"box\", # or violin, rug\n",
        "                        hover_data=df_analysis.columns, template=\"ggplot2\",\n",
        "                        title=\"Product Score Dist for different top_k values\",\n",
        "                        log_y=True, \n",
        "                        height=720, width=int(720*1.613))\n",
        "fig_temp.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47h82DB1feLb"
      },
      "source": [
        "df_analysis.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe9DhFcaes5C"
      },
      "source": [
        "px.scatter_matrix(df_analysis, template=\"plotly_dark\", \n",
        "                    dimensions=['temp', 'topk', 'top_p','geom_score', \n",
        "                                'mean_score', 'hmean_score', 'prod_score'],\n",
        "                    height=1080, width=int(1080*1.613),\n",
        "                    title=\"scatter matrix - all quant vars\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "637mHFl588-N"
      },
      "source": [
        "## autoviz\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AR99vX70vu9X"
      },
      "source": [
        "save_av = True #@param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Xe1p0D9IhG"
      },
      "source": [
        "%%capture\n",
        "!pip install -U autoviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIFwVxNA-ssc"
      },
      "source": [
        "from autoviz.AutoViz_Class import AutoViz_Class\n",
        "\n",
        "AV = AutoViz_Class()\n",
        "filename = \"for_autoviz.csv\"\n",
        "\n",
        "df_analysis.to_csv(filename, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xClTiYHi-ztz"
      },
      "source": [
        "sep = \",\"\n",
        "dft = AV.AutoViz(\n",
        "    filename,\n",
        "    sep=\",\",\n",
        "    depVar=\"prod_score\",\n",
        "    dfte=None,\n",
        "    header=0,\n",
        "    verbose=2,\n",
        "    lowess=True,\n",
        "    chart_format=\"svg\",\n",
        "    max_rows_analyzed=150000,\n",
        "    max_cols_analyzed=30,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeWQCS31pKOB"
      },
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "autopath = Path(\"/content/AutoViz_Plots\")\n",
        "av_arc = \"GPT-Peter Hyperparam Opt_autoviz_plots_{}\".format(timestamp)\n",
        "shutil.make_archive(av_arc, \"gztar\", autopath)\n",
        "\n",
        "if save_av: put_in_dropbox(av_arc + \".tar.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQzqDKSB4cym"
      },
      "source": [
        "## sweetviz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLPiuLqN4cyy"
      },
      "source": [
        "!pip install -q sweetviz\n",
        "import sweetviz as sv\n",
        "\n",
        "df_sv = df_analysis.copy().convert_dtypes()\n",
        "\n",
        "feature_config = sv.FeatureConfig(skip=['prompt', 'speaker', 'model_response',],\n",
        "                                  force_num=['temp', 'topk', 'top_p',\n",
        "                                             'geom_score', 'prod_score'],\n",
        "                                  )\n",
        "\n",
        "\n",
        "\n",
        "df_sv = df_sv[['temp', 'topk', 'top_p', 'geom_score', 'prod_score']]\n",
        "\n",
        "df_sv = df_sv.astype(float)\n",
        "\n",
        "df_sv[\"model_response\"] = df_analysis[\"model_response\"].astype(str)\n",
        "df_sv[\"model_response\"] = df_sv[\"model_response\"].astype(\"string\")\n",
        "df_sv.info()\n",
        "# df_sv[\"topk\"] = df_sv[\"topk\"].astype(float)\n",
        "# df_sv[\"topk\"] = df_sv[\"topk\"].astype(float)\n",
        "# arameters are skip, force_cat, force_num and force_text. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awgOo3WM4cyz"
      },
      "source": [
        "my_report = sv.analyze(df_sv, \n",
        "                       target_feat=\"prod_score\")\n",
        "\n",
        "sv_eda_name = \"GPT-Peter Hyperparam Opt-SWEETVIZ-EDA-{}.html\".format(timestamp)\n",
        "\n",
        "my_report.show_html(filepath=sv_eda_name,\n",
        "                    layout='vertical',) # Default arguments will generate to \"SWEE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r85YSapL4cyz"
      },
      "source": [
        "\n",
        "\n",
        "from IPython.display import HTML\n",
        "HTML(sv_eda_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUAn8N4Y4cy0"
      },
      "source": [
        "put_in_dropbox(sv_eda_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2Hq36wgDGF8"
      },
      "source": [
        "## data table + export "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUT20qJODJ6Z"
      },
      "source": [
        "from google.colab import *\n",
        "\n",
        "\n",
        "data_table.DataTable(df_analysis, num_rows_per_page=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vdKLWUyDK2L"
      },
      "source": [
        "df_analysis.reset_index(drop=True, inplace=True)\n",
        "data_header = \"GPT-Peter Hyperparam Analysis w Metrics - {}\".format(timestamp)\n",
        "df_analysis.to_excel(data_header + \".xlsx\")\n",
        "put_in_dropbox(data_header + \".xlsx\")\n",
        "\n",
        "df_analysis.to_feather(data_header + \".ftr\")\n",
        "put_in_dropbox(data_header + \".xlsx\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HJpIS6V_UPi"
      },
      "source": [
        "## pandas profiling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-22dBq03naW"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTwrpwVk_Vax"
      },
      "source": [
        "!pip uninstall -y -q pandas-profiling\n",
        "!pip install -U -q pandas-profiling\n",
        "\n",
        "import numpy as np\n",
        "from pandas_profiling import ProfileReport"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fge_Euam6iii"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9WDV2R__gR9"
      },
      "source": [
        "df_profile = df_analysis[['temp', 'topk', 'top_p',\n",
        "                          'prod_score', 'mean_score',\n",
        "                          'hmean_score', 'model_response']]\n",
        "profile = ProfileReport(df_profile, dark_mode=True,\n",
        "                        title=\"GPT-Peter Hyperparam Opt (Sentence Score)\",\n",
        "                        minimal=True,\n",
        "                        )\n",
        "\n",
        "\n",
        "profile.to_notebook_iframe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1Y8l96jBIbG"
      },
      "source": [
        "\n",
        "pd_eda_name = \"GPT-Peter Hyperparam Opt-EDA-{}.html\".format(timestamp)\n",
        "profile.to_file(pd_eda_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfDiMaVDTeK4"
      },
      "source": [
        "put_in_dropbox(pd_eda_name)\n",
        "print(\"saved - \", datetime.now())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}